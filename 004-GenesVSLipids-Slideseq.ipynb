{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769a7283-59df-415c-85a8-9ceec3a7096b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# force all backends to use just 1 thread\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"]   = \"1\"\n",
    "os.environ[\"OMP_NUM_THREADS\"]        = \"1\"\n",
    "os.environ[\"MKL_NUM_THREADS\"]        = \"1\"\n",
    "os.environ[\"VECLIB_MAXIMUM_THREADS\"] = \"1\"\n",
    "os.environ[\"NUMEXPR_NUM_THREADS\"]    = \"1\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "from scipy.stats import pearsonr\n",
    "import matplotlib\n",
    "matplotlib.rcParams['pdf.fonttype'] = 42\n",
    "\n",
    "from threadpoolctl import threadpool_limits, threadpool_info\n",
    "threadpool_limits(limits=8)\n",
    "import os\n",
    "os.environ['OMP_NUM_THREADS'] = '6'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b9ee5d-490d-4910-a838-36008baa49c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dat = pd.read_parquet(\"./zenodo/maindata_2.parquet\")\n",
    "dat = dat.loc[dat['Sample'] == \"ReferenceAtlas\",:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd490380-62b7-4e5b-91d4-ebd078fb39c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "beb6cc26-73b6-4b17-a362-5cc40e36d699",
   "metadata": {},
   "source": [
    "## Summarize genes and lipids at the cell type level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3bedf8d-1dbb-4200-93c9-7983b22780e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "gexpr = pd.read_parquet(\"./zenodo/multimodal/multimodal_on_macoscko.parquet\")\n",
    "gexpr = gexpr[~gexpr.index.duplicated(keep='first')]\n",
    "genes = gexpr.iloc[:,:-176]\n",
    "lipids = gexpr.iloc[:,-173:]\n",
    "celltypesnow = pd.read_hdf(\"./zenodo/multimodal/celltypesnow.h5ad\", key=\"table\")\n",
    "lipids = lipids.groupby(celltypesnow).mean()\n",
    "genes = genes.groupby(celltypesnow).mean()\n",
    "genes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44438f19-ede6-4095-b917-2ff4b1ab7d36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5b42cb96-6d52-4ba5-b7fb-a1a78eb4bbdb",
   "metadata": {},
   "source": [
    "## Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ddf230e-3f5c-4be8-b039-fdc6921f6701",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = genes\n",
    "y = lipids\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "print(f\"Training set size: {X_train.shape[0]} samples\")\n",
    "print(f\"Testing set size: {X_test.shape[0]} samples\")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "pca = PCA(n_components=0.99, random_state=42) # we'll use a PCA capturing most variance of gene expression and look at the PC loadings to bypass multicollinearity\n",
    "X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "X_test_pca = pca.transform(X_test_scaled)\n",
    "print(f\"Original number of features: {X_train.shape[1]}\")\n",
    "print(f\"Reduced number of principal components: {X_train_pca.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd121318-e841-4a88-8dd6-069c6f44e702",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1ae64d61-e0b7-4ac2-b81e-f42ada6824e2",
   "metadata": {},
   "source": [
    "## Linear predictive model genes to lipids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56fda5dc-dee9-42b9-9c84-066637d5ceda",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_regressor = LinearRegression()\n",
    "\n",
    "multi_output_regressor = MultiOutputRegressor(linear_regressor, n_jobs=8)\n",
    "multi_output_regressor.fit(X_train_pca, y_train)\n",
    "print(\"Linear Regression MultiOutputRegressor training complete.\")\n",
    "\n",
    "y_train_pred = multi_output_regressor.predict(X_train_pca)\n",
    "y_test_pred = multi_output_regressor.predict(X_test_pca)\n",
    "\n",
    "train_mse = mean_squared_error(y_train, y_train_pred, multioutput='raw_values')\n",
    "test_mse = mean_squared_error(y_test, y_test_pred, multioutput='raw_values')\n",
    "\n",
    "train_r = []\n",
    "test_r = []\n",
    "\n",
    "for i, lipid in enumerate(y_train.columns):\n",
    "    r_train, _ = pearsonr(y_train[lipid], y_train_pred[:, i])\n",
    "    train_r.append(r_train)\n",
    "    r_test, _ = pearsonr(y_test[lipid], y_test_pred[:, i])\n",
    "    test_r.append(r_test)\n",
    "\n",
    "pearson_df = pd.DataFrame({\n",
    "    'Lipid': y_train.columns,\n",
    "    'Train_Pearson_R': train_r,\n",
    "    'Test_Pearson_R': test_r,\n",
    "    'Train_MSE': train_mse,\n",
    "    'Test_MSE': test_mse\n",
    "})\n",
    "\n",
    "pearson_df.to_csv(\"linear_genestolipids.csv\")\n",
    "\n",
    "plt.hist(pearson_df['Test_Pearson_R'], bins=20, color=\"darkred\", alpha=0.8)\n",
    "plt.hist(pearson_df['Train_Pearson_R'], bins=20, color=\"black\", alpha=0.8)\n",
    "plt.gca().spines['top'].set_visible(False)\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "plt.gca().spines['left'].set_visible(False)\n",
    "plt.gca().yaxis.set_ticks([])\n",
    "plt.gca().xaxis.set_ticks_position('bottom')\n",
    "plt.show()\n",
    "\n",
    "pearson_df.sort_values('Test_Pearson_R')[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa42d24-30d7-4e87-afa6-a2ca3923aaa3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7299d29f-4e14-43ef-817f-1cba84039a6a",
   "metadata": {},
   "source": [
    "## Elastic net: a linear but regularized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee0e261-af83-4dda-bc3f-100fc3a9ccd5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.stats import pearsonr\n",
    "from tqdm import tqdm\n",
    "\n",
    "param_grid = {\n",
    "    'alpha': [0.001, 0.01, 0.1, 1.0],\n",
    "    'l1_ratio': [0.1, 0.5, 0.9]\n",
    "}\n",
    "\n",
    "lipids = []\n",
    "train_mses = []\n",
    "test_mses = []\n",
    "train_r_values = []\n",
    "test_r_values = []\n",
    "best_alphas = []\n",
    "best_l1_ratios = []\n",
    "\n",
    "for lipid in tqdm(y_train.columns):\n",
    "\n",
    "    y_train_lipid = y_train[lipid]\n",
    "    y_test_lipid = y_test[lipid]\n",
    "\n",
    "    elastic_net = ElasticNet(max_iter=100000, random_state=42)\n",
    "\n",
    "    grid_search = GridSearchCV(\n",
    "        elastic_net, \n",
    "        param_grid, \n",
    "        cv=3, \n",
    "        scoring='neg_mean_squared_error',\n",
    "        n_jobs=8, \n",
    "        verbose=2\n",
    "    )\n",
    "    grid_search.fit(X_train_pca, y_train_lipid)\n",
    "\n",
    "    best_model = grid_search.best_estimator_\n",
    "    best_alpha = grid_search.best_params_['alpha']\n",
    "    best_l1_ratio = grid_search.best_params_['l1_ratio']\n",
    "    best_model.fit(X_train_pca, y_train_lipid)\n",
    "\n",
    "    y_train_pred = best_model.predict(X_train_pca)\n",
    "    y_test_pred = best_model.predict(X_test_pca)\n",
    "\n",
    "    # metrics\n",
    "    train_mse = mean_squared_error(y_train_lipid, y_train_pred)\n",
    "    test_mse = mean_squared_error(y_test_lipid, y_test_pred)\n",
    "    r_train, _ = pearsonr(y_train_lipid, y_train_pred)\n",
    "    r_test, _ = pearsonr(y_test_lipid, y_test_pred)\n",
    "    lipids.append(lipid)\n",
    "    train_mses.append(train_mse)\n",
    "    test_mses.append(test_mse)\n",
    "    train_r_values.append(r_train)\n",
    "    test_r_values.append(r_test)\n",
    "    best_alphas.append(best_alpha)\n",
    "    best_l1_ratios.append(best_l1_ratio)\n",
    "\n",
    "pearson_df = pd.DataFrame({\n",
    "    'Lipid': lipids,\n",
    "    'Train_Pearson_R': train_r_values,\n",
    "    'Test_Pearson_R': test_r_values,\n",
    "    'Train_MSE': train_mses,\n",
    "    'Test_MSE': test_mses,\n",
    "    'Best_Alpha': best_alphas,\n",
    "    'Best_L1_Ratio': best_l1_ratios\n",
    "})\n",
    "\n",
    "pearson_df.to_csv(\"elastic_genestolipids.csv\", index=False)\n",
    "\n",
    "plt.hist(pearson_df['Test_Pearson_R'], bins=20, color=\"darkred\", alpha=0.8)\n",
    "plt.hist(pearson_df['Train_Pearson_R'], bins=20, color=\"black\", alpha=0.8)\n",
    "plt.gca().spines['top'].set_visible(False)\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "plt.gca().spines['left'].set_visible(False)\n",
    "plt.gca().yaxis.set_ticks([])\n",
    "plt.gca().xaxis.set_ticks_position('bottom')\n",
    "plt.show()\n",
    "\n",
    "pearson_df.sort_values('Test_Pearson_R')[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5b63f6-e32f-4b46-84db-2053ab8c0408",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "baf64900-005b-4138-aa76-8cd888ef0422",
   "metadata": {},
   "source": [
    "## Elastic net but using genes as predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945e7943-73bb-4e09-bd67-065674b04e92",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.stats import pearsonr\n",
    "from tqdm import tqdm\n",
    "\n",
    "param_grid = {\n",
    "    'alpha': [1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2],\n",
    "    'l1_ratio': [0.1, 0.5, 0.9]\n",
    "}\n",
    "\n",
    "lipids = []\n",
    "train_mses = []\n",
    "test_mses = []\n",
    "train_r_values = []\n",
    "test_r_values = []\n",
    "best_alphas = []\n",
    "best_l1_ratios = []\n",
    "all_feature_importances = []\n",
    "\n",
    "for lipid in tqdm(y_train.columns):\n",
    "\n",
    "    y_train_lipid = y_train[lipid]\n",
    "    y_test_lipid = y_test[lipid]\n",
    "\n",
    "    elastic_net = ElasticNet(max_iter=100000, random_state=42)\n",
    "\n",
    "    grid_search = GridSearchCV(\n",
    "        elastic_net, \n",
    "        param_grid, \n",
    "        cv=3, \n",
    "        scoring='neg_mean_squared_error',\n",
    "        n_jobs=8, \n",
    "        verbose=2\n",
    "    )\n",
    "    grid_search.fit(X_train_scaled, y_train_lipid)\n",
    "\n",
    "    best_model = grid_search.best_estimator_\n",
    "    best_alpha = grid_search.best_params_['alpha']\n",
    "    best_l1_ratio = grid_search.best_params_['l1_ratio']\n",
    "    best_model.fit(X_train_scaled, y_train_lipid)\n",
    "\n",
    "    y_train_pred = best_model.predict(X_train_scaled)\n",
    "    y_test_pred = best_model.predict(X_test_scaled)\n",
    "\n",
    "    # metrics\n",
    "    train_mse = mean_squared_error(y_train_lipid, y_train_pred)\n",
    "    test_mse = mean_squared_error(y_test_lipid, y_test_pred)\n",
    "    r_train, _ = pearsonr(y_train_lipid, y_train_pred)\n",
    "    r_test, _ = pearsonr(y_test_lipid, y_test_pred)\n",
    "    lipids.append(lipid)\n",
    "    train_mses.append(train_mse)\n",
    "    test_mses.append(test_mse)\n",
    "    train_r_values.append(r_train)\n",
    "    test_r_values.append(r_test)\n",
    "    best_alphas.append(best_alpha)\n",
    "    best_l1_ratios.append(best_l1_ratio)\n",
    "    \n",
    "    feature_importance = pd.DataFrame({\n",
    "    'feature': X_train.columns,\n",
    "    'importance': abs(best_model.coef_)\n",
    "    })\n",
    "\n",
    "    import pickle\n",
    "    with open('elastic_net_model_'+lipid+'.pkl', 'wb') as f:\n",
    "        pickle.dump(best_model, f)\n",
    "        \n",
    "    all_feature_importances.append(feature_importance)\n",
    "\n",
    "pearson_df = pd.DataFrame({\n",
    "    'Lipid': lipids,\n",
    "    'Train_Pearson_R': train_r_values,\n",
    "    'Test_Pearson_R': test_r_values,\n",
    "    'Train_MSE': train_mses,\n",
    "    'Test_MSE': test_mses,\n",
    "    'Best_Alpha': best_alphas,\n",
    "    'Best_L1_Ratio': best_l1_ratios\n",
    "})\n",
    "\n",
    "#pearson_df.to_csv(\"elastic_genestolipids.csv\", index=False)\n",
    "\n",
    "plt.hist(pearson_df['Test_Pearson_R'], bins=20, color=\"darkred\", alpha=0.8)\n",
    "plt.hist(pearson_df['Train_Pearson_R'], bins=20, color=\"black\", alpha=0.8)\n",
    "plt.gca().spines['top'].set_visible(False)\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "plt.gca().spines['left'].set_visible(False)\n",
    "plt.gca().yaxis.set_ticks([])\n",
    "plt.gca().xaxis.set_ticks_position('bottom')\n",
    "#plt.savefig(\"elastic_genestolipids.pdf\")\n",
    "plt.show()\n",
    "\n",
    "pearson_df.sort_values('Test_Pearson_R')[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb361617-fcdb-4ec5-90d5-5c7d4974a990",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "98417971-b47a-4fca-809b-4e6527e6609a",
   "metadata": {},
   "source": [
    "## Study the gene feature importances to predict lipids to chase functional categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb653fd-e444-4ad5-bdf3-555dca7da2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_feature_importancesss = [pd.Series(all_feature_importances[x]['importance'].values, index = all_feature_importances[x]['feature'].values) for x in range(len(all_feature_importances))]\n",
    "featimps = pd.DataFrame(all_feature_importancesss)\n",
    "featimps.index = y_train.columns\n",
    "\n",
    "featimps.sum()[featimps.sum() > np.percentile(featimps.sum(), 90)].index # 2188 genes that contribute significantly to the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e25e65-6bbc-406c-bf5a-27423fae1600",
   "metadata": {},
   "outputs": [],
   "source": [
    "featimps = pd.DataFrame(all_feature_importancesss)\n",
    "featimps.index = y_train.columns\n",
    "featimps = featimps.loc[:,featimps.sum()[featimps.sum() > np.percentile(featimps.sum(), 90)].index]\n",
    "featimps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46d0102-d1d0-46f6-9b00-66830467a607",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare relative ranks of features rather than absolute importance values\n",
    "\n",
    "df_ranked = featimps.rank(axis=1, method='max')\n",
    "df_ranked[featimps == 0] = 9999\n",
    "\n",
    "x = range(len(df_ranked.sum().sort_values()))\n",
    "y = df_ranked.sum().sort_values().values\n",
    "\n",
    "from scipy.signal import savgol_filter\n",
    "\n",
    "y_smooth = savgol_filter(y, window_length=51, polyorder=4)\n",
    "dy = np.gradient(y_smooth)\n",
    "dy2 = np.gradient(dy)\n",
    "curvature = np.abs(dy2) / (1 + dy**2)**1.5\n",
    "elbow_point = np.argmax(curvature[50:-50]) + 50 \n",
    "\n",
    "plt.plot(x, y)\n",
    "plt.xticks([])\n",
    "plt.axvline(x=elbow_point, color='r', linestyle='--', label=f'Elbow at {elbow_point}')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7886e1-bcf5-4664-b92e-218fe07fa13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "genesthatpredictlipids = df_ranked.sum().sort_values().index[:elbow_point]\n",
    "lipidpredgene = -df_ranked.loc[:,genesthatpredictlipids]\n",
    "lipidpredgene[df_ranked == 9999] = None\n",
    "lipidpredgene = lipidpredgene-lipidpredgene.min().min()\n",
    "background = genes.columns.values\n",
    "background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0bae25b-e065-43e2-88e6-be3c375cb6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) how many high feature importance genes are cell type markers? are markers over or underrepresented?\n",
    "\n",
    "## focus on the \"smaller\" classes + readd cell type markers in a controlled way - we need to say something about them too...\n",
    "celltypes = genes.index.unique()\n",
    "import numpy as np\n",
    "\n",
    "def extract_markers(celltypes):\n",
    "    markers = set()\n",
    "    \n",
    "    for celltype in celltypes:\n",
    "        _, marker_part = celltype.split('_', 1)\n",
    "        marker_tokens = marker_part.split('_')\n",
    "        for token in marker_tokens:\n",
    "            try:\n",
    "                int(token)\n",
    "            except ValueError:\n",
    "                markers.add(token)\n",
    "\n",
    "    return sorted(list(markers))\n",
    "\n",
    "markers = extract_markers(celltypes)\n",
    "len(np.unique(markers)) # they are nice but too many! but we can downsample many times to have 300 of them and take the best iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e2906a-77ad-4209-b97b-e91cadf9ef50",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.intersect1d(markers,genesthatpredictlipids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463e9d97-03ba-4131-8921-51f3358c5725",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = set(markers)\n",
    "B = set(genesthatpredictlipids)\n",
    "C = set(background)\n",
    "\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "# a, b, and c are Python sets\n",
    "A_B = A.intersection(B)            # A ∩ B\n",
    "A_notB = A.difference(B)           # A ∩ (C \\ B)\n",
    "notA_B = B.difference(A)           # (C \\ A) ∩ B\n",
    "notA_notB = C.difference(A.union(B))  # (C \\ A) ∩ (C \\ B)\n",
    "\n",
    "n11 = len(A_B)\n",
    "n12 = len(A_notB)\n",
    "n21 = len(notA_B)\n",
    "n22 = len(notA_notB)\n",
    "\n",
    "table = np.array([[n11, n12],\n",
    "                  [n21, n22]], dtype=np.float64)\n",
    "table"
   ]
  },
  {
   "cell_type": "raw",
   "id": "abbef33a-460b-4a14-845a-925e8665a49d",
   "metadata": {},
   "source": [
    "# alternative\n",
    "\n",
    "A = set(markers)\n",
    "B = set(genesthatpredictlipids)\n",
    "C = set(background)\n",
    "\n",
    "x = len(A.intersection(B))\n",
    "m_x = len(A) - x\n",
    "k_x = len(B.difference(A))\n",
    "n_k__x = len(C) - k_x\n",
    "\n",
    "table = np.array([[x, m_x],\n",
    "                  [k_x, n_k__x]], dtype=np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1a6564-df95-4c50-a2ee-8da729f63f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chi-square Test\n",
    "chi2_stat, chi2_p, dof, expected = chi2_contingency(table)\n",
    "print(\"Chi-square statistic:\", chi2_stat)\n",
    "print(\"p-value:\", chi2_p)\n",
    "print(\"Degrees of freedom:\", dof)\n",
    "print(\"Expected counts:\\n\", expected)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def odds_ratio_and_ci_2x2(n11, n12, n21, n22, alpha=0.05):\n",
    "    \"\"\"\n",
    "    Returns the odds ratio and an approximate (1-alpha)% CI \n",
    "    for a 2x2 contingency table.\n",
    "    \"\"\"\n",
    "    # odds ratio\n",
    "    or_ = (n11 * n22) / (n12 * n21)\n",
    "    \n",
    "    # standard error of ln(OR)\n",
    "    se_log_or = np.sqrt(1/n11 + 1/n12 + 1/n21 + 1/n22)\n",
    "    \n",
    "    # z-value for the two-sided (1-alpha) confidence\n",
    "    z = 1.96 if alpha == 0.05 else None\n",
    "   \n",
    "    # log of odds ratio\n",
    "    log_or = np.log(or_)\n",
    "    \n",
    "    # confidence limits in log scale\n",
    "    lower_log = log_or - z * se_log_or\n",
    "    upper_log = log_or + z * se_log_or\n",
    "    \n",
    "    # exponentiate back\n",
    "    lower = np.exp(lower_log)\n",
    "    upper = np.exp(upper_log)\n",
    "    \n",
    "    return or_, (lower, upper)\n",
    "\n",
    "OR, (ci_lower, ci_upper) = odds_ratio_and_ci_2x2(n11, n12, n21, n22)\n",
    "print(f\"Odds Ratio: {OR:.4f}\")\n",
    "print(f\"95% CI: ({ci_lower:.4f}, {ci_upper:.4f})\") # no over, no under representation, can't say"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324c76e3-777d-4280-8217-782e9385ae8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) gene ontology?\n",
    "\n",
    "genesthatpredictlipids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee76ccc-7187-4781-9fc7-093b88fdd044",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import goatools\n",
    "from goatools.anno.genetogo_reader import Gene2GoReader\n",
    "from goatools.base import download_go_basic_obo, download_ncbi_associations\n",
    "from goatools.obo_parser import GODag\n",
    "from goatools.test_data.genes_NCBI_10090_ProteinCoding import GENEID2NT as GeneID2nt_mus\n",
    "from goatools.goea.go_enrichment_ns import GOEnrichmentStudyNS\n",
    "import collections as cx\n",
    "import pandas as pd\n",
    "from goatools.godag_plot import plot_gos, plot_results, plot_goid2goobj\n",
    "from goatools.associations import read_ncbi_gene2go\n",
    "from goatools.anno.factory import get_objanno\n",
    "from goatools.go_enrichment import GOEnrichmentStudy\n",
    "import mygene\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from goatools.gosubdag.gosubdag import GoSubDag\n",
    "mpl.rcParams['pdf.fonttype'] = 42\n",
    "\n",
    "obo_dag = GODag(\"./zenodo/mixed/go-basic.obo\")\n",
    "# wget http://geneontology.org/ontology/go-basic.obo\n",
    "obo_fname = download_go_basic_obo()\n",
    "\n",
    "# dictionary of  gene symbols: Gene Ontology terms\n",
    "associations = read_ncbi_gene2go('./zenodo/mixed/gene2go', taxids=[10090],namespace='MF')  # 10090 is the taxid for mouse\n",
    "obj_ncbi = get_objanno('./zenodo/mixed/gene2go', taxid=10090)\n",
    "associations = obj_ncbi.get_id2gos(namespace='all')\n",
    "\n",
    "bp_terms = []\n",
    "for go_id, go_term in obo_dag.items():\n",
    "    if go_term.namespace == 'biological_process':\n",
    "        bp_terms.append(go_id)\n",
    "    elif go_term.namespace == 'molecular_function':\n",
    "        bp_terms.append(go_id)\n",
    "    elif go_term.namespace == 'cellular_component':\n",
    "        bp_terms.append(go_id)\n",
    "        \n",
    "        \n",
    "go_subdag = GoSubDag(bp_terms, obo_dag)\n",
    "bp_associations = {}\n",
    "for gene, terms in associations.items():\n",
    "    bp_associations[gene] = [term for term in terms if term in bp_terms]\n",
    "    \n",
    "mg = mygene.MyGeneInfo()\n",
    "\n",
    "def convert_symbols_to_entrez(gene_symbols):\n",
    "    gene_info = mg.querymany(gene_symbols, scopes='symbol', fields='entrezgene', species='mouse')\n",
    "    entrez_ids = [int(gene['entrezgene']) for gene in gene_info if 'entrezgene' in gene]\n",
    "    return entrez_ids\n",
    "population_genes = convert_symbols_to_entrez(background)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0658d0dc-4206-4851-abd1-9c2e9e20e7a3",
   "metadata": {},
   "source": [
    "# loop over clusters\n",
    "\n",
    "from itertools import islice\n",
    "\n",
    "study_genes = convert_symbols_to_entrez(genesthatpredictlipids)\n",
    "\n",
    "g = GOEnrichmentStudy(\n",
    "    population_genes,\n",
    "    associations,\n",
    "    obo_dag,\n",
    "    propagate_counts=False,\n",
    "    alpha=0.05,\n",
    "    methods=['fdr_bh']  # use FDR Benjamini-Hochberg correction\n",
    ")\n",
    "\n",
    "# run the enrichment analysis\n",
    "results = g.run_study(study_genes)\n",
    "results_df = pd.DataFrame(columns=['GO_name', 'p-value', 'number_items', 'category'])\n",
    "\n",
    "for r in results:\n",
    "    results_df.loc[r.GO] = [r.name, r.p_fdr_bh, r.study_count, obo_dag[r.GO].namespace]\n",
    "\n",
    "results_df = results_df.loc[results_df['p-value'] < 0.1,:]\n",
    "\n",
    "from itertools import islice\n",
    "\n",
    "study_genes = convert_symbols_to_entrez(df_ranked.sum().sort_values().index.values) # try checking the very top ones\n",
    "\n",
    "g = GOEnrichmentStudy(\n",
    "    population_genes,\n",
    "    associations,\n",
    "    obo_dag,\n",
    "    propagate_counts=False,\n",
    "    alpha=0.05,\n",
    "    methods=['fdr_bh']  # use FDR Benjamini-Hochberg correction\n",
    ")\n",
    "\n",
    "# run the enrichment analysis\n",
    "results = g.run_study(study_genes)\n",
    "results_df = pd.DataFrame(columns=['GO_name', 'p-value', 'number_items', 'category'])\n",
    "\n",
    "for r in results:\n",
    "    results_df.loc[r.GO] = [r.name, r.p_fdr_bh, r.study_count, obo_dag[r.GO].namespace]\n",
    "\n",
    "results_df = results_df.loc[results_df['p-value'] < 0.1,:]\n",
    "results_df"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c5c4490a-7c59-4b0d-bb50-7fc5bd08a710",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "source": [
    "# do it one lipid at a time\n",
    "\n",
    "goperlip = {}\n",
    "\n",
    "from itertools import islice\n",
    "\n",
    "for xxxxx in tqdm(df_ranked.index):\n",
    "    study_genes = convert_symbols_to_entrez(df_ranked.loc[xxxxx,:][df_ranked.loc[xxxxx,:] != 9999].index.values) # try checking the very top ones\n",
    "\n",
    "    g = GOEnrichmentStudy(\n",
    "        population_genes,\n",
    "        associations,\n",
    "        obo_dag,\n",
    "        propagate_counts=False,\n",
    "        alpha=0.05,\n",
    "        methods=['fdr_bh']  # use FDR Benjamini-Hochberg correction\n",
    "    )\n",
    "\n",
    "    # run the enrichment analysis\n",
    "    results = g.run_study(study_genes)\n",
    "    results_df = pd.DataFrame(columns=['GO_name', 'p-value', 'number_items', 'category'])\n",
    "\n",
    "    for r in results:\n",
    "        results_df.loc[r.GO] = [r.name, r.p_fdr_bh, r.study_count, obo_dag[r.GO].namespace]\n",
    "\n",
    "    results_df = results_df.loc[results_df['p-value'] < 0.1,:]\n",
    "    goperlip[xxxxx] = results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84cd318b-8d35-4506-ad96-df02b99f9f28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "63becf3c-5096-46d1-bee5-be712fbdf1a6",
   "metadata": {},
   "source": [
    "## XGBoost as a vanilla but powerfully generalizing nonlinear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4189c61-a46b-4dbd-997d-8dce409a565e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import anndata\n",
    "import scanpy as sc\n",
    "\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, index = X_train.index, columns = X_train.columns)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, index = X_test.index, columns = X_test.columns)\n",
    "\n",
    "# at least some feature selection is needed... \n",
    "\n",
    "adata = anndata.AnnData(X_train_scaled)\n",
    "\n",
    "sc.pp.highly_variable_genes( #################\n",
    "    adata, \n",
    "    n_top_genes=3000, \n",
    "    min_mean=0.0125, \n",
    "    max_mean=3, \n",
    "    min_disp=0.5\n",
    ")\n",
    "\n",
    "adata_hvg = adata[:, adata.var['highly_variable']]\n",
    "\n",
    "X_train_scaled_fc = pd.DataFrame(\n",
    "    adata_hvg.X, \n",
    "    index=X_train_scaled.index, \n",
    "    columns=adata_hvg.var.index\n",
    ")\n",
    "\n",
    "X_test_scaled_fc = X_test_scaled.loc[:, X_train_scaled_fc.columns]\n",
    "X_test_scaled_fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abdf672d-d7cc-4d41-a56e-926ade23186a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "residuals_list = []\n",
    "\n",
    "param_grid = {\n",
    "    \"n_estimators\": [50, 200, 400],\n",
    "    \"learning_rate\": [0.01, 0.05, 0.1],\n",
    "    \"max_depth\": [3, 5, 7],\n",
    "    # \"subsample\": [0.8, 1.0],\n",
    "    # \"colsample_bytree\": [0.8, 1.0],\n",
    "}\n",
    "\n",
    "lipids = []\n",
    "train_mses = []\n",
    "test_mses = []\n",
    "train_r_values = []\n",
    "test_r_values = []\n",
    "best_params_list = []\n",
    "\n",
    "for lipid in tqdm(y_train.columns):\n",
    "\n",
    "    X_train_sub, X_val_sub, y_train_sub, y_val_sub = train_test_split(\n",
    "        X_train_scaled_fc,\n",
    "        y_train[lipid],\n",
    "        test_size=0.2,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    best_val_mse = float(\"inf\")\n",
    "    best_params = None\n",
    "    best_model = None\n",
    "\n",
    "    subsample_values = [1.0]\n",
    "    colsample_values = [1.0]\n",
    "\n",
    "    for n_estimators in param_grid[\"n_estimators\"]:\n",
    "        for learning_rate in param_grid[\"learning_rate\"]:\n",
    "            for max_depth in param_grid[\"max_depth\"]:\n",
    "                for subsample in subsample_values:\n",
    "                    for colsample_bytree in colsample_values:\n",
    "                        \n",
    "                        xgb_regressor = xgb.XGBRegressor(\n",
    "                            objective='reg:squarederror',\n",
    "                            n_estimators=n_estimators,\n",
    "                            learning_rate=learning_rate,\n",
    "                            max_depth=max_depth,\n",
    "                            subsample=subsample,\n",
    "                            colsample_bytree=colsample_bytree,\n",
    "                            random_state=42,\n",
    "                            n_jobs=8\n",
    "                        )\n",
    "\n",
    "                        xgb_regressor.fit(X_train_sub, y_train_sub)\n",
    "                        val_pred = xgb_regressor.predict(X_val_sub)\n",
    "                        val_mse = mean_squared_error(y_val_sub, val_pred)\n",
    "\n",
    "                        if val_mse < best_val_mse:\n",
    "                            best_val_mse = val_mse\n",
    "                            best_params = {\n",
    "                                \"n_estimators\": n_estimators,\n",
    "                                \"learning_rate\": learning_rate,\n",
    "                                \"max_depth\": max_depth,\n",
    "                                \"subsample\": subsample,\n",
    "                                \"colsample_bytree\": colsample_bytree,\n",
    "                            }\n",
    "                            best_model = xgb_regressor\n",
    "\n",
    "    print(f\"Best val MSE for {lipid}: {best_val_mse}\")\n",
    "    print(f\"Best params for {lipid}: {best_params}\")\n",
    "\n",
    "    xgb_regressor_final = xgb.XGBRegressor(\n",
    "        objective='reg:squarederror',\n",
    "        **best_params,  \n",
    "        random_state=42,\n",
    "        n_jobs=8\n",
    "    )\n",
    "    xgb_regressor_final.fit(X_train_scaled_fc, y_train[lipid])\n",
    "\n",
    "    y_train_pred = xgb_regressor_final.predict(X_train_scaled_fc)\n",
    "    y_test_pred = xgb_regressor_final.predict(X_test_scaled_fc)\n",
    "    train_mse = mean_squared_error(y_train[lipid], y_train_pred)\n",
    "    test_mse = mean_squared_error(y_test[lipid], y_test_pred)\n",
    "    r_train, _ = pearsonr(y_train[lipid], y_train_pred)\n",
    "    r_test, _ = pearsonr(y_test[lipid], y_test_pred)\n",
    "    lipids.append(lipid)\n",
    "    train_mses.append(train_mse)\n",
    "    test_mses.append(test_mse)\n",
    "    train_r_values.append(r_train)\n",
    "    test_r_values.append(r_test)\n",
    "    best_params_list.append(best_params)\n",
    "\n",
    "    train_residuals = y_train[lipid] - y_train_pred\n",
    "    test_residuals = y_test[lipid] - y_test_pred\n",
    "    train_residuals_df = pd.DataFrame({\"lipid\": lipid, \"residual\": train_residuals})\n",
    "    train_residuals_df[\"set\"] = \"train\"\n",
    "    test_residuals_df = pd.DataFrame({\"lipid\": lipid, \"residual\": test_residuals})\n",
    "    test_residuals_df[\"set\"] = \"test\"\n",
    "    residuals_list.append(train_residuals_df)\n",
    "    residuals_list.append(test_residuals_df)\n",
    "\n",
    "    joblib.dump(xgb_regressor_final, f\"{lipid}_best_xgb_model.pkl\")\n",
    "\n",
    "pearson_df = pd.DataFrame({\n",
    "    \"Lipid\": lipids,\n",
    "    \"Train_MSE\": train_mses,\n",
    "    \"Test_MSE\": test_mses,\n",
    "    \"Train_Pearson_R\": train_r_values,\n",
    "    \"Test_Pearson_R\": test_r_values,\n",
    "    \"Best_Params\": best_params_list\n",
    "})\n",
    "\n",
    "#pearson_df.to_csv(\"xgb_genestolipids.csv\", index=False)\n",
    "\n",
    "plt.hist(pearson_df['Test_Pearson_R'], bins=20, color=\"darkred\", alpha=0.8)\n",
    "plt.hist(pearson_df['Train_Pearson_R'], bins=20, color=\"black\", alpha=0.8)\n",
    "plt.gca().spines['top'].set_visible(False)\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "plt.gca().spines['left'].set_visible(False)\n",
    "plt.gca().yaxis.set_ticks([])\n",
    "plt.gca().xaxis.set_ticks_position('bottom')\n",
    "plt.show()\n",
    "\n",
    "pearson_df.sort_values('Test_Pearson_R')[:20]\n",
    "\n",
    "residuals_df = pd.concat(residuals_list)\n",
    "adata = anndata.AnnData(\n",
    "    X=residuals_df[[\"residual\"]].values,\n",
    "    obs=residuals_df[[\"lipid\", \"set\"]].copy(),\n",
    "    var=pd.DataFrame(index=[\"residual\"])\n",
    ")\n",
    "#adata.write_h5ad(\"residuals.h5ad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3736bc32-00ed-452a-94f6-34242c087b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(pearson_df['Test_Pearson_R'], bins=20, color=\"darkred\", alpha=0.8)\n",
    "plt.hist(pearson_df['Train_Pearson_R'], bins=20, color=\"black\", alpha=0.8)\n",
    "plt.gca().spines['top'].set_visible(False)\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "plt.gca().spines['left'].set_visible(False)\n",
    "plt.gca().yaxis.set_ticks([])\n",
    "plt.gca().xaxis.set_ticks_position('bottom')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874e92a1-6dd1-4ae6-a4ff-b888fc84e695",
   "metadata": {},
   "outputs": [],
   "source": [
    "difficult = pearson_df.sort_values(by=\"Test_Pearson_R\")\n",
    "difficult = difficult.loc[difficult[\"Test_Pearson_R\"] < 0.34,:]\n",
    "print(difficult['Lipid'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e4ad33-48b9-4a86-a36e-e9a6f34c7ded",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ff23ec8c-4d24-43c9-99d2-4fba8a04bcf1",
   "metadata": {},
   "source": [
    "## Characterize the hard-to-predict lipids: tests to assess if any class, chain length, or insaturation shift occurs in a given lipid set (on elastic net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e44bf3-aa08-4c83-b84a-21bc5d2c5f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "THR = 0.4\n",
    "hardtopredict = pearson_df.sort_values('Test_Pearson_R')['Lipid'][pearson_df.sort_values('Test_Pearson_R')['Test_Pearson_R'] < THR]\n",
    "\n",
    "import re\n",
    "\n",
    "df = pd.DataFrame(pearson_df['Lipid']).fillna('')\n",
    "df.columns = [\"lipid_name\"]\n",
    "\n",
    "# extract the \"class\" etc from the lipid_name\n",
    "df[\"class\"] = df[\"lipid_name\"].apply(lambda x: re.split(' |\\(', x)[0])\n",
    "df[\"carbons\"] = df[\"lipid_name\"].apply(lambda x: int(re.search(r'(\\d+):', x).group(1)) if re.search(r'(\\d+):', x) else np.nan)\n",
    "df[\"insaturations\"] = df[\"lipid_name\"].apply(lambda x: int(re.search(r':(\\d+)', x).group(1)) if re.search(r':(\\d+)', x) else np.nan)\n",
    "df[\"insaturations_per_Catom\"] = df[\"insaturations\"] / df[\"carbons\"]\n",
    "\n",
    "df[\"broken\"] = df[\"lipid_name\"].str.endswith('_uncertain')\n",
    "df.loc[df[\"broken\"], 'carbons'] = np.nan\n",
    "df.loc[df[\"broken\"], 'class'] = np.nan\n",
    "df.loc[df[\"broken\"], 'insaturations'] = np.nan\n",
    "df.loc[df[\"broken\"], 'insaturations_per_Catom'] = np.nan\n",
    "df.loc[df[\"broken\"], 'color'] = \"gray\"\n",
    "\n",
    "df.index = df['lipid_name']\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "test = df.loc[hardtopredict,:]\n",
    "nontest = df.loc[~df.index.isin(hardtopredict),:]\n",
    "\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62471c38-306f-40e8-a25e-9dffc2239ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def permutation_test_categorical(\n",
    "    test_labels, \n",
    "    other_labels, \n",
    "    n_permutations=10_000, \n",
    "    alternative='two-sided', \n",
    "    random_state=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Perform a permutation test to assess whether each category in test_labels \n",
    "    is over- or under-represented compared to what we would expect by chance.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    test_labels : 1D array-like of categorical labels (the \"test\" set)\n",
    "    other_labels : 1D array-like of categorical labels (all non-test elements)\n",
    "    n_permutations : int, optional\n",
    "        Number of random permutations\n",
    "    alternative : {'two-sided', 'greater', 'less'}, optional\n",
    "        - 'two-sided': tests if the proportion differs in either direction\n",
    "        - 'greater': tests if test_labels has a higher proportion of the category\n",
    "        - 'less': tests if test_labels has a lower proportion of the category\n",
    "    random_state : int, optional\n",
    "        If provided, sets the random seed for reproducibility\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    results : pd.DataFrame\n",
    "        A DataFrame with columns: 'category', 'observed_count', 'expected_count',\n",
    "        'observed_proportion', 'expected_proportion', 'p_value'\n",
    "    \"\"\"\n",
    "    if random_state is not None:\n",
    "        np.random.seed(random_state)\n",
    "    \n",
    "    test_labels = np.array(test_labels)\n",
    "    other_labels = np.array(other_labels)\n",
    "    all_labels = np.concatenate([test_labels, other_labels])\n",
    "    n_test = len(test_labels)\n",
    "    unique_categories = np.unique(all_labels)\n",
    "    total_counts = {cat: np.sum(all_labels == cat) for cat in unique_categories}\n",
    "    expected_props = {cat: count/len(all_labels) for cat, count in total_counts.items()}\n",
    "    observed_counts = {cat: np.sum(test_labels == cat) for cat in unique_categories}\n",
    "    observed_props = {cat: count/n_test for cat, count in observed_counts.items()}\n",
    "    perm_counts = {cat: np.zeros(n_permutations) for cat in unique_categories}\n",
    "\n",
    "    for i in range(n_permutations):\n",
    "        np.random.shuffle(all_labels)\n",
    "        perm_test = all_labels[:n_test]\n",
    "        for cat in unique_categories:\n",
    "            perm_counts[cat][i] = np.sum(perm_test == cat)\n",
    "    \n",
    "    results = []\n",
    "    for cat in unique_categories:\n",
    "        observed = observed_counts[cat]\n",
    "        distribution = perm_counts[cat]\n",
    "        expected = expected_props[cat] * n_test\n",
    "        \n",
    "        if alternative == 'two-sided':\n",
    "            observed_dev = abs(observed - expected)\n",
    "            p_value = np.mean(abs(distribution - expected) >= observed_dev)\n",
    "        \n",
    "        elif alternative == 'greater':\n",
    "            p_value = np.mean(distribution >= observed)\n",
    "        \n",
    "        elif alternative == 'less':\n",
    "            p_value = np.mean(distribution <= observed)\n",
    "        \n",
    "        results.append({\n",
    "            'category': cat,\n",
    "            'observed_count': observed,\n",
    "            'expected_count': expected,\n",
    "            'observed_proportion': observed_props[cat],\n",
    "            'expected_proportion': expected_props[cat],\n",
    "            'p_value': p_value\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "class_enrichments = permutation_test_categorical(\n",
    "    test['class'].values, nontest['class'].values, \n",
    "    n_permutations=5000, \n",
    "    alternative='two-sided', \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "class_enrichments # no class seems to be statistically differentially represented in this set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6587a8-d57a-4b52-9eb6-455be8c78b84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8c9b5dcf-cf6f-4ccc-a8dd-f5fbcf1991e7",
   "metadata": {},
   "source": [
    "## Stratification: (lipid) metabolism genes, transporters, TFs, cellular localization, cell-cell communication, NTs"
   ]
  },
  {
   "cell_type": "raw",
   "id": "32157877-6820-4772-a0c9-e7265add5c8b",
   "metadata": {},
   "source": [
    "# WIKIPATHWAYS\n",
    "\n",
    "phospholipidmet = pd.read_csv(\"WP4345-datanodes_PHOSPHO.tsv\", sep=\"\\t\")\n",
    "phospholipidmet = phospholipidmet.loc[(phospholipidmet['Type'] == \"GeneProduct\") | (phospholipidmet['Type'] == \"Protein\"), 'Label'].values\n",
    "\n",
    "sphingolipidmet = pd.read_csv(\"WP4690-datanodes_SPHINGO.tsv\", sep=\"\\t\")\n",
    "sphingolipidmet = sphingolipidmet.loc[(sphingolipidmet['Type'] == \"GeneProduct\") | (sphingolipidmet['Type'] == \"Protein\"), 'Label'].values\n",
    "\n",
    "coxmet = pd.read_csv(\"WP4347-datanodes_COX.tsv\", sep=\"\\t\")\n",
    "coxmet = coxmet.loc[(coxmet['Type'] == \"GeneProduct\") | (coxmet['Type'] == \"Protein\"), 'Label'].values\n",
    "\n",
    "cholesterolmet = pd.read_csv(\"WP4346-datanodes_CHOLESTEROL.tsv\", sep=\"\\t\")\n",
    "cholesterolmet = cholesterolmet.loc[(cholesterolmet['Type'] == \"GeneProduct\") | (cholesterolmet['Type'] == \"Protein\"), 'Label'].values\n",
    "\n",
    "# GENE ONTOLOGY\n",
    "\n",
    "catalytic = pd.read_csv(\"CATALYTICACTIVITY_GO_term_summary_20241227_062949.txt\", sep=\"\\t\")\n",
    "catalytic = catalytic['MGI Gene/Marker ID'].unique()\n",
    "print(len(catalytic)) # (oh wow)\n",
    "\n",
    "metabproc = pd.read_csv(\"METABOLICPROCESS_GO_term_summary_20241227_062641.txt\", sep=\"\\t\")\n",
    "metabproc = metabproc['MGI Gene/Marker ID'].unique()\n",
    "\n",
    "cellcomm = pd.read_csv(\"CELLCOMMUNICATION_GO_term_summary_20241227_062732.txt\", sep=\"\\t\")\n",
    "cellcomm = cellcomm['MGI Gene/Marker ID'].unique()\n",
    "\n",
    "transportact = pd.read_csv(\"TRANSPORTERACTIVITY_GO_term_summary_20241227_062925.txt\", sep=\"\\t\")\n",
    "transportact = transportact['MGI Gene/Marker ID'].unique()\n",
    "\n",
    "transcreg = pd.read_csv(\"TRANSCRIPTIONREGULATORACTIVITY_GO_term_summary_20241227_062830.txt\", sep=\"\\t\")\n",
    "transcreg = transcreg['MGI Gene/Marker ID'].unique()\n",
    "\n",
    "lipidtransp = pd.read_csv(\"LIPIDTRANSPORTERACTIVITY_GO_term_summary_20241227_062856.txt\", sep=\"\\t\")\n",
    "lipidtransp = lipidtransp['MGI Gene/Marker ID'].unique()\n",
    "\n",
    "ensheatneuro = pd.read_csv(\"ENSHEATMENTOFNEURONS_GO_term_summary_20241227_062755.txt\", sep=\"\\t\")\n",
    "ensheatneuro = ensheatneuro['MGI Gene/Marker ID'].unique()\n",
    "\n",
    "synapticves = pd.read_csv(\"SYNAPTICVESICLE_GO_term_summary_20241227_062813.txt\", sep=\"\\t\")\n",
    "synapticves = synapticves['MGI Gene/Marker ID'].unique()\n",
    "\n",
    "# NEUROTRANSMITTER TRANSPORTERS AND TRANSCRIPTION FACTORS - other databases\n",
    "\n",
    "neurotra = pd.read_csv(\"GOBP_NEUROTRANSMITTER_TRANSPORT.v2024.1.Mm.tsv\", sep=\"\\t\", index_col=0).loc['GENE_SYMBOLS',:].values[0]\n",
    "neurotra = [gene.strip() for gene in neurotra.split(',')]\n",
    "\n",
    "transcriptionfactors = pd.read_csv(\"TFC2_16102023b.tsv\", sep=\"\\t\")\n",
    "transcriptionfactors = transcriptionfactors['Associated.Gene.Name'].values\n",
    "transcriptionfactors ## care on human vs mouse (?)\n",
    "\n",
    "# start prep a (human) subcellular loc., would be nicer with a mouse db as below... currently undone\n",
    "\n",
    "subloc = pd.read_csv(\"subcellular_location.tsv\", sep=\"\\t\")\n",
    "\n",
    "# SOURCES:\n",
    "# this one currently down http://locate.imb.uq.edu.au SUBCELLULAR LOCALIZ\n",
    "# https://www.tfcheckpoint.org/\n",
    "# https://www.informatics.jax.org/go/term/GO:0008152\n",
    "# https://www.informatics.jax.org/go/term/GO:0007154\n",
    "# https://www.informatics.jax.org/go/term/GO:0007272\n",
    "# https://www.informatics.jax.org/go/term/GO:0048489\n",
    "# https://www.informatics.jax.org/go/term/GO:0140110\n",
    "# https://www.informatics.jax.org/go/term/GO:0005319\n",
    "# https://www.informatics.jax.org/go/term/GO:0005215\n",
    "# https://www.informatics.jax.org/go/term/GO:0003824\n",
    "# https://www.gsea-msigdb.org/gsea/msigdb/mouse/geneset/GOBP_NEUROTRANSMITTER_TRANSPORT"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7702c48f-9f27-4d9c-9eea-625970830a29",
   "metadata": {},
   "source": [
    "Could be more stringent on neurotransmitter genes...\n",
    "\n",
    "Glutamatergic (Glut): Slc17a6 (also known as Vglut2), Slc17a7 (Vglut1) or Slc17a8 (Vglut3).\n",
    "GABAergic (GABA): (Slc32a1 (Vgat) or Slc18a2 (Vmat2)) and (Gad1, Gad2 or Aldh1a1).\n",
    "Glycinergic (Glyc): Slc6a5.\n",
    "Cholinergic (Chol): Slc18a3 (Vacht) and Chat.\n",
    "Dopaminergic (Dopa): (Slc6a3 (Dat) or Slc18a2) and (Th and Ddc).\n",
    "Serotonergic (Sero): (Slc6a4 (Sert) or Slc18a2) and (Tph2 and Ddc).\n",
    "Noradrenergic (Nora): (Slc6a2 (Net) or Slc18a2) and Dbh.\n",
    "Histaminergic (Hist): Slc18a2 and Hdc."
   ]
  },
  {
   "cell_type": "raw",
   "id": "d3b7cca7-50a9-46f6-a438-ef24b2332082",
   "metadata": {},
   "source": [
    "X_train_scaled = pd.DataFrame(X_train_scaled, index=X_train.index, columns = X_train.columns)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, index=X_test.index, columns = X_test.columns)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "aa893f39-fe42-4440-a8d4-a2ca9ae4d3fc",
   "metadata": {},
   "source": [
    "columnssubsets = [\"phospholipidmet\", \"sphingolipidmet\", \"coxmet\", \"cholesterolmet\", \"catalytic\", \"metabproc\", \"cellcomm\", \"transportact\", \"transcreg\", \"lipidtransp\", \"ensheatneuro\", \"synapticves\", \"neurotra\", \"transcriptionfactors\"]\n",
    "\n",
    "subs = []\n",
    "for i in range(14):\n",
    "    sub = pd.read_csv(\"subset\"+str(i)+\".csv\", index_col=0)\n",
    "    sub = sub[['Lipid', 'Test_Pearson_R']]\n",
    "    sub.index = sub['Lipid']\n",
    "    sub = sub.iloc[:,1:]\n",
    "    sub.columns = [columnssubsets[i]]\n",
    "    subs.append(sub)\n",
    "    \n",
    "pearsonbystratification = pd.concat(subs, axis=1)\n",
    "pearsonbystratification = pearsonbystratification.fillna(0)\n",
    "pearsonbystratification"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0a097058-7f4e-46af-a13e-bcde223ab930",
   "metadata": {},
   "source": [
    "npara = [len(np.intersect1d(gsnow, X_train_scaled.columns.values)) for gsnow in [phospholipidmet, sphingolipidmet, coxmet, cholesterolmet, catalytic, metabproc, cellcomm, transportact, transcreg, lipidtransp, ensheatneuro, synapticves, neurotra, transcriptionfactors]]\n",
    "npara\n",
    "\n",
    "### recheck the coverages!"
   ]
  },
  {
   "cell_type": "raw",
   "id": "85e3e9e5-2a2a-4876-a571-9ece90f3ab0e",
   "metadata": {},
   "source": [
    "coverages = [(len(np.intersect1d(gsnow, X_train_scaled.columns.values)) / len(gsnow)) for gsnow in [phospholipidmet, sphingolipidmet, coxmet, cholesterolmet, catalytic, metabproc, cellcomm, transportact, transcreg, lipidtransp, ensheatneuro, synapticves, neurotra, transcriptionfactors]]\n",
    "coverages # only the last one (TFs) is not properly formatted"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fe73ef85-336f-4795-b537-2751961d2324",
   "metadata": {},
   "source": [
    "## focus on the \"smaller\" classes + readd cell type markers in a controlled way - we need to say something about them too...\n",
    "celltypes = ct.unique()\n",
    "import numpy as np\n",
    "\n",
    "def extract_markers(celltypes):\n",
    "    markers = set()\n",
    "    \n",
    "    for celltype in celltypes:\n",
    "        # split by first underscore and take everything after it\n",
    "        _, marker_part = celltype.split('_', 1)\n",
    "        \n",
    "        # split the remaining part by underscore\n",
    "        marker_tokens = marker_part.split('_')\n",
    "\n",
    "        for token in marker_tokens:\n",
    "            try:\n",
    "                int(token)\n",
    "            except ValueError:\n",
    "                markers.add(token)\n",
    "    \n",
    "    return sorted(list(markers))\n",
    "\n",
    "markers = extract_markers(celltypes)\n",
    "len(np.unique(markers)) # they are nice but too many! but we can downsample many times to have 300 of them and take the best iteration"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7fc3880f-8b45-4083-9cf8-0485f7027b76",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "source": [
    "from threadpoolctl import threadpool_limits, threadpool_info\n",
    "threadpool_limits(limits=8)\n",
    "import os\n",
    "os.environ['OMP_NUM_THREADS'] = '6'\n",
    "\n",
    "iii = -1\n",
    "rdfs = []\n",
    "all_subsets = [phospholipidmet, \n",
    "               sphingolipidmet, \n",
    "               cholesterolmet, \n",
    "               lipidtransp, \n",
    "               ensheatneuro, \n",
    "               synapticves, \n",
    "               neurotra, \n",
    "               markers]\n",
    "\n",
    "for gsnow in tqdm(all_subsets):\n",
    "    iii += 1\n",
    "\n",
    "    param_grid = {\n",
    "        'alpha': [0.00001, 0.0001, 0.001],\n",
    "        'l1_ratio': [0.1, 0.5, 0.9]\n",
    "    }\n",
    "\n",
    "    # intersect the gene set with the training set and print out to confirm only a minority of genes are lost\n",
    "    genesetnow = np.intersect1d(gsnow, X_train_scaled.columns.values)\n",
    "    print(\"coverage for gene set: \" + str(len(genesetnow)/len(gsnow)))\n",
    "    \n",
    "    if not np.array_equal(gsnow, markers):\n",
    "        X_train_scaled_NOW = X_train_scaled.loc[:, genesetnow]\n",
    "        X_test_scaled_NOW = X_test_scaled.loc[:, genesetnow]\n",
    "\n",
    "        lipids = []\n",
    "        train_mses = []\n",
    "        test_mses = []\n",
    "        train_r_values = []\n",
    "        test_r_values = []\n",
    "        best_alphas = []\n",
    "        best_l1_ratios = []\n",
    "        aics = []\n",
    "        bics = []\n",
    "        adj_r2_trains = []\n",
    "        adj_r2_tests = []\n",
    "\n",
    "        # train a cross-validated elastic net with a grid of hyperparameter experiments per lipid\n",
    "        for lipid in y_train.columns:\n",
    "            y_train_lipid = y_train[lipid]\n",
    "            y_test_lipid = y_test[lipid]\n",
    "\n",
    "            elastic_net = ElasticNet(max_iter=100000, random_state=42)\n",
    "\n",
    "            grid_search = GridSearchCV(\n",
    "                elastic_net, \n",
    "                param_grid, \n",
    "                cv=2, \n",
    "                scoring='neg_mean_squared_error',\n",
    "                n_jobs=8, \n",
    "                verbose=2\n",
    "            )\n",
    "            grid_search.fit(X_train_scaled_NOW, y_train_lipid)\n",
    "\n",
    "            best_model = grid_search.best_estimator_\n",
    "            best_alpha = grid_search.best_params_['alpha']\n",
    "            best_l1_ratio = grid_search.best_params_['l1_ratio']\n",
    "            best_model.fit(X_train_scaled_NOW, y_train_lipid)\n",
    "\n",
    "            y_train_pred = best_model.predict(X_train_scaled_NOW)\n",
    "            y_test_pred = best_model.predict(X_test_scaled_NOW)\n",
    "\n",
    "            # metrics\n",
    "            train_mse = mean_squared_error(y_train_lipid, y_train_pred)\n",
    "            test_mse = mean_squared_error(y_test_lipid, y_test_pred)\n",
    "            r_train, _ = pearsonr(y_train_lipid, y_train_pred)\n",
    "            r_test, _ = pearsonr(y_test_lipid, y_test_pred)\n",
    "\n",
    "            # AIC and BIC\n",
    "            n_train = len(y_train_lipid)  \n",
    "            n_test = len(y_test_lipid)\n",
    "            k = np.sum(best_model.coef_ != 0) ####\n",
    "            sigma_squared = train_mse   #####\n",
    "\n",
    "            log_likelihood = -0.5 * n_train * (np.log(2 * np.pi * sigma_squared) + 1)\n",
    "            aic = 2 * k - 2 * log_likelihood\n",
    "            bic = k * np.log(n_train) - 2 * log_likelihood\n",
    "\n",
    "            # Adjusted R²\n",
    "            r_squared_train = r_train**2\n",
    "            r_squared_test = r_test**2\n",
    "            adj_r2_train = 1 - ((1 - r_squared_train) * (n_train - 1) / (n_train - k - 1))\n",
    "            adj_r2_test = 1 - ((1 - r_squared_test) * (n_test - 1) / (n_test - k - 1))\n",
    "\n",
    "            lipids.append(lipid)\n",
    "            train_mses.append(train_mse)\n",
    "            test_mses.append(test_mse)\n",
    "            train_r_values.append(r_train)\n",
    "            test_r_values.append(r_test)\n",
    "            best_alphas.append(best_alpha)\n",
    "            best_l1_ratios.append(best_l1_ratio)\n",
    "            aics.append(aic)\n",
    "            bics.append(bic)\n",
    "            adj_r2_trains.append(adj_r2_train)\n",
    "            adj_r2_tests.append(adj_r2_test)\n",
    "\n",
    "        pearson_df = pd.DataFrame({\n",
    "            'Lipid': lipids,\n",
    "            'Train_Pearson_R': train_r_values,\n",
    "            'Test_Pearson_R': test_r_values,\n",
    "            'Train_MSE': train_mses,\n",
    "            'Test_MSE': test_mses,\n",
    "            'Best_Alpha': best_alphas,\n",
    "            'Best_L1_Ratio': best_l1_ratios,\n",
    "            'AIC': aics,\n",
    "            'BIC': bics,\n",
    "            'Adjusted_R2_Train': adj_r2_trains,\n",
    "            'Adjusted_R2_Test': adj_r2_tests\n",
    "        })\n",
    "\n",
    "        pearson_df.to_csv(\"subset_controlledstats_\"+str(iii)+\".csv\", index=False)\n",
    "        rdfs.append(pearson_df)\n",
    "\n",
    "    else:\n",
    "        # track the best results and the best sub-features for reasonably-sized markers sets\n",
    "        best_mean_test_r = -np.inf\n",
    "        best_pearson_df = None\n",
    "        best_features_for_all_lipids = None\n",
    "\n",
    "        # do N=10 sub-samplings\n",
    "        for subsample_idx in range(10):\n",
    "            # randomly pick 300 features from the intersection\n",
    "            if len(genesetnow) <= 300:\n",
    "                sampled_features = genesetnow  \n",
    "            else:\n",
    "                sampled_features = np.random.choice(genesetnow, size=300, replace=False)\n",
    "\n",
    "            X_train_scaled_NOW = X_train_scaled.loc[:, sampled_features]\n",
    "            X_test_scaled_NOW = X_test_scaled.loc[:, sampled_features]\n",
    "\n",
    "            lipids = []\n",
    "            train_mses = []\n",
    "            test_mses = []\n",
    "            train_r_values = []\n",
    "            test_r_values = []\n",
    "            best_alphas = []\n",
    "            best_l1_ratios = []\n",
    "            aics = []\n",
    "            bics = []\n",
    "            adj_r2_trains = []\n",
    "            adj_r2_tests = []\n",
    "\n",
    "            # same model-fitting loop as before\n",
    "            for lipid in y_train.columns:\n",
    "                y_train_lipid = y_train[lipid]\n",
    "                y_test_lipid = y_test[lipid]\n",
    "\n",
    "                elastic_net = ElasticNet(max_iter=100000, random_state=42)\n",
    "\n",
    "                grid_search = GridSearchCV(\n",
    "                    elastic_net, \n",
    "                    param_grid, \n",
    "                    cv=2, \n",
    "                    scoring='neg_mean_squared_error',\n",
    "                    n_jobs=8, \n",
    "                    verbose=0 \n",
    "                )\n",
    "                grid_search.fit(X_train_scaled_NOW, y_train_lipid)\n",
    "\n",
    "                best_model = grid_search.best_estimator_\n",
    "                best_alpha = grid_search.best_params_['alpha']\n",
    "                best_l1_ratio = grid_search.best_params_['l1_ratio']\n",
    "                best_model.fit(X_train_scaled_NOW, y_train_lipid)\n",
    "                y_train_pred = best_model.predict(X_train_scaled_NOW)\n",
    "                y_test_pred = best_model.predict(X_test_scaled_NOW)\n",
    "                train_mse = mean_squared_error(y_train_lipid, y_train_pred)\n",
    "                test_mse = mean_squared_error(y_test_lipid, y_test_pred)\n",
    "                r_train, _ = pearsonr(y_train_lipid, y_train_pred)\n",
    "                r_test, _ = pearsonr(y_test_lipid, y_test_pred)\n",
    "                n_train = len(y_train_lipid)\n",
    "                n_test = len(y_test_lipid)\n",
    "                k = np.sum(best_model.coef_ != 0)\n",
    "                sigma_squared = train_mse\n",
    "                log_likelihood = -0.5 * n_train * (np.log(2 * np.pi * sigma_squared) + 1)\n",
    "                aic = 2 * k - 2 * log_likelihood\n",
    "                bic = k * np.log(n_train) - 2 * log_likelihood\n",
    "                r_squared_train = r_train**2\n",
    "                r_squared_test = r_test**2\n",
    "                adj_r2_train = 1 - ((1 - r_squared_train) * (n_train - 1) / (n_train - k - 1))\n",
    "                adj_r2_test = 1 - ((1 - r_squared_test) * (n_test - 1) / (n_test - k - 1))\n",
    "\n",
    "                lipids.append(lipid)\n",
    "                train_mses.append(train_mse)\n",
    "                test_mses.append(test_mse)\n",
    "                train_r_values.append(r_train)\n",
    "                test_r_values.append(r_test)\n",
    "                best_alphas.append(best_alpha)\n",
    "                best_l1_ratios.append(best_l1_ratio)\n",
    "                aics.append(aic)\n",
    "                bics.append(bic)\n",
    "                adj_r2_trains.append(adj_r2_train)\n",
    "                adj_r2_tests.append(adj_r2_test)\n",
    "\n",
    "            pearson_df_sub = pd.DataFrame({\n",
    "                'Lipid': lipids,\n",
    "                'Train_Pearson_R': train_r_values,\n",
    "                'Test_Pearson_R': test_r_values,\n",
    "                'Train_MSE': train_mses,\n",
    "                'Test_MSE': test_mses,\n",
    "                'Best_Alpha': best_alphas,\n",
    "                'Best_L1_Ratio': best_l1_ratios,\n",
    "                'AIC': aics,\n",
    "                'BIC': bics,\n",
    "                'Adjusted_R2_Train': adj_r2_trains,\n",
    "                'Adjusted_R2_Test': adj_r2_tests\n",
    "            })\n",
    "\n",
    "            mean_test_r = pearson_df_sub['Test_Pearson_R'].mean()\n",
    "\n",
    "            if mean_test_r > best_mean_test_r:\n",
    "                best_mean_test_r = mean_test_r\n",
    "                best_pearson_df = pearson_df_sub.copy()\n",
    "                best_features_for_all_lipids = sampled_features\n",
    "\n",
    "        # keep only the best subsampling for the cell type markers set\n",
    "        best_pearson_df['Used_Features'] = [\",\".join(best_features_for_all_lipids)] * len(best_pearson_df)\n",
    "        best_pearson_df.to_csv(\"subset_controlledstats_\"+str(iii)+\".csv\", index=False)\n",
    "        rdfs.append(best_pearson_df)\n",
    "\n",
    "import pickle\n",
    "pickle_filename = 'rdfs_controlledstats_elastic_stratification.pkl'\n",
    "with open(pickle_filename, 'wb') as file:\n",
    "    pickle.dump(rdfs, file)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fff64ede-f36b-444b-a9ea-ed51d1f3ddbb",
   "metadata": {},
   "source": [
    "columnssubsets = [\"phospholipidmet\", \"sphingolipidmet\", \"cholesterolmet\", \"lipidtransp\", \"ensheatneuro\", \"synapticves\", \"neurotra\", \"markers\"]\n",
    "\n",
    "subs = []\n",
    "for i in range(8):\n",
    "    sub = pd.read_csv(\"subset_controlledstats_\"+str(i)+\".csv\", index_col=0)\n",
    "    sub = sub[['AIC']]\n",
    "    sub.columns = [columnssubsets[i]]\n",
    "    subs.append(sub)\n",
    "    \n",
    "# best AIC = smaller, make it a score by negativizing\n",
    "aicbystratification = -pd.concat(subs, axis=1)\n",
    "\n",
    "# meaningful AIC differences have a delta at least 2, so remove the average AIC across models per lipid to center and rank by score\n",
    "aicbystratification = (aicbystratification.T - aicbystratification.T.mean()).T\n",
    "normalized_df = aicbystratification\n",
    "medians = normalized_df.median()\n",
    "sorted_columns = medians.sort_values().index"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5666c743-e603-478d-8dea-4ea62ff52fb3",
   "metadata": {},
   "source": [
    "# check the residuals of a model as an example...\n",
    "residuals = y_train_pred - y_train_lipid\n",
    "plt.scatter(y_train_pred, residuals, alpha=0.5)\n",
    "plt.axhline(y=0, color='r', linestyle='--')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8830a72a-9bc8-420b-97f7-f23acc043e05",
   "metadata": {},
   "source": [
    "plt.figure(figsize=(5, 5))\n",
    "\n",
    "sns.boxplot(\n",
    "    data=aicbystratification[sorted_columns],\n",
    "    color='lightgray',\n",
    "    boxprops=dict(alpha=0.5, linewidth=1.5),  # Transparent boxes, tunable border thickness\n",
    "    whiskerprops=dict(linewidth=1.5),\n",
    "    capprops=dict(linewidth=1.5),\n",
    "    medianprops=dict(linewidth=1.5),\n",
    "    flierprops=dict(marker='o', markersize=3, linestyle='none', markeredgewidth=0.5)  # Smaller, thinner outliers\n",
    ")\n",
    "\n",
    "x_coords = np.arange(len(sorted_columns))\n",
    "for idx in normalized_df.index:\n",
    "    row_values = normalized_df[sorted_columns].loc[idx]\n",
    "    plt.plot(x_coords, row_values, \n",
    "             color='gray',         \n",
    "             alpha=0.075,           \n",
    "             linewidth=1,         \n",
    "             zorder=1)              \n",
    "\n",
    "plt.xticks(range(len(sorted_columns)), sorted_columns, rotation=45, ha='right')\n",
    "plt.xlabel('Predictor gene set')\n",
    "plt.ylabel('Negative ΔAIC')\n",
    "plt.gca().spines['top'].set_visible(False)\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b3c9d2ec-adec-4445-9e30-40b8a1d3c529",
   "metadata": {},
   "source": [
    "# also do it using the BIC\n",
    "\n",
    "columnssubsets = [\"phospholipidmet\", \"sphingolipidmet\", \"cholesterolmet\", \"lipidtransp\", \"ensheatneuro\", \"synapticves\", \"neurotra\", \"markers\"]\n",
    "\n",
    "subs = []\n",
    "for i in range(8):\n",
    "    sub = pd.read_csv(\"subset_controlledstats_\"+str(i)+\".csv\", index_col=0)\n",
    "    sub = sub[['BIC']]\n",
    "    sub.columns = [columnssubsets[i]]\n",
    "    subs.append(sub)\n",
    "    \n",
    "# best BIC = smaller, make it a score by negativizing\n",
    "bicbystratification = -pd.concat(subs, axis=1)\n",
    "\n",
    "# meaningful BIC differences have a delta at least 2, so remove the average BIC across models per lipid to center and rank by score\n",
    "bicbystratification = (bicbystratification.T - bicbystratification.T.mean()).T\n",
    "normalized_df = bicbystratification\n",
    "medians = normalized_df.median()\n",
    "sorted_columns = medians.sort_values().index\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "\n",
    "sns.boxplot(\n",
    "    data=normalized_df[sorted_columns],\n",
    "    color='lightgray',\n",
    "    boxprops=dict(alpha=0.5, linewidth=1.5),  # Transparent boxes, tunable border thickness\n",
    "    whiskerprops=dict(linewidth=1.5),\n",
    "    capprops=dict(linewidth=1.5),\n",
    "    medianprops=dict(linewidth=1.5),\n",
    "    flierprops=dict(marker='o', markersize=3, linestyle='none', markeredgewidth=0.5)  # Smaller, thinner outliers\n",
    ")\n",
    "\n",
    "x_coords = np.arange(len(sorted_columns))\n",
    "for idx in normalized_df.index:\n",
    "    row_values = normalized_df[sorted_columns].loc[idx]\n",
    "    plt.plot(x_coords, row_values, \n",
    "             color='gray',         \n",
    "             alpha=0.075,           \n",
    "             linewidth=1,         \n",
    "             zorder=1)              \n",
    "\n",
    "plt.xticks(range(len(sorted_columns)), sorted_columns, rotation=45, ha='right')\n",
    "plt.xlabel('Predictor gene set')\n",
    "plt.ylabel('Negative BIC')\n",
    "plt.gca().spines['top'].set_visible(False)\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# remember that BIC penalizes complexity more strongly\n",
    "# also we  believe there is no true model, we want a model that provides the best predictive accuracy\n",
    "# AIC is probably to be preferred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14217fd6-d0ed-4a66-9412-bb8cc5df0e3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "62b0abd6-b27f-499f-83c0-186a6af57c3c",
   "metadata": {},
   "source": [
    "## Can we impaint genes from lipidomic measurements using the lipizones? Assess predictability first, spatial patterns next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b35b974-fc6a-4ed6-8f3b-a4088d53d7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_hdf(\"./zenodo/multimodal/lipicent.h5ad\", key=\"table\")\n",
    "y = pd.read_hdf(\"./zenodo/multimodal/genecent.h5ad\", key=\"table\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.5, random_state=42 # !!! make a larger test set since some lipizone centroids are almost redundant given how we define a lipizone and we want to prevent the model from trivially memorizing\n",
    ")\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "pca = PCA(n_components=0.99, random_state=42) # we'll use a PCA capturing most variance of lipids and look at the PC loadings to bypass multicollinearity\n",
    "X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "X_test_pca = pca.transform(X_test_scaled)\n",
    "print(f\"Original number of features: {X_train.shape[1]}\")\n",
    "print(f\"Reduced number of principal components: {X_train_pca.shape[1]}\")\n",
    "\n",
    "linear_regressor = LinearRegression()\n",
    "\n",
    "multi_output_regressor = MultiOutputRegressor(linear_regressor, n_jobs=8)\n",
    "multi_output_regressor.fit(X_train_pca, y_train)\n",
    "print(\"Linear Regression MultiOutputRegressor training complete.\")\n",
    "\n",
    "y_train_pred = multi_output_regressor.predict(X_train_pca)\n",
    "y_test_pred = multi_output_regressor.predict(X_test_pca)\n",
    "\n",
    "train_mse = mean_squared_error(y_train, y_train_pred, multioutput='raw_values')\n",
    "test_mse = mean_squared_error(y_test, y_test_pred, multioutput='raw_values')\n",
    "\n",
    "train_r = []\n",
    "test_r = []\n",
    "\n",
    "for i, lipid in enumerate(y_train.columns):\n",
    "    r_train, _ = pearsonr(y_train[lipid], y_train_pred[:, i])\n",
    "    train_r.append(r_train)\n",
    "    r_test, _ = pearsonr(y_test[lipid], y_test_pred[:, i])\n",
    "    test_r.append(r_test)\n",
    "\n",
    "# NOTE: ONCE LIPIZONES ARE DEFINED, IT IS VERY EASY TO PREDICT GENES FROM LIPIDS\n",
    "pearson_df = pd.DataFrame({\n",
    "    'Lipid': y_train.columns,\n",
    "    'Train_Pearson_R': train_r,\n",
    "    'Test_Pearson_R': test_r,\n",
    "    'Train_MSE': train_mse,\n",
    "    'Test_MSE': test_mse\n",
    "})\n",
    "\n",
    "plt.hist(pearson_df['Test_Pearson_R'], bins=20, color=\"darkred\", alpha=0.8)\n",
    "plt.hist(pearson_df['Train_Pearson_R'], bins=20, color=\"black\", alpha=0.8)\n",
    "plt.gca().spines['top'].set_visible(False)\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "plt.gca().xaxis.set_ticks_position('bottom')\n",
    "plt.show()\n",
    "\n",
    "pearson_df.sort_values('Test_Pearson_R')[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60215ace-409a-404c-a5f5-4ce5fd93dec7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "552e52ae-7373-46fd-bbb9-52faae46ee4a",
   "metadata": {},
   "source": [
    "## Study enzyme-lipid correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4611b1-8ba2-4494-8a28-b7d0c077f5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_premanannot = pd.read_csv(\"/data/luca/lipidatlas/ManuscriptGithub/zenodo/csv/corereactions_wenzymes.csv\")\n",
    "\n",
    "corronz = pd.read_hdf(\"/data/luca/lipidatlas/ManuscriptGithub/zenodo/csv/allvsallcorr.h5ad\", key=\"table\")\n",
    "\n",
    "expanded_reamat = filtered_premanannot[[\"reagent\", \"product\", \"enzyme\"]]\n",
    "def convert_to_tuple(text):\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    \n",
    "    if text.startswith('(') and text.endswith(')'):\n",
    "        text = text[1:-1]\n",
    "        items = [item.strip().strip(\"'\") for item in text.split(',')]\n",
    "        return tuple(items)\n",
    "    return text\n",
    "\n",
    "expanded_reamat['enzyme'] = expanded_reamat['enzyme'].apply(convert_to_tuple)\n",
    "expanded_reamat = expanded_reamat.explode(\"enzyme\", ignore_index=True)\n",
    "expanded_reamat\n",
    "expanded_reamat = expanded_reamat.loc[expanded_reamat['product'].isin(corronz.index),:]\n",
    "expanded_reamat = expanded_reamat.loc[expanded_reamat['reagent'].isin(corronz.index),:]\n",
    "\n",
    "expanded_reamat = expanded_reamat.loc[expanded_reamat['enzyme'].isin(corronz.index),:]\n",
    "expanded_reamat['corr_enz_prod'] = [corronz.loc[expanded_reamat['product'].iloc[i], np.array(expanded_reamat['enzyme'].iloc[i])] for i in range(expanded_reamat.shape[0])]\n",
    "expanded_reamat['corr_enz_rea'] = [corronz.loc[expanded_reamat['reagent'].iloc[i], np.array(expanded_reamat['enzyme'].iloc[i])] for i in range(expanded_reamat.shape[0])]\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.rcParams['pdf.fonttype'] = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85d0722-4e69-40be-b32a-3568b0cfd42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.rcParams['pdf.fonttype'] = 42\n",
    "\n",
    "# 1) compute medians and get sorted enzyme order\n",
    "medians = expanded_reamat.groupby('enzyme')['corr_enz_prod'].median()\n",
    "order = medians.sort_values().index.tolist()\n",
    "\n",
    "# 2) build list of arrays for each enzyme in that order\n",
    "data = [\n",
    "    expanded_reamat.loc[expanded_reamat['enzyme'] == enz, 'corr_enz_prod']\n",
    "    for enz in order\n",
    "]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# 3) boxplot with patch_artist=True so we can fill\n",
    "bp = plt.boxplot(\n",
    "    data,\n",
    "    labels=order,\n",
    "    patch_artist=True,\n",
    "    boxprops=dict(facecolor='lightgray', edgecolor='none', linewidth=1.5),\n",
    "    whiskerprops=dict(color='lightgray', linewidth=1.5),\n",
    "    capprops=dict(color='lightgray', linewidth=1.5),\n",
    "    flierprops=dict(\n",
    "        marker='o',\n",
    "        markerfacecolor='lightgray',\n",
    "        markeredgecolor='lightgray',\n",
    "        markersize=3,\n",
    "        linestyle='none'\n",
    "    ),\n",
    "    medianprops=dict(color='white', linewidth=1.5)\n",
    ")\n",
    "\n",
    "# 4) dark red dashed line with 50% transparency\n",
    "plt.axhline(0.0, color='darkred', linestyle='--', linewidth=1, alpha=0.5)\n",
    "\n",
    "plt.suptitle(\"\")\n",
    "plt.xlabel(\"Enzyme\")\n",
    "plt.ylabel(\"Correlation enzyme-product\")\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "#plt.savefig(\"correnzprod.pdf\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5dfc01-a844-466f-af6a-08124c6327de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from matplotlib.colors import Normalize\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "matplotlib.rcParams['pdf.fonttype'] = 42\n",
    "\n",
    "# 1) compute medians and get sorted enzyme order\n",
    "medians = expanded_reamat.groupby('enzyme')['corr_enz_prod'].median()\n",
    "order = medians.sort_values().index.tolist()\n",
    "\n",
    "# 2) prepare DataFrame with numeric x positions + jitter\n",
    "df = expanded_reamat.copy()\n",
    "df['enzyme'] = pd.Categorical(df['enzyme'], categories=order, ordered=True)\n",
    "df['x'] = df['enzyme'].cat.codes\n",
    "np.random.seed(0)\n",
    "df['x_jitter'] = df['x'] + np.random.uniform(-0.25, 0.25, size=len(df))\n",
    "\n",
    "# Ensure lipid column exists\n",
    "# Replace 'lipid' with your actual lipid name column if different\n",
    "if 'lipid' not in df.columns and 'product' in df.columns:\n",
    "    df = df.rename(columns={'product': 'lipid'})\n",
    "\n",
    "# 3) set up colormap normalization\n",
    "vmin, vmax = df['corr_enz_prod'].min(), df['corr_enz_prod'].max()\n",
    "norm = Normalize(vmin=vmin, vmax=vmax)\n",
    "cmap = cm.get_cmap('coolwarm')\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# 4) scatter each point colored by its correlation\n",
    "sc = plt.scatter(\n",
    "    df['x_jitter'],\n",
    "    df['corr_enz_prod'],\n",
    "    c=df['corr_enz_prod'],\n",
    "    cmap=cmap,\n",
    "    norm=norm,\n",
    "    s=30,\n",
    "    alpha=0.8,\n",
    "    edgecolors='none'\n",
    ")\n",
    "\n",
    "# 5) add a colorbar\n",
    "cbar = plt.colorbar(sc)\n",
    "cbar.set_label('Correlation enzyme–product')\n",
    "\n",
    "# 6) horizontal zero line\n",
    "plt.axhline(0.0, color='darkred', linestyle='--', linewidth=1, alpha=0.5)\n",
    "\n",
    "# 7) annotate the 3 highest and 3 lowest correlations by lipid name\n",
    "extremes = pd.concat([\n",
    "    df.nlargest(3, 'corr_enz_prod'),\n",
    "    df.nsmallest(3, 'corr_enz_prod')\n",
    "])\n",
    "\n",
    "# 8) annotate an additional 14 random points (excluding extremes)\n",
    "remaining = df.drop(extremes.index)\n",
    "random_pts = remaining.sample(n=14, random_state=42)\n",
    "\n",
    "to_label = pd.concat([extremes, random_pts])\n",
    "\n",
    "for _, row in to_label.iterrows():\n",
    "    label = row['lipid'] if 'lipid' in row else ''\n",
    "    plt.text(\n",
    "        row['x_jitter'],\n",
    "        row['corr_enz_prod'],\n",
    "        f\"{label}\\n{row['corr_enz_prod']:.2f}\",\n",
    "        fontsize=7,\n",
    "        ha='center',\n",
    "        va='bottom' if row['corr_enz_prod'] > 0 else 'top'\n",
    "    )\n",
    "\n",
    "# 9) finalize axes\n",
    "plt.xticks(range(len(order)), order, rotation=45, ha='right')\n",
    "plt.xlabel(\"Enzyme\")\n",
    "plt.ylabel(\"Correlation enzyme–product\")\n",
    "plt.tight_layout()\n",
    "#plt.savefig(\"strip.pdf\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a1c2a4-bc3f-42a0-b2b8-f0e755bb7258",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(expanded_reamat['corr_enz_prod'] > 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e31895d-999a-4cde-9ecc-f593901cb36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "expanded_reamat['corr_enz_prod']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3253e578-6db4-448c-9fc5-8d96c4ee35e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "expanded_reamat['corr_enz_prod'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc59c0a-4402-4e31-9bf8-505823085f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "expanded_reamat['corr_enz_prod'].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5dfacbc-8436-43a5-bef0-2298f415f398",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3435cda3-d077-4892-8f77-f6197a4fb2c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
