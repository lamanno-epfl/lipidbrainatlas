{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9512d1b7-39a4-4c82-8ec9-1bc9e8b5dfbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "matplotlib.rcParams['pdf.fonttype'] = 42\n",
    "import pandas as pd\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "datavignettes = pd.read_parquet(\"./zenodo/maindata_2.parquet\")\n",
    " \n",
    "namingtable = pd.DataFrame({\n",
    "    \"cluster\": [\n",
    "        11111, 11112, 11121, 11122, 11211, 11212, 11221, 11222, 12111, 12112, \n",
    "        12121, 12122, 12211, 12212, 12221, 12222, 21111, 21112, 21120, 21211, \n",
    "        21212, 21221, 21222, 22111, 22112, 22121, 22122, 22211, 22212, 22221, 22222\n",
    "    ],\n",
    "    \"zone\": [\n",
    "        \"Mixed and hindbrain white matter\", \"Core callosal white matter\", \n",
    "        \"Callosal and cerebellar white matter\", \"Ventral white matter\", \n",
    "        \"Boundary white matter\", \"Thalamic and mid/hindbrain white matter\", \n",
    "        \"Mid/hindbrain white matter\", \"Mixed white matter\", \n",
    "        \"Choroid plexus and ventricles\", \"Ventricular linings\", \n",
    "        \"Thalamic and midbrain regions\", \"White and gray matter boundary\", \n",
    "        \"Thalamic mixed gray and white matter\", \"Thalamic mixed gray and white matter #2\", \n",
    "        \"Neuron-rich lateral white matter\", \"Neuron-rich lateral white matter #2\", \n",
    "        \"Pallidum and projections\", \"Cortical layer 4\", \n",
    "        \"Subcortical plate, hippocampus and hypothalamus\", \n",
    "        \"GABA-ergic Purkinje cells of the cerebellum\", \"Cortical layers 2-3 and 4\", \n",
    "        \"Piriform cortex\", \"Cortical layers 1 and 2-3\", \"Cortical layer 5\", \n",
    "        \"Cortical layer 6, dentate gyrus\", \"Striatum, hypothalamus and hippocampus\", \n",
    "        \"Striatum, hypothalamus and hippocampus #2\", \n",
    "        \"Retrosplenial, cortical, cerebellar\", \"Cortical layer 6 and cerebellar Y\", \n",
    "        \"Cerebellar glutamatergic neurons\", \"Cortical layer 6 and thalamic\"\n",
    "    ],\n",
    "    \"color\": [\n",
    "        \"#360064\", \"#980053\", \"#170b3b\", \"#ac2f5c\", \"#2a3f6d\", \"#002657\", \n",
    "        \"#21366b\", \"#3e4b6c\", \"#f75400\", \"#ef633e\", \"#a5d4e6\", \"#6399c6\", \n",
    "        \"#853a00\", \"#edeef4\", \"#fdbf71\", \"#ce710e\", \"#940457\", \"#a2d36c\", \n",
    "        \"#d5edb5\", \"#0065d6\", \"#bcf18b\", \"#a68d68\", \"#79e47e\", \"#2f0097\", \n",
    "        \"#47029f\", \"#7500a8\", \"#d70021\", \"#ca99c9\", \"#d4b9da\", \"#e00085\", \n",
    "        \"#f6f3f8\"\n",
    "    ]\n",
    "})\n",
    "\n",
    "clusters_short = pd.Series([x[:5] for x in datavignettes['lipizone']], name='cluster').astype(int)\n",
    "clusters = clusters_short.to_frame().merge(namingtable, on='cluster', how='left')\n",
    "clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be5003b-9234-4dcb-b43f-4878e6630286",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scanpy as sc\n",
    "import anndata\n",
    "import matplotlib\n",
    "matplotlib.rcParams['pdf.fonttype'] = 42\n",
    "\n",
    "atlas = datavignettes\n",
    "\n",
    "allen = pd.read_csv(\"./zenodo/csv/allenconnectome_ipsi.csv\", index_col=0)  #pd.read_csv(\"allenconnectome_contra.csv\", index_col=0) + \n",
    "\n",
    "# match the Allen acronyms used in the 2014 Oh et al paper\n",
    "import difflib\n",
    "\n",
    "injsitesall = allen.index.dropna().astype(str).values\n",
    "unique_acronyms = datavignettes['acronym'].dropna().astype(str).unique()\n",
    "\n",
    "best_match_dict = {}\n",
    "\n",
    "for unique_acr in unique_acronyms:\n",
    "    if not isinstance(unique_acr, str) or unique_acr.strip() == \"\":\n",
    "        continue \n",
    "    matches = difflib.get_close_matches(unique_acr, injsitesall, n=1)\n",
    "    best_match_dict[unique_acr] = matches[0] if matches else None\n",
    "\n",
    "datavignettes['adaptedacronym'] = datavignettes['acronym'].map(best_match_dict)\n",
    "print(f\"Atlas length: {len(datavignettes)}\")\n",
    "print(f\"Adapted acronym column length: {datavignettes['adaptedacronym'].notna().sum()}\")\n",
    "datavignettes['adaptedacronym'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923db7da-4b85-4ca5-bdb7-44f25c81ca1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "431be35c-23e2-4805-87b1-fc39a2200cfa",
   "metadata": {},
   "source": [
    "## 1 Connectomic strength matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ac0442-188c-408f-9bcf-f957e43c6495",
   "metadata": {},
   "outputs": [],
   "source": [
    "connectome = allen.loc[allen.index.isin(atlas['adaptedacronym'].unique()), allen.index.isin(atlas['adaptedacronym'].unique())]\n",
    "atlas = atlas.loc[atlas['adaptedacronym'].isin(allen.index),:]\n",
    "\n",
    "connectome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c45859-6a4a-43bb-bb93-0c16bb47646b",
   "metadata": {},
   "outputs": [],
   "source": [
    "connectome = connectome + connectome.T # symmetrize\n",
    "connectome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaffe278-96bc-48b3-8b8a-0d0c7ae02542",
   "metadata": {},
   "outputs": [],
   "source": [
    "connectome_values = connectome.values  \n",
    "i_upper = np.triu_indices_from(connectome_values, k=1)\n",
    "values = connectome_values[i_upper]\n",
    "\n",
    "top_5_percentile = np.percentile(values, 95)\n",
    "plt.hist(values, bins=50)\n",
    "plt.axvline(x=top_5_percentile, color='r', linestyle='--', label=f'Top 5% threshold: {top_5_percentile:.4f}')\n",
    "\n",
    "plt.xlabel(\"Value\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Histogram of upper triangle (excluding diagonal)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4152503d-43df-44f0-94c6-a47726933cb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5d6ba496-b300-4fc7-9a80-73fd190c643c",
   "metadata": {},
   "source": [
    "## 2 Physical distance matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953b3519-82df-47ea-a895-4a85a4680c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from allensdk.core.mouse_connectivity_cache import MouseConnectivityCache\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "## use with care!\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "mcc = MouseConnectivityCache(manifest_file='mouse_connectivity_manifest.json')\n",
    "structure_tree = mcc.get_structure_tree()\n",
    "\n",
    "annotation, _ = mcc.get_annotation_volume()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0fc187e-1213-4b3f-a99b-3fb465fc0547",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "annotation = annotation[:,:, :int(annotation.shape[2]/2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a0eb8d-69e8-477c-b0a8-b8ae38d85473",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- Allen Structure Graph Functions ---\n",
    "\n",
    "def download_structure_graph():\n",
    "    \"\"\"\n",
    "    Downloads the Allen Brain Atlas structure graph from the Allen API.\n",
    "    Returns the JSON response as a dictionary.\n",
    "    \"\"\"\n",
    "    url = \"http://api.brain-map.org/api/v2/structure_graph_download/1.json\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(\"Failed to download structure graph; status code \" + str(response.status_code))\n",
    "    return response.json()\n",
    "\n",
    "def find_node_by_acronym(node, acronym):\n",
    "    \"\"\"\n",
    "    Recursively searches for a node with the given acronym in the structure graph.\n",
    "    Returns the node (a dict) if found, or None otherwise.\n",
    "    \"\"\"\n",
    "    if node.get(\"acronym\") == acronym:\n",
    "        return node\n",
    "    for child in node.get(\"children\", []):\n",
    "        found = find_node_by_acronym(child, acronym)\n",
    "        if found is not None:\n",
    "            return found\n",
    "    return None\n",
    "\n",
    "def extract_all_ids(node):\n",
    "    \"\"\"\n",
    "    Recursively extracts and returns a list of all region IDs from the given node and its children.\n",
    "    \"\"\"\n",
    "    ids = [node.get(\"id\")]\n",
    "    for child in node.get(\"children\", []):\n",
    "        ids.extend(extract_all_ids(child))\n",
    "    return ids\n",
    "\n",
    "def get_downstream_ids(acronym, structure_graph):\n",
    "    \"\"\"\n",
    "    Given an acronym and the Allen structure graph (as a dict), finds the node with that acronym\n",
    "    and returns a list of all downstream region IDs (including the node's own ID).\n",
    "    \"\"\"\n",
    "    for node in structure_graph.get(\"msg\", []):\n",
    "        found = find_node_by_acronym(node, acronym)\n",
    "        if found is not None:\n",
    "            return extract_all_ids(found)\n",
    "    return []\n",
    "\n",
    "\n",
    "# --- Annotation Volume Processing Functions ---\n",
    "\n",
    "def compute_region_center(annotation, region_ids):\n",
    "    \"\"\"\n",
    "    Given a 3D annotation volume (where voxel values are Allen region IDs)\n",
    "    and a list of region IDs, creates a mask, computes the average (x,y,z)\n",
    "    voxel coordinate (center-of-mass) of those voxels, and returns that along with\n",
    "    the voxel count, the mask, and the indices of the matching voxels.\n",
    "    \"\"\"\n",
    "    # Create a boolean mask for voxels matching any of the region_ids\n",
    "    mask = np.isin(annotation, region_ids)\n",
    "    \n",
    "    # Find the indices of all voxels in the mask\n",
    "    indices = np.nonzero(mask)\n",
    "    \n",
    "    if np.sum(mask) == 0:\n",
    "        raise ValueError(\"No voxels found matching the provided region IDs: \" + str(region_ids))\n",
    "    \n",
    "    # Compute the average coordinate for each axis.\n",
    "    # Note: The array shape is (rostral-caudal, dorsoventral, mediolateral)\n",
    "    avg_coords = (np.mean(indices[0]), np.mean(indices[1]), np.mean(indices[2]),  np.sum(mask))\n",
    "\n",
    "    return avg_coords # mask, indices\n",
    "\n",
    "structure_graph = download_structure_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd9c964-0626-4bc9-b4ec-64535910f2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "allacro = []\n",
    "\n",
    "for acronym in tqdm(connectome.index.values):\n",
    "    try:\n",
    "        allacro.append(compute_region_center(annotation, get_downstream_ids(acronym, structure_graph)))\n",
    "    except:\n",
    "        print(acronym)\n",
    "        continue\n",
    "\n",
    "spatialcentroids = pd.DataFrame(allacro, index = connectome.index.values[connectome.index.values != \"SUBv\"], columns = ['x_index','y_index','z_index','count']) # (only) this got lost, omit for now SUBv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a2b4ff-6618-446b-80c3-67f1e37bcaa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "def compute_distance_matrix(spatialcentroids):\n",
    "    \"\"\"\n",
    "    Computes the pairwise Euclidean distance matrix between regions given a\n",
    "    pandas DataFrame of centroids with columns: 'x_index', 'y_index', 'z_index'.\n",
    "\n",
    "    Parameters:\n",
    "        spatialcentroids (pd.DataFrame): DataFrame containing the region centroids.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame representing the pairwise distance matrix,\n",
    "                      with rows and columns corresponding to the index of the input DataFrame.\n",
    "    \"\"\"\n",
    "    coords = spatialcentroids[['x_index', 'y_index', 'z_index']].values\n",
    "    \n",
    "    dist_matrix = cdist(coords, coords, metric='euclidean')\n",
    "    \n",
    "    distance_df = pd.DataFrame(dist_matrix, index=spatialcentroids.index, columns=spatialcentroids.index)\n",
    "    return distance_df\n",
    "\n",
    "distance_matrix_df = compute_distance_matrix(spatialcentroids)\n",
    "distance_matrix_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062b29b4-5be0-4431-aec1-5d9b8cc642bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "connectome = connectome.loc[distance_matrix_df.index, distance_matrix_df.columns]\n",
    "atlas = datavignettes.loc[datavignettes['adaptedacronym'].isin(allen.index),:] # filter accordingly\n",
    "spatialcentroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa1a564-b476-466c-87bf-c45fe983855c",
   "metadata": {},
   "outputs": [],
   "source": [
    "spatialcentroids['count'].min() # sanity check on region size used for estimate..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa55a2c-e6ac-435d-bfb9-26a5e91cf67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "spatialcentroids.loc[\"ICd\",:] # sanity check on posterior guy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b1b244-b102-48ea-93bf-581dc971623d",
   "metadata": {},
   "outputs": [],
   "source": [
    "spatialcentroids['x_index'].sort_values() # makes sense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b311179-ba80-4306-9c11-591b9678a529",
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_matrix_df_values = distance_matrix_df.values  \n",
    "\n",
    "i_upper = np.triu_indices_from(distance_matrix_df_values, k=1)\n",
    "values = distance_matrix_df_values[i_upper]\n",
    "\n",
    "plt.hist(values, bins=100)\n",
    "plt.xlabel(\"Value\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Histogram of upper triangle (excluding diagonal)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df18057-270e-4833-8369-2326e6bb65f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "913c7147-20b6-4eb7-8d74-089325b1dc6d",
   "metadata": {},
   "source": [
    "## Prepare the lipizones-based region x region matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6afcf977-3e8a-4f4c-9614-a3457b37c257",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) who's anatomical?\n",
    "\n",
    "import scipy.cluster.hierarchy as sch\n",
    "\n",
    "acronyms = atlas['acronym'].copy()\n",
    "lipizones = atlas['lipizone_names'].copy()\n",
    "\n",
    "acronyms = acronyms.loc[acronyms.isin(acronyms.value_counts().index[acronyms.value_counts() > 50])]\n",
    "lipizones = lipizones.loc[acronyms.index]\n",
    "\n",
    "cmat = pd.crosstab(acronyms, lipizones)\n",
    "\n",
    "normalized_df = cmat / cmat.sum() # fraction \n",
    "normalized_df = (normalized_df.T / normalized_df.T.mean()).T ## switch to enrichments\n",
    "normalized_df1 = normalized_df.copy()\n",
    "normalized_df1\n",
    "\n",
    "cmat = pd.crosstab(lipizones, acronyms)\n",
    "normalized_df = cmat / cmat.sum() \n",
    "normalized_df = (normalized_df.T / normalized_df.T.mean()).T \n",
    "normalized_df2 = normalized_df.copy().T\n",
    "normalized_df2\n",
    "\n",
    "normalized_df = normalized_df2 * normalized_df1\n",
    "normalized_df[cmat.T < 20] = 0\n",
    "\n",
    "linkage = sch.linkage(sch.distance.pdist(normalized_df.T), method='weighted', optimal_ordering=True)\n",
    "order = sch.leaves_list(linkage)\n",
    "normalized_df = normalized_df.iloc[:, order]\n",
    "\n",
    "order = np.argmax(normalized_df.values, axis=1)\n",
    "order = np.argsort(order)\n",
    "normalized_df = normalized_df.iloc[order,:]\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "sns.heatmap(normalized_df, cmap=\"Grays\", cbar_kws={'label': 'Enrichment'}, xticklabels=True, yticklabels=False, vmin = np.percentile(normalized_df, 2), vmax = np.percentile(normalized_df, 98))\n",
    "\n",
    "plt.tick_params(axis='x', which='both', bottom=False, top=False, labelbottom=False)\n",
    "plt.tick_params(axis='y', which='both', left=False, right=False)\n",
    "\n",
    "plt.yticks(rotation=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a83a308-2bec-4d83-9c3b-a49bc0b41cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_df[cmat.T < 50] = 0\n",
    "values = normalized_df.values.flatten()\n",
    "\n",
    "np.sum(values > 10) / len(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a98d6e-81ef-4b2b-a2d9-3b33ad83e7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "presencematrix = normalized_df > 10\n",
    "sharingmatrix = presencematrix @ presencematrix.T\n",
    "sharingmatrix = sharingmatrix.loc[sharingmatrix.index.isin(distance_matrix_df.index), sharingmatrix.index.isin(distance_matrix_df.index)]\n",
    "sharingmatrix # we lost 60 regions... check later!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d3752c-b52b-485e-9bab-4c40e9ce94ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "S = sharingmatrix.copy().values\n",
    "C = connectome.loc[sharingmatrix.index, sharingmatrix.index].copy().values\n",
    "D = distance_matrix_df.loc[sharingmatrix.index, sharingmatrix.index].copy().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136dac6a-cef5-4911-b3d8-b6b7b651879d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f628602e-48cb-4a83-a0ad-ad352e4a5a62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "91701b2b-3c5d-4712-bab2-f91c27a50b08",
   "metadata": {},
   "source": [
    "## Make a null distribution with Metropolis sampling and test significance accounting for the spatial distance confounder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651f4832-acd2-40e2-9815-ded3b558aed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- Helper functions ---\n",
    "\n",
    "def get_upper_triangular_indices(n):\n",
    "    \"\"\"Return indices for the upper triangular part of an n x n matrix (excluding the diagonal).\"\"\"\n",
    "    return np.triu_indices(n, k=1)\n",
    "\n",
    "def get_high_connectivity_mask(C, indices, percentile=90):\n",
    "    \"\"\"\n",
    "    Given a connectivity matrix C and indices for the upper triangle,\n",
    "    determine which pairs are 'highly connected' based on a given percentile.\n",
    "    \"\"\"\n",
    "    c_vals = C[indices]\n",
    "    thresh = np.percentile(c_vals, percentile)\n",
    "    high_mask = c_vals >= thresh\n",
    "    return high_mask, thresh\n",
    "\n",
    "def compute_T_from_pairs(L, S_pairs):\n",
    "    \"\"\"\n",
    "    Compute the test statistic T as the difference between:\n",
    "      - the proportion of pairs with connectivity label 1 (highly connected)\n",
    "        that share at least one cell type\n",
    "      - the proportion of pairs with connectivity label 0 (not highly connected)\n",
    "        that share at least one cell type\n",
    "    L: binary connectivity labels for each pair\n",
    "    S_pairs: corresponding cell type sharing values for each pair\n",
    "    \"\"\"\n",
    "    high_indices = (L == 1)\n",
    "    low_indices = (L == 0)\n",
    "    prop_high = np.mean(S_pairs[high_indices]) if np.sum(high_indices) > 0 else 0\n",
    "    prop_low = np.mean(S_pairs[low_indices]) if np.sum(low_indices) > 0 else 0\n",
    "    return prop_high - prop_low\n",
    "\n",
    "def estimate_probability_function(D, high_mask, indices, bins=20):\n",
    "    \"\"\"\n",
    "    Estimate the probability of a pair being highly connected as a function of its physical distance.\n",
    "    Returns:\n",
    "      - bin_centers: centers of distance bins\n",
    "      - p_est: estimated probabilities for each bin\n",
    "      - bin_edges: the edges of the bins used\n",
    "    \"\"\"\n",
    "    d_vals = D[indices]\n",
    "    # Histogram of all pairs' distances.\n",
    "    hist_total, bin_edges = np.histogram(d_vals, bins=bins)\n",
    "    # Histogram for pairs that are highly connected.\n",
    "    hist_high, _ = np.histogram(d_vals[high_mask], bins=bin_edges)\n",
    "    # Calculate probability for each bin, taking care to avoid division by zero.\n",
    "    p_est = np.divide(hist_high, hist_total, out=np.zeros_like(hist_high, dtype=float), where=hist_total != 0)\n",
    "    bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2\n",
    "    return bin_centers, p_est, bin_edges\n",
    "\n",
    "def get_probability_for_distance(d, bin_edges, p_est):\n",
    "    \"\"\"\n",
    "    For a given distance d, return the estimated probability from the binned p_est.\n",
    "    \"\"\"\n",
    "    bin_index = np.digitize(d, bin_edges) - 1  # np.digitize returns index starting at 1\n",
    "    # Ensure index is within valid range.\n",
    "    bin_index = np.clip(bin_index, 0, len(p_est) - 1)\n",
    "    return p_est[bin_index]\n",
    "\n",
    "def simulate_connectivity_labels(D, indices, bin_edges, p_est):\n",
    "    \"\"\"\n",
    "    For each region pair (given by indices), simulate a connectivity label (0/1)\n",
    "    from a Bernoulli distribution with probability determined by its physical distance.\n",
    "    \"\"\"\n",
    "    d_vals = D[indices]\n",
    "    # For each distance, get the corresponding probability.\n",
    "    p_vals = np.array([get_probability_for_distance(d, bin_edges, p_est) for d in d_vals])\n",
    "    # Sample a binary label for each pair.\n",
    "    simulated_labels = np.random.rand(len(p_vals)) < p_vals\n",
    "    return simulated_labels.astype(int)\n",
    "\n",
    "# --- Main permutation test function ---\n",
    "\n",
    "def run_permutation_test(C, D, S, percentile=90, bins=20, n_perm=1000, random_state=42):\n",
    "    \"\"\"\n",
    "    Run the permutation test using a distance-controlled simulation to generate\n",
    "    a null distribution of the test statistic.\n",
    "    \n",
    "    C: connectomic strength matrix (n x n)\n",
    "    D: physical distance matrix (n x n)\n",
    "    S: cell type similarity matrix (n x n) (assumed binary here)\n",
    "    percentile: threshold percentile for 'high connectivity'\n",
    "    bins: number of bins for estimating p(d)\n",
    "    n_perm: number of permutations to simulate\n",
    "    random_state: for reproducibility\n",
    "    \"\"\"\n",
    "    np.random.seed(random_state)\n",
    "    n = C.shape[0]\n",
    "    indices = get_upper_triangular_indices(n)\n",
    "    \n",
    "    # Step 1: Determine observed high connectivity pairs.\n",
    "    high_mask_obs, thresh = get_high_connectivity_mask(C, indices, percentile=percentile)\n",
    "    L_obs = high_mask_obs.astype(int)\n",
    "    \n",
    "    # Step 2: Compute observed test statistic.\n",
    "    S_pairs = S[indices]  # cell type sharing for each region pair\n",
    "    T_obs = compute_T_from_pairs(L_obs, S_pairs)\n",
    "    print(\"Observed T statistic:\", T_obs)\n",
    "    \n",
    "    # Step 3: Estimate the conditional probability p(d) from observed data.\n",
    "    bin_centers, p_est, bin_edges = estimate_probability_function(D, high_mask_obs, indices, bins=bins)\n",
    "    \n",
    "    # Plot the estimated probability function.\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.plot(bin_centers, p_est, 'o-')\n",
    "    plt.xlabel('Physical Distance')\n",
    "    plt.ylabel('P(high connectivity | distance)')\n",
    "    plt.title('Estimated Connectivity Probability vs. Distance')\n",
    "    plt.show()\n",
    "    \n",
    "    # Step 4: Run simulations.\n",
    "    T_null = []\n",
    "    for i in range(n_perm):\n",
    "        sim_labels = simulate_connectivity_labels(D, indices, bin_edges, p_est)\n",
    "        T_sim = compute_T_from_pairs(sim_labels, S_pairs)\n",
    "        T_null.append(T_sim)\n",
    "    T_null = np.array(T_null)\n",
    "    \n",
    "    # Step 5: Calculate one-tailed p-value.\n",
    "    p_value = np.mean(T_null >= T_obs)\n",
    "    print(\"One-tailed p-value:\", p_value)\n",
    "    \n",
    "    # (Optional) Plot the null distribution and mark T_obs.\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.hist(T_null, bins=30, alpha=0.7, color='gray', label='Null distribution')\n",
    "    plt.axvline(T_obs, color='red', linestyle='dashed', linewidth=2, label='T_obs')\n",
    "    plt.xlabel('Test Statistic T')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Null Distribution of T with Observed T')\n",
    "    plt.legend()\n",
    "    plt.savefig(\"significancelipizonesconnections.pdf\")\n",
    "    plt.show()\n",
    "    \n",
    "    return T_obs, T_null, p_value\n",
    "\n",
    "T_obs, T_null, p_value = run_permutation_test(C, D, S, percentile=95, bins=50, n_perm=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c792ac1-85f3-4afd-ac21-c3add5c997be",
   "metadata": {},
   "outputs": [],
   "source": [
    "percentile=95\n",
    "bins=50\n",
    "n_perm=1000\n",
    "random_state=42\n",
    "np.random.seed(random_state)\n",
    "n = C.shape[0]\n",
    "indices = get_upper_triangular_indices(n)\n",
    "\n",
    "high_mask_obs, thresh = get_high_connectivity_mask(C, indices, percentile=percentile)\n",
    "L_obs = high_mask_obs.astype(int)\n",
    "\n",
    "S_pairs = S[indices]\n",
    "\n",
    "bin_centers, p_est, bin_edges = estimate_probability_function(D, high_mask_obs, indices, bins=bins)\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(bin_centers, p_est, 'o-')\n",
    "plt.xlabel('Physical Distance')\n",
    "plt.ylabel('P(high connectivity | distance)')\n",
    "plt.title('Estimated Connectivity Probability vs. Distance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea5ddaf-6673-4523-ac7b-63235c4c11fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try different percentiles\n",
    "T_obs, T_null, p_value = run_permutation_test(C, D, S, percentile=99.5, bins=50, n_perm=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec24611c-a5d8-4946-86d8-9b850154ec7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "T_obs, T_null, p_value = run_permutation_test(C, D, S, percentile=75.0, bins=50, n_perm=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca028d4e-3a71-42de-b3d4-f04186e1b8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try different percentiles\n",
    "T_obs, T_null, p_value = run_permutation_test(C, D, S, percentile=97.5, bins=50, n_perm=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be47471f-2ce8-44ce-87f4-5ddaddf55761",
   "metadata": {},
   "outputs": [],
   "source": [
    "def diagnostic_convergence(C, D, S, percentile=90, bins=20, n_perm=1000, random_state=42):\n",
    "    \"\"\"\n",
    "    Plot the trace of the test statistic T over the permutation iterations and\n",
    "    its autocorrelation function.\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(random_state)\n",
    "    n = C.shape[0]\n",
    "    indices = get_upper_triangular_indices(n)\n",
    "    \n",
    "    # Step 1: Determine observed high connectivity pairs.\n",
    "    high_mask_obs, thresh = get_high_connectivity_mask(C, indices, percentile=percentile)\n",
    "    L_obs = high_mask_obs.astype(int)\n",
    "    \n",
    "    # Step 2: Compute observed test statistic.\n",
    "    S_pairs = S[indices]  # cell type sharing for each region pair\n",
    "    T_obs = compute_T_from_pairs(L_obs, S_pairs)\n",
    "    print(\"Observed T statistic:\", T_obs)\n",
    "    \n",
    "    # Step 3: Estimate the conditional probability p(d) from observed data.\n",
    "    bin_centers, p_est, bin_edges = estimate_probability_function(D, high_mask_obs, indices, bins=bins)\n",
    "    \n",
    "    # Plot the estimated probability function.\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.plot(bin_centers, p_est, 'o-')\n",
    "    plt.xlabel('Physical Distance')\n",
    "    plt.ylabel('P(high connectivity | distance)')\n",
    "    plt.title('Estimated Connectivity Probability vs. Distance')\n",
    "    plt.show()\n",
    "    \n",
    "    # Step 4: Run simulations.\n",
    "    T_null = []\n",
    "    for i in range(n_perm):\n",
    "        sim_labels = simulate_connectivity_labels(D, indices, bin_edges, p_est)\n",
    "        T_sim = compute_T_from_pairs(sim_labels, S_pairs)\n",
    "        T_null.append(T_sim)\n",
    "    T_null = np.array(T_null)\n",
    "    \n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(T_null, marker='o', linestyle='-', alpha=0.7)\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Test Statistic T')\n",
    "    plt.title('Trace Plot of T over Permutations')\n",
    "    plt.show()\n",
    "    \n",
    "    # Autocorrelation plot.\n",
    "    from statsmodels.graphics.tsaplots import plot_acf\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plot_acf(T_null, lags=30)\n",
    "    plt.title('Autocorrelation of T')\n",
    "    plt.show()\n",
    "\n",
    "diagnostic_convergence(C, D, S, percentile=97.5, bins=50, n_perm=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1484cb2-a79f-4de4-9db8-40cb8c2a9f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def degree_preserving_randomization_with_distance(matrix, D, bin_edges, p_est, \n",
    "                                                  n_swaps=1000, temperature=1.0, \n",
    "                                                  random_state=42):\n",
    "    \"\"\"\n",
    "    Perform degree-preserving randomization with a Metropolis–Hastings step \n",
    "    to bias the swaps towards preserving the distance-based edge probabilities.\n",
    "    \n",
    "    Parameters:\n",
    "      matrix: symmetric binary connectivity matrix (observed).\n",
    "      D: physical distance matrix.\n",
    "      bin_edges, p_est: parameters of the estimated p(d) function.\n",
    "      n_swaps: number of attempted edge swaps.\n",
    "      temperature: controls the acceptance of swaps that worsen the p(d) match.\n",
    "      random_state: seed for reproducibility.\n",
    "      \n",
    "    Returns:\n",
    "      A new connectivity matrix with the same degree sequence as matrix.\n",
    "    \"\"\"\n",
    "    np.random.seed(random_state)\n",
    "    M = matrix.copy()\n",
    "    n = M.shape[0]\n",
    "    \n",
    "    # Get list of existing edges in the upper triangle (i < j)\n",
    "    edges = list(zip(*np.where(np.triu(M, k=1) == 1)))\n",
    "    num_edges = len(edges)\n",
    "    \n",
    "    for _ in range(n_swaps):\n",
    "        if num_edges < 2:\n",
    "            break\n",
    "        # Randomly select two distinct edges.\n",
    "        idx1, idx2 = np.random.choice(num_edges, 2, replace=False)\n",
    "        i, j = edges[idx1]\n",
    "        k, l = edges[idx2]\n",
    "        \n",
    "        # Ensure the four nodes are distinct.\n",
    "        if len({i, j, k, l}) != 4:\n",
    "            continue\n",
    "        \n",
    "        # Proposed new edges: (i, l) and (k, j)\n",
    "        # Check that these edges do not already exist.\n",
    "        if M[i, l] == 1 or M[k, j] == 1:\n",
    "            continue\n",
    "        \n",
    "        # Compute the distance-based probabilities for the old and new edges.\n",
    "        p_old1 = get_probability_for_distance(D[i, j], bin_edges, p_est)\n",
    "        p_old2 = get_probability_for_distance(D[k, l], bin_edges, p_est)\n",
    "        p_new1 = get_probability_for_distance(D[i, l], bin_edges, p_est)\n",
    "        p_new2 = get_probability_for_distance(D[k, j], bin_edges, p_est)\n",
    "        \n",
    "        # Sum the probabilities for old and new pairs.\n",
    "        sum_old = p_old1 + p_old2\n",
    "        sum_new = p_new1 + p_new2\n",
    "        \n",
    "        # Compute the change; if new edges are “better” (i.e. higher total probability)\n",
    "        # then delta will be negative.\n",
    "        delta = sum_old - sum_new\n",
    "        \n",
    "        # Metropolis acceptance probability: if delta < 0 (improvement), accept always;\n",
    "        # otherwise, accept with probability exp(-delta/temperature).\n",
    "        accept_prob = np.exp(-delta/temperature) if delta > 0 else 1.0\n",
    "        \n",
    "        if np.random.rand() < accept_prob:\n",
    "            # Perform the swap.\n",
    "            M[i, j] = M[j, i] = 0\n",
    "            M[k, l] = M[l, k] = 0\n",
    "            M[i, l] = M[l, i] = 1\n",
    "            M[k, j] = M[j, k] = 1\n",
    "            # Update the edges list with the new edges (sorted as (min, max)).\n",
    "            edges[idx1] = (min(i, l), max(i, l))\n",
    "            edges[idx2] = (min(k, j), max(k, j))\n",
    "            \n",
    "    return M\n",
    "\n",
    "def run_permutation_test_degree_preserved_with_distance(C, D, S, percentile=95, \n",
    "                                                        n_perm=1000, n_swaps=1000, \n",
    "                                                        temperature=1.0, bins=50,\n",
    "                                                        random_state=42):\n",
    "    \"\"\"\n",
    "    Run the permutation test using degree-preserving randomization that also\n",
    "    biases swaps to preserve the distance structure.\n",
    "    \n",
    "    Parameters:\n",
    "      C: Original connectivity matrix.\n",
    "      D: Physical distance matrix.\n",
    "      S: Cell type similarity matrix (binary).\n",
    "      percentile: threshold percentile to define the observed binary connectivity.\n",
    "      n_perm: number of permutations.\n",
    "      n_swaps: number of swaps for each randomization.\n",
    "      temperature: MH temperature for swap acceptance.\n",
    "      bins: number of bins to estimate the p(d) function.\n",
    "      random_state: seed for reproducibility.\n",
    "      \n",
    "    Returns:\n",
    "      T_obs: observed test statistic.\n",
    "      T_null: array of test statistics from permutations.\n",
    "      p_value: one-tailed p-value.\n",
    "      perm_conn_matrices: list of randomized connectivity matrices.\n",
    "    \"\"\"\n",
    "    np.random.seed(random_state)\n",
    "    n = C.shape[0]\n",
    "    \n",
    "    # Create the observed binary connectivity matrix.\n",
    "    ground_truth = (C > np.percentile(C, percentile)).astype(int)\n",
    "    indices = np.triu_indices(n, k=1)\n",
    "    \n",
    "    # Compute the observed test statistic.\n",
    "    S_pairs = S[indices]\n",
    "    T_obs = compute_T_from_pairs(ground_truth[indices].astype(int), S_pairs)\n",
    "    print(\"Observed T statistic:\", T_obs)\n",
    "    \n",
    "    # Estimate the conditional probability p(d) from observed data.\n",
    "    bin_centers, p_est, bin_edges = estimate_probability_function(D, \n",
    "                                                                    ground_truth[indices].astype(bool), \n",
    "                                                                    indices, \n",
    "                                                                    bins=bins)\n",
    "    \n",
    "    T_null = []\n",
    "    perm_conn_matrices = []\n",
    "    \n",
    "    for i in range(n_perm):\n",
    "        perm_matrix = degree_preserving_randomization_with_distance(ground_truth, D, bin_edges, p_est, \n",
    "                                                                    n_swaps=n_swaps, temperature=temperature, \n",
    "                                                                    random_state=random_state + i)\n",
    "        T_sim = compute_T_from_pairs(perm_matrix[indices].astype(int), S_pairs)\n",
    "        T_null.append(T_sim)\n",
    "        perm_conn_matrices.append(perm_matrix)\n",
    "    \n",
    "    T_null = np.array(T_null)\n",
    "    p_value = np.mean(T_null >= T_obs)\n",
    "    print(\"One-tailed p-value:\", p_value)\n",
    "    \n",
    "    # Plot the null distribution.\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.hist(T_null, bins=30, alpha=0.7, label='Null distribution')\n",
    "    plt.axvline(T_obs, color='red', linestyle='dashed', linewidth=2, label='T_obs')\n",
    "    plt.xlabel('Test Statistic T')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Null Distribution of T with Observed T')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    return T_obs, T_null, p_value, perm_conn_matrices\n",
    "\n",
    "# Example usage with a small number of permutations (e.g., 5) for testing:\n",
    "T_obs, T_null, p_value, perm_conn_matrices = run_permutation_test_degree_preserved_with_distance(\n",
    "    C, D, S, percentile=95, n_perm=1000, n_swaps=1000, temperature=1.0, bins=50, random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a2b01449-38d3-4789-a4fe-aa00ea90b4da",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Compute degrees for each simulated connectivity matrix.\n",
    "# This creates an array where each row corresponds to one permutation's degree vector.\n",
    "perm_degrees = np.array([np.sum(perm, axis=1) for perm in perm_conn_matrices])\n",
    "\n",
    "# Compute the average degree for each node across all permutations.\n",
    "avg_perm_degrees = np.mean(perm_degrees, axis=0)\n",
    "\n",
    "# Compute the ground truth degrees from the thresholded connectivity matrix.\n",
    "# Here, we assume the ground truth binary connectivity is defined as values in C \n",
    "# exceeding its 95th percentile.\n",
    "ground_truth_binary = (C > np.percentile(C, 95)).astype(int)\n",
    "ground_truth_degrees = np.sum(ground_truth_binary, axis=1)\n",
    "\n",
    "# Plot the average simulated degrees against the ground truth degrees.\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.scatter(avg_perm_degrees, pd.DataFrame(C).sum(axis=0))\n",
    "plt.xlabel(\"Average Simulated Degree\")\n",
    "plt.ylabel(\"Ground Truth Degree\")\n",
    "plt.title(\"Comparison of Node Degrees\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92068978-cbfd-4873-a6ad-66102cad394d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e8f8053a-169f-4aa9-9249-efd5daba3333",
   "metadata": {},
   "source": [
    "## Flag lipizones as input-output related based on the density of the local connectivity network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef5289d-ba2b-4738-a5e7-2a4d1f4a56af",
   "metadata": {},
   "outputs": [],
   "source": [
    "allen = pd.read_csv(\"./zenodo/csv/allenconnectome_contra.csv\", index_col=0) + pd.read_csv(\"./zenodo/csv/allenconnectome_ipsi.csv\", index_col=0) \n",
    "cm = normalized_df.T\n",
    "cm = cm.loc[:, cm.sum()>0]\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ba5b50-0b09-4824-bf44-4064c78b997e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def assess_subnetwork_density(subnet_indices, allen, num_permutations=10000):\n",
    "\n",
    "    subnet_indices = np.array(subnet_indices)\n",
    "    \n",
    "    observed_submatrix = allen.loc[subnet_indices, subnet_indices]\n",
    "    n = len(subnet_indices)\n",
    "    \n",
    "    if n > 1:\n",
    "        mask = ~np.eye(n, dtype=bool)\n",
    "        observed_values = observed_submatrix.values[mask]\n",
    "        observed_mean = observed_values.mean()\n",
    "    else:\n",
    "        observed_mean = observed_submatrix.values[0, 0]\n",
    "    \n",
    "    null_means = np.empty(num_permutations)\n",
    "    all_nodes = allen.index.to_numpy()\n",
    "    for i in range(num_permutations):\n",
    "        random_nodes = np.random.choice(all_nodes, size=n, replace=False)\n",
    "        random_submatrix = allen.loc[random_nodes, random_nodes].values\n",
    "        if n > 1:\n",
    "            mask = ~np.eye(n, dtype=bool)\n",
    "            mean_val = random_submatrix[mask].mean()\n",
    "        else:\n",
    "            mean_val = random_submatrix[0, 0]\n",
    "        null_means[i] = mean_val\n",
    "\n",
    "    p_value = np.mean(null_means >= observed_mean)\n",
    "    \n",
    "    return {\"observed_mean\": observed_mean,\n",
    "            \"p_value\": p_value,\n",
    "            \"null_distribution\": null_means}\n",
    "\n",
    "allanalyses = []\n",
    "\n",
    "from tqdm import tqdm\n",
    "for lev7 in tqdm(cm.index.values):\n",
    "    try:\n",
    "        topregionshere = cm.loc[lev7,:].sort_values()[::-1]\n",
    "        topregionshere = topregionshere[topregionshere > 49].index\n",
    "        subnetwork_nodes = topregionshere.values\n",
    "        allanalyses.append(assess_subnetwork_density(subnetwork_nodes, allen, num_permutations=10000))\n",
    "    except:\n",
    "        print(lev7)\n",
    "        print(\"problem!\")\n",
    "        continue\n",
    "        \n",
    "import pickle\n",
    "\n",
    "with open(\"allanalyses.pkl\", \"wb\") as f:\n",
    "    pickle.dump(allanalyses, f)\n",
    "    \n",
    "pvals = [x['p_value'] for x in allanalyses]\n",
    "pvalsdf = pd.DataFrame(pvals, index=indexes)\n",
    "pvalsdf = pvalsdf.groupby(pvalsdf.index).mean()\n",
    "pvalsdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53a6180-75e3-453c-a534-bb2018aa6fdf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd69ed0a-4c1a-404d-bd38-a6e999e02231",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b20d265-b5aa-493a-abf9-d9712fcbc22f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
