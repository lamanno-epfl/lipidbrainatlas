{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63bd6757-db0f-4bef-8c00-5491b4c3480e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import warnings\n",
    "from collections import deque, Counter\n",
    "import random\n",
    "import itertools\n",
    "import json\n",
    "from datetime import datetime\n",
    "import cProfile\n",
    "import pstats\n",
    "\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import anndata\n",
    "import scanpy as sc\n",
    "import squidpy as sq\n",
    "import backSPIN\n",
    "import leidenalg\n",
    "import networkx as nx\n",
    "import igraph as ig\n",
    "\n",
    "from matplotlib import colors as mcolors\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "from matplotlib.cm import ScalarMappable\n",
    "from matplotlib.colors import Normalize\n",
    "\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.decomposition import PCA, NMF\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "from scipy.cluster.hierarchy import linkage, fcluster\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "from scipy.signal import find_peaks\n",
    "from scipy.spatial.distance import squareform, pdist\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.stats import mannwhitneyu, entropy\n",
    "\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "from threadpoolctl import threadpool_limits, threadpool_info\n",
    "\n",
    "from tqdm import tqdm\n",
    "from kneed import KneeLocator\n",
    "from PyPDF2 import PdfMerger\n",
    "\n",
    "# Set thread limits and environment variables\n",
    "threadpool_limits(limits=8)\n",
    "os.environ['OMP_NUM_THREADS'] = '6'\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05026e8a-1119-4364-9fba-27ab1900e663",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "95fda255-76bb-4d95-a200-0e2240a18c71",
   "metadata": {},
   "source": [
    "## Prepare and evaluate the global embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a06d63-bd1d-44df-aef3-deb310621cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_hdf(\"20241103_featsel_lba.h5\", key=\"table\")\n",
    "\n",
    "coordinates = data[['Section', 'xccf', 'yccf', 'zccf']]\n",
    "coordinates['Section'] = coordinates['Section'].astype(int)\n",
    "\n",
    "metdat = data.iloc[:,:23]\n",
    "data = data.iloc[:,23:]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c8b07e-cffd-4408-8341-d3ec385ed406",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we cluster the atlas with a bipartite, hierarchical splitter strategy\n",
    "\n",
    "harmonized_nmf_result = pd.read_hdf(\"corrected_nmfall_nochunking.h5ad\", key=\"table\")\n",
    "factor_to_lipid = np.load(\"factor_to_lipid.npy\")\n",
    "reconstructed_data_df = pd.DataFrame(np.dot(harmonized_nmf_result.values, factor_to_lipid), index = harmonized_nmf_result.index, columns = data.columns)\n",
    "\n",
    "# make sure the data is positive as all computations are NMF-based\n",
    "reconstructed_data_df = reconstructed_data_df - np.min(reconstructed_data_df) + 1e-7\n",
    "\n",
    "# subset the atlas\n",
    "reconstructed_data_df = reconstructed_data_df.loc[data.index,:]\n",
    "\n",
    "# compute the standardized global embeddings to always use them along with the local ones and the local ones with memory\n",
    "nmf_result = harmonized_nmf_result.loc[data.index,:]\n",
    "scalerglobal = StandardScaler()\n",
    "standardized_embeddings_GLOBAL = pd.DataFrame(scalerglobal.fit_transform(nmf_result), index=nmf_result.index, columns=nmf_result.columns)\n",
    "standardized_embeddings_GLOBAL ##### DATAFRAMEEEEE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9caa23e-2b67-4fc2-bede-7eeaad1e72f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_df = pd.read_hdf(\"tsne_df.h5ad\", key=\"table\")\n",
    "tsne = tsne_df.values"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d6c67663-132d-4ec9-b973-1858bc4d798c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## how much did the harmony-NMF-mediated batch removal for reconstruction affect the data?\n",
    "\n",
    "# check the scatterplots for some example lipids\n",
    "a = data.columns[17]\n",
    "b = data.columns[52]\n",
    "c = data.columns[104]\n",
    "\n",
    "plt.scatter(data[a], reconstructed_data_df[a], s=0.005, alpha=0.5)\n",
    "plt.show()\n",
    "plt.scatter(data[b], reconstructed_data_df[b], s=0.005, alpha=0.5)\n",
    "plt.show()\n",
    "plt.scatter(data[c], reconstructed_data_df[c], s=0.005, alpha=0.5)\n",
    "plt.show()\n",
    "\n",
    "## visually inspect the result of rank reduction on a few random lipids\n",
    "checks = [a, b, c]\n",
    "\n",
    "for c in checks:\n",
    "\n",
    "    results = []\n",
    "\n",
    "    filtered_data = pd.concat([coordinates.loc[reconstructed_data_df.index,:], pd.DataFrame(reconstructed_data_df.loc[:,c].values, index=coordinates.loc[reconstructed_data_df.index,:].index,columns=[\"test\"])], axis=1)\n",
    "    \n",
    "    currentPC = \"test\"\n",
    "    for section in filtered_data['Section'].unique():\n",
    "        subset = filtered_data[filtered_data['Section'] == section]\n",
    "\n",
    "        perc_2 = subset[currentPC].quantile(0.02)\n",
    "        perc_98 = subset[currentPC].quantile(0.98)\n",
    "\n",
    "        results.append([section, perc_2, perc_98])\n",
    "    percentile_df = pd.DataFrame(results, columns=['Section', '2-perc', '98-perc'])\n",
    "    med2p = percentile_df['2-perc'].median()\n",
    "    med98p = percentile_df['98-perc'].median()\n",
    "\n",
    "    cmap = plt.cm.PuOr\n",
    "\n",
    "    fig, axes = plt.subplots(4, 8, figsize=(20, 10))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for section in range(1, 33):\n",
    "        ax = axes[section - 1]\n",
    "        ddf = filtered_data[(filtered_data['Section'] == section)]\n",
    "\n",
    "        ax.scatter(ddf['zccf'], -ddf['yccf'], c=ddf[currentPC], cmap=\"inferno\", s=0.5,rasterized=True, vmin=med2p, vmax=med98p) \n",
    "        ax.axis('off')\n",
    "        ax.set_aspect('equal')\n",
    "\n",
    "    cbar_ax = fig.add_axes([0.92, 0.15, 0.02, 0.7])\n",
    "    norm = Normalize(vmin=med2p, vmax=med98p)\n",
    "    sm = ScalarMappable(norm=norm, cmap=cmap)\n",
    "    fig.colorbar(sm, cax=cbar_ax)\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 0.9, 1])\n",
    "    plt.show()\n",
    "\n",
    "for c in checks:\n",
    "\n",
    "    results = []\n",
    "    filtered_data = pd.concat([coordinates.loc[reconstructed_data_df.index,:], pd.DataFrame(data.loc[:,c].values, index=coordinates.loc[reconstructed_data_df.index,:].index,columns=[\"test\"])], axis=1)\n",
    "\n",
    "    currentPC = \"test\"\n",
    "    for section in filtered_data['Section'].unique():\n",
    "        subset = filtered_data[filtered_data['Section'] == section]\n",
    "\n",
    "        perc_2 = subset[currentPC].quantile(0.02)\n",
    "        perc_98 = subset[currentPC].quantile(0.98)\n",
    "\n",
    "        results.append([section, perc_2, perc_98])\n",
    "    percentile_df = pd.DataFrame(results, columns=['Section', '2-perc', '98-perc'])\n",
    "    med2p = percentile_df['2-perc'].median()\n",
    "    med98p = percentile_df['98-perc'].median()\n",
    "\n",
    "    cmap = plt.cm.PuOr\n",
    "\n",
    "    fig, axes = plt.subplots(4, 8, figsize=(20, 10))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for section in range(1, 33):\n",
    "        ax = axes[section - 1]\n",
    "        ddf = filtered_data[(filtered_data['Section'] == section)]\n",
    "\n",
    "        ax.scatter(ddf['zccf'], -ddf['yccf'], c=ddf[currentPC], cmap=\"inferno\", s=0.5,rasterized=True, vmin=med2p, vmax=med98p) \n",
    "        ax.axis('off')\n",
    "        ax.set_aspect('equal')\n",
    "\n",
    "    cbar_ax = fig.add_axes([0.92, 0.15, 0.02, 0.7])\n",
    "    norm = Normalize(vmin=med2p, vmax=med98p)\n",
    "    sm = ScalarMappable(norm=norm, cmap=cmap)\n",
    "    fig.colorbar(sm, cax=cbar_ax)\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 0.9, 1])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ad3ed8-951d-4b6b-8577-8c227f8c5e3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8aa5e72d-05d2-4ce8-aaa5-202e3e9c8fd3",
   "metadata": {},
   "source": [
    "## Functions (and data prep) to be called by the splitter: seeded NMF, differential lipids, variance criterion, continuity across sections, in-line plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560bcd9e-be90-47ac-b527-7d29927024fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_seeded_NMF(data):  # data is a dataframe pixels x lipids\n",
    "    # 1. calculate the correlation matrix of this dataset\n",
    "    corr = np.corrcoef(data.values.T)\n",
    "    corr_matrix = np.abs(corr)  # anticorrelated lipids convey the same info\n",
    "    np.fill_diagonal(corr_matrix, 0)\n",
    "    \n",
    "    adata = anndata.AnnData(X=np.zeros_like(corr_matrix))\n",
    "    adata.obsp['connectivities'] = csr_matrix(corr_matrix)\n",
    "    adata.uns['neighbors'] = {\n",
    "        'connectivities_key': 'connectivities',\n",
    "        'distances_key': 'distances',\n",
    "        'params': {'n_neighbors': 10, 'method': 'custom'}\n",
    "    }\n",
    "    \n",
    "    G = nx.from_numpy_array(corr_matrix)\n",
    "    \n",
    "    # span reasonable Leiden resolution parameters\n",
    "    gamma_values = np.linspace(0.8, 1.5, num=100)\n",
    "    num_communities = []\n",
    "    modularity_scores = []\n",
    "    objective_values = []\n",
    "    \n",
    "    for gamma in gamma_values:\n",
    "        sc.tl.leiden(adata, resolution=gamma, key_added=f'leiden_{gamma}')\n",
    "        clusters = adata.obs[f'leiden_{gamma}'].astype(int).values\n",
    "        num_comms = len(np.unique(clusters))\n",
    "        num_communities.append(num_comms)\n",
    "        partition = {i: clusters[i] for i in range(len(clusters))}\n",
    "        modularity = nx.community.modularity(G, [np.where(clusters == i)[0] for i in range(num_comms)])\n",
    "        modularity_scores.append(modularity)\n",
    "    \n",
    "    # 3. pick a number of blocks that is relatively high while preserving good modularity\n",
    "    epsilon = 1e-10\n",
    "    alpha = 0.7  # controls the weight of modularity vs pushing higher the number of communities\n",
    "    for Q, N_c in zip(modularity_scores, num_communities):\n",
    "        f_gamma = Q**alpha * np.log(N_c + 1 + epsilon)\n",
    "        objective_values.append(f_gamma)\n",
    "    \n",
    "    \"\"\"\n",
    "    plt.plot(np.arange(len(objective_values)), objective_values)\n",
    "    plt.title(\"obj\")\n",
    "    plt.show()\n",
    "    \n",
    "    plt.plot(np.arange(len(modularity_scores)), modularity_scores)\n",
    "    plt.title(\"mod\")\n",
    "    plt.show()\n",
    "        \n",
    "    plt.plot(np.arange(len(num_communities)), num_communities)\n",
    "    plt.title(\"ncomms\")\n",
    "    plt.show()\n",
    "    \"\"\"\n",
    "    \n",
    "    max_index = np.argmax(objective_values)\n",
    "    best_gamma = gamma_values[max_index]\n",
    "    best_modularity = modularity_scores[max_index]\n",
    "    best_num_comms = num_communities[max_index]\n",
    "    #print(f'Number of communities at best gamma: {best_num_comms}')\n",
    "    \n",
    "    sc.tl.leiden(adata, resolution=best_gamma, key_added='leiden_best') # run Leiden one final time with best parameters\n",
    "    clusters = adata.obs['leiden_best'].astype(int).values\n",
    "    #print(clusters)\n",
    "    \n",
    "    N_factors = best_num_comms\n",
    "    \n",
    "    # 4. pick a representative lipid from each block, use to initialize W\n",
    "    dist = 1 - corr_matrix\n",
    "    np.fill_diagonal(dist, 0)\n",
    "    dist = np.maximum(dist, dist.T)  # as numerical instability makes it unreasonably asymmetric\n",
    "    dist_condensed = squareform(dist, checks=True)\n",
    "    representatives = []\n",
    "    \n",
    "    for i in range(0, N_factors):\n",
    "        cluster_members = np.where(clusters == i)[0]\n",
    "        #print(cluster_members)\n",
    "        if len(cluster_members) > 0:  # find most central feature in cluster\n",
    "            mean_dist = dist[cluster_members][:, cluster_members].mean(axis=1)\n",
    "            central_idx = cluster_members[np.argmin(mean_dist)]\n",
    "            representatives.append(central_idx)\n",
    "    \n",
    "    W_init = data.values[:, representatives]\n",
    "    \n",
    "    # 5. initialize H as a subset of the correlation matrix\n",
    "    H_init = corr[representatives,:]\n",
    "    H_init[H_init < 0.] = 0.  # only positive correlated can contribute by def in NMF\n",
    "    \n",
    "    # 6. compute the NMF with this initialization and rank N\n",
    "    N_factors = W_init.shape[1]\n",
    "    nmf = NMF(\n",
    "        n_components=N_factors,\n",
    "        init='custom',\n",
    "        random_state=42\n",
    "    )\n",
    "    MINDATA = np.min(data)\n",
    "    data_offset = data\n",
    "    \n",
    "    data_offset = np.ascontiguousarray(data_offset)\n",
    "    W_init = np.ascontiguousarray(W_init)\n",
    "    H_init = np.ascontiguousarray(H_init)\n",
    "    \n",
    "    nmf_result = nmf.fit_transform(\n",
    "        data_offset,\n",
    "        W=W_init,\n",
    "        H=H_init\n",
    "    )\n",
    "    \n",
    "    nmf_result = nmf.transform(data_offset)\n",
    "    \n",
    "    nmfdf = pd.DataFrame(nmf_result, index=data.index)\n",
    "    factor_to_lipid = nmf.components_\n",
    "    \n",
    "    return nmfdf, factor_to_lipid, N_factors, nmf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa325db-f3c7-40ba-8371-03eb4de448cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the harmony-NMF-corrected data from now on for splitting, but the uMAIA output for differential testing with conservative FDR control\n",
    "\n",
    "rawlips = data.copy()\n",
    "data = reconstructed_data_df\n",
    "\n",
    "# do vmin-vmax normalization with the percentiles + clipping for differential lipids testing\n",
    "datemp = rawlips.copy() \n",
    "p2 = datemp.quantile(0.02)\n",
    "p98 = datemp.quantile(0.98)\n",
    "\n",
    "datemp_values = datemp.values\n",
    "p2_values = p2.values\n",
    "p98_values = p98.values\n",
    "\n",
    "normalized_values = (datemp_values - p2_values) / (p98_values - p2_values)\n",
    "\n",
    "clipped_values = np.clip(normalized_values, 0, 1)\n",
    "\n",
    "normalized_datemp = pd.DataFrame(clipped_values, columns=datemp.columns, index=datemp.index)\n",
    "#normalized_datemp.to_hdf(\"normalized_datemp_raw.h5ad\", key=\"table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae87797f-a09e-41f5-b9ad-a8a06e29df6e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# prepare a scanpy object with also the raw lipids values for differential testing\n",
    "\n",
    "adata = sc.AnnData(X=data)\n",
    "adata.obsm['spatial'] = coordinates[['zccf', 'yccf', 'Section']].loc[data.index,:].values\n",
    "\n",
    "adata.obsm['lipids'] = normalized_datemp\n",
    "\n",
    "adata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6a5cbc-b3de-43c2-b8d9-d2bdf9013eae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# prepare useful plotting functions\n",
    "\n",
    "global_min_z = coordinates['zccf'].min()\n",
    "global_max_z = coordinates['zccf'].max()\n",
    "global_min_y = -coordinates['yccf'].max() \n",
    "global_max_y = -coordinates['yccf'].min()  \n",
    "\n",
    "def plot_spatial_localPCA(spat, pc_top):\n",
    "    \n",
    "    figs = []\n",
    "    \n",
    "    for PC_I in range(0, pc_top.shape[1]):\n",
    "    \n",
    "        results = []\n",
    "        filtered_data = pd.concat([pd.DataFrame(spat, columns = ['zccf','yccf','Section']), pd.DataFrame(pc_top[:,PC_I],columns=[\"test\"])], axis=1)\n",
    "    \n",
    "        currentPC = \"test\"\n",
    "        for section in filtered_data['Section'].unique():\n",
    "            subset = filtered_data[filtered_data['Section'] == section]\n",
    "    \n",
    "            perc_2 = subset[currentPC].quantile(0.02)\n",
    "            perc_98 = subset[currentPC].quantile(0.98)\n",
    "    \n",
    "            results.append([section, perc_2, perc_98])\n",
    "        percentile_df = pd.DataFrame(results, columns=['Section', '2-perc', '98-perc'])\n",
    "        med2p = percentile_df['2-perc'].median()\n",
    "        med98p = percentile_df['98-perc'].median()\n",
    "    \n",
    "        cmap = plt.cm.PuOr\n",
    "    \n",
    "        fig, axes = plt.subplots(4, 8, figsize=(20, 10))\n",
    "        axes = axes.flatten()\n",
    "    \n",
    "        for section in range(1, 33):\n",
    "            ax = axes[section - 1]\n",
    "            ddf = filtered_data[(filtered_data['Section'] == section)]\n",
    "    \n",
    "            ax.scatter(ddf['zccf'], -ddf['yccf'], c=ddf[currentPC], cmap=\"PuOr\", s=0.5,rasterized=True, vmin=med2p, vmax=med98p) \n",
    "            ax.axis('off')\n",
    "            ax.set_aspect('equal')\n",
    "            ax.set_xlim(global_min_z, global_max_z)\n",
    "            ax.set_ylim(global_min_y, global_max_y)\n",
    "    \n",
    "        cbar_ax = fig.add_axes([0.92, 0.15, 0.02, 0.7])\n",
    "        norm = Normalize(vmin=med2p, vmax=med98p)\n",
    "        sm = ScalarMappable(norm=norm, cmap=cmap)\n",
    "        fig.colorbar(sm, cax=cbar_ax)\n",
    "    \n",
    "        plt.tight_layout(rect=[0, 0, 0.9, 1])\n",
    "        #plt.show()\n",
    "        \n",
    "        figs.append(fig)\n",
    "        \n",
    "    return figs\n",
    "\n",
    "def plot_spatial_localPCA_kMeans(dd2, kmeans_labels):\n",
    "\n",
    "    dd2 = pd.DataFrame(dd2, columns = ['zccf','yccf','Section'])\n",
    "    dd2['cat_code'] = pd.Series(np.array(kmeans_labels)).astype('category').cat.codes\n",
    "\n",
    "    color_map = {0: 'purple', 1: 'yellow'}\n",
    "    dd2['color'] = dd2['cat_code'].map(color_map)\n",
    "    \n",
    "    fig, axes = plt.subplots(4, 8, figsize=(40, 20))\n",
    "    axes = axes.flatten()\n",
    "    dot_size = 0.3\n",
    "    \n",
    "    sections_to_plot = range(1,33)\n",
    "    \n",
    "    for i, section_num in enumerate(sections_to_plot):\n",
    "        ax = axes[i]\n",
    "        xx = dd2[dd2[\"Section\"] == section_num]\n",
    "        #print(xx['color'].value_counts())\n",
    "        sc1 = ax.scatter(xx['zccf'], -xx['yccf'],\n",
    "                         c=np.array(xx['color']), s=dot_size, alpha=1, rasterized=True)\n",
    "        ax.axis('off')\n",
    "        ax.set_aspect('equal')  \n",
    "        ax.set_xlim(global_min_z, global_max_z)\n",
    "        ax.set_ylim(global_min_y, global_max_y)\n",
    "    \n",
    "    for j in range(i+1, len(axes)):\n",
    "        fig.delaxes(axes[j])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    #plt.show()\n",
    "    \n",
    "    return fig\n",
    "    \n",
    "def plot_embeddingsPCA(embeddings, kmeans_labels):\n",
    "    \n",
    "    figs = []\n",
    "    \n",
    "    for i in range(embeddings.shape[1]-1):\n",
    "        fig = plt.figure()\n",
    "        \n",
    "        plt.scatter(embeddings[:, i][::10], embeddings[:, (i+1)][::10], c=kmeans_labels[::10], s=0.005, alpha=0.5, rasterized=True)\n",
    "        #plt.show()\n",
    "        \n",
    "        figs.append(fig)\n",
    "        \n",
    "    return figs\n",
    "        \n",
    "def plot_tSNE(indexes, kmeans_labels):\n",
    "    \n",
    "    kmeans_labels = pd.DataFrame(kmeans_labels, index = indexes)\n",
    "    tsne_ds_now = tsne_ds.loc[np.intersect1d(indexes, np.array(tsne_ds.index)),:]\n",
    "    kmeans_labels = kmeans_labels.loc[tsne_ds_now.index,:]\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    \n",
    "    plt.scatter(tsne_ds.iloc[:,0], tsne_ds.iloc[:,1], s=0.0005, alpha=0.5, c=\"gray\", rasterized=True)\n",
    "    plt.scatter(tsne_ds_now.iloc[:,0], tsne_ds_now.iloc[:,1], s=0.005, alpha=1, c=np.array(kmeans_labels), rasterized=True)\n",
    "    #plt.show()\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e967c97-2065-4ece-80db-c0ed7f30c385",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function to assess whether a cluster is continuous along the AP axis\n",
    "\n",
    "# count the voxels per section as we'll need this as a normalization factor for continuity assessment\n",
    "vcnorm = coordinates.loc[data.index,'Section'].value_counts()\n",
    "vcnorm.index = vcnorm.index.astype(int)\n",
    "vcnorm = vcnorm.sort_index()\n",
    "\n",
    "def continuity_check(dd2, kmeans_labels):\n",
    "    dd2 = pd.DataFrame(dd2, columns = ['zccf','yccf','Section'])\n",
    "    dd2['color'] = pd.Series(kmeans_labels).astype('category').cat.codes\n",
    "\n",
    "    enough_sectionss = []\n",
    "    number_of_peakss = []\n",
    "    peak_ratios = []\n",
    "\n",
    "    for bbb in np.array(dd2['color'].unique()):\n",
    "        test = dd2.loc[dd2['color'] == bbb,] \n",
    "        value_counts = test['Section'].value_counts()\n",
    "        value_counts.index = value_counts.index.astype(int)\n",
    "        aap = value_counts.sort_index()\n",
    "        ap = np.array(aap)\n",
    "        ap[ap < 10] = 0\n",
    "\n",
    "        # check if in ap there are at least two consecutive nonzero entries or 4 total nonzero entries\n",
    "        ap_nonnull = np.sum(ap > 0)  > 3\n",
    "        apflag = False\n",
    "        for boh in range(len(ap) - 1):\n",
    "            if ap[boh] != 0 and ap[boh + 1] != 0:\n",
    "                apflag = True\n",
    "        enough_sections = ap_nonnull & apflag\n",
    "\n",
    "        # check the number of peaks and if >1 peak the ratio of peak heights to assess continuuty\n",
    "        ap = aap / vcnorm\n",
    "        ap = np.array(ap)\n",
    "        zero_padded_ap = np.pad(ap, pad_width=1, mode='constant', constant_values=0)\n",
    "        smoothed_ap = gaussian_filter1d(zero_padded_ap, sigma=1.8)\n",
    "    \n",
    "        #plt.plot(smoothed_ap)\n",
    "        #plt.show()\n",
    "        \n",
    "        peaks, properties = find_peaks(smoothed_ap, height=0)\n",
    "        number_of_peaks = len(peaks)\n",
    "        \n",
    "        if number_of_peaks > 1:\n",
    "            peak_heights = properties['peak_heights']\n",
    "            top_peaks = np.sort(peak_heights)[-2:]  # get the two highest peaks\n",
    "            peak_ratio = top_peaks[1] / top_peaks[0] \n",
    "        else:\n",
    "            peak_ratio = 10\n",
    "\n",
    "        enough_sectionss.append(enough_sections)\n",
    "        number_of_peakss.append(number_of_peaks)\n",
    "        peak_ratios.append(peak_ratio)\n",
    "\n",
    "    return enough_sectionss[0], enough_sectionss[1], number_of_peakss[0], number_of_peakss[1], peak_ratios[0], peak_ratios[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80bc7b7f-e480-4170-baba-3f999fcdd879",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function to check for differential lipids between two groups\n",
    "\n",
    "def differential_lipids(lipidata, kmeans_labels, min_fc=0.2, pthr=0.05):\n",
    "    results = []\n",
    "\n",
    "    a = lipidata[kmeans_labels == 0,:]\n",
    "    b = lipidata[kmeans_labels == 1,:]\n",
    "    \n",
    "    for rrr in range(lipidata.shape[1]):\n",
    "       \n",
    "        groupA = a[:,rrr]\n",
    "        groupB = b[:,rrr]\n",
    "    \n",
    "        # log2 fold change\n",
    "        meanA = np.mean(groupA)\n",
    "        meanB = np.mean(groupB)\n",
    "        log2fold_change = np.abs(np.log2(meanB / meanA)) if meanA > 0 and meanB > 0 else np.nan\n",
    "    \n",
    "        # Wilcoxon test\n",
    "        try:\n",
    "            _, p_value = mannwhitneyu(groupA, groupB, alternative='two-sided')\n",
    "        except ValueError:\n",
    "            p_value = np.nan\n",
    "    \n",
    "        results.append({'lipid': rrr, 'log2fold_change': log2fold_change, 'p_value': p_value})\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "\n",
    "    # correct for multiple testing\n",
    "    reject, pvals_corrected, _, _ = multipletests(results_df['p_value'].values, alpha=0.05, method='fdr_bh')\n",
    "    results_df['p_value_corrected'] = pvals_corrected\n",
    "    promoted = results_df.loc[(results_df['log2fold_change'] > min_fc) & (results_df['p_value_corrected'] < pthr),:]\n",
    "\n",
    "    alteredlips = np.sum((results_df['log2fold_change'] > min_fc) & (results_df['p_value_corrected'] < pthr))\n",
    "\n",
    "    return alteredlips, promoted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28b2957",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function to check for ranking PCs based on their spatial informativeness, meaning low variance of section-wise variances (i.e., low batch effect,\n",
    "# all sections live in the same world, and high mean of variances, i.e., high informativeness, individual sections tend to contain variation)\n",
    "\n",
    "def rank_features_by_combined_score(tempadata):\n",
    "    sections = tempadata.obsm['spatial'][:, 2]  \n",
    "    \n",
    "    unique_sections = np.unique(sections)\n",
    "\n",
    "    var_of_vars = []\n",
    "    mean_of_vars = []\n",
    "\n",
    "    for i in range(tempadata.X.shape[1]):\n",
    "        feature_values = tempadata.X[:, i]\n",
    "\n",
    "        section_variances = []\n",
    "        for section in unique_sections:\n",
    "            section_values = feature_values[sections == section]\n",
    "            section_variance = np.var(section_values)\n",
    "            section_variances.append(section_variance)\n",
    "\n",
    "        var_of_vars.append(np.var(section_variances))\n",
    "        mean_of_vars.append(np.mean(section_variances))\n",
    "\n",
    "    var_of_vars = np.array(var_of_vars)\n",
    "    mean_of_vars = np.array(mean_of_vars)\n",
    "\n",
    "    # put them in a comparable scale\n",
    "    var_of_vars = var_of_vars / np.mean(var_of_vars)\n",
    "    mean_of_vars = mean_of_vars / np.mean(mean_of_vars)\n",
    "\n",
    "    # give a bit more weight to the variability in the section vs the lack of batch effect\n",
    "    combined_score = -var_of_vars/2 + mean_of_vars\n",
    "\n",
    "    ranked_indices = np.argsort(combined_score)[::-1]\n",
    "\n",
    "    return ranked_indices\n",
    "\n",
    "# a function to find an elbow in cumulative absolute loadings to decide how many and which lipids have at least partially contributed to a split\n",
    "def find_elbow_point(values):\n",
    "    sorted_values = np.sort(np.abs(values))[::-1] \n",
    "    cumulative_variance = np.cumsum(sorted_values) / np.sum(sorted_values)\n",
    "    #plt.plot(cumulative_variance)\n",
    "    kneedle = KneeLocator(range(1, len(cumulative_variance) + 1), cumulative_variance, curve='concave', direction='increasing')\n",
    "    elbow = kneedle.elbow\n",
    "    #plt.scatter(elbow, cumulative_variance[elbow], c=\"red\")\n",
    "    #plt.show()\n",
    "    return elbow\n",
    "\n",
    "import itertools\n",
    "\n",
    "# a function to generate sorted combinations of components to be used to attempt a batch effect-free clustering step\n",
    "def generate_combinations(n, limit=200):\n",
    "    all_combinations = []\n",
    "    for r in range(n, 0, -1):\n",
    "        for combination in itertools.combinations(range(n), r):\n",
    "            all_combinations.append(combination)\n",
    "            if len(all_combinations) == limit:\n",
    "                return all_combinations\n",
    "    return all_combinations\n",
    "\n",
    "# define a function (to be optimized) for faster Leiden clustering\n",
    "def leidenalg_clustering(inputdata, Nneigh=40, Niter=5):\n",
    "    nn = NearestNeighbors(n_neighbors=Nneigh, n_jobs=4)\n",
    "    nn.fit(inputdata)\n",
    "    knn = nn.kneighbors_graph(inputdata)\n",
    "    \n",
    "    G = nx.Graph(knn)\n",
    "    \n",
    "    g = ig.Graph.from_networkx(G)\n",
    "    \n",
    "    partitions = leidenalg.find_partition(g, leidenalg.ModularityVertexPartition, n_iterations=Niter, seed=230598)\n",
    "    labels = np.array(partitions.membership)\n",
    "    \n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde0481d-a0c0-4fa6-af3e-d26738d4084b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "08d88118-6729-4e04-908a-9a49b40c4ccd",
   "metadata": {},
   "source": [
    "## Prepare the Node class to create a hierarchical classifier deployable on new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e56da9-6603-4393-8166-92077c735bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this class will store the tree of classifiers, it's the trained object that, given a lipidomic dataset, is able to return lipizones\n",
    "class Node:\n",
    "    def __init__(self, level, path=[]):\n",
    "        self.level = level\n",
    "        self.path = path\n",
    "        self.scaler = None\n",
    "        self.nmf = None\n",
    "        self.xgb_model = None\n",
    "        self.feature_importances = None  # store feature importances at each split to establish a minimal palette\n",
    "        self.children = {}\n",
    "        self.factors_to_use = None\n",
    "        \n",
    "def undersample(X, y, sampling_strategy='auto'):\n",
    "    rus = RandomUnderSampler(sampling_strategy=sampling_strategy, random_state=42)\n",
    "    X_resampled, y_resampled = rus.fit_resample(X, y)\n",
    "    return X_resampled, y_resampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3297b481-a1c0-470a-8d9f-6edc26215610",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define validation and test sections\n",
    "\n",
    "valsec = (coordinates['Section'].unique()[::5] + 2)[:-1]\n",
    "testsec = (coordinates['Section'].unique()[::5] + 1)[:-1]\n",
    "trainsec = np.setdiff1d(np.setdiff1d(coordinates['Section'].unique(), testsec), valsec)\n",
    "\n",
    "valpoints = coordinates.loc[coordinates['Section'].isin(valsec),:].index\n",
    "testpoints = coordinates.loc[coordinates['Section'].isin(testsec),:].index\n",
    "trainpoints = coordinates.loc[coordinates['Section'].isin(trainsec),:].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8e9610-8ece-42e9-a460-6db987a8d7e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cea5dbb0-c0e0-498e-a4b3-5957518c84b6",
   "metadata": {},
   "source": [
    "## Hierarchical bipartite splitter with NMF recalculation at each iteration and an XGBC on top of it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1003bf3e-58fd-4695-93d8-f6038f053e75",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## how many splits do we want?\n",
    "max_depth = 15 # go down where possible! it should automatically stop before everywhere given the biological plausibility criteria!\n",
    "\n",
    "# parameters\n",
    "K = 60 # big K for kmeans split before hierarchically reaggregating\n",
    "min_voxels = 150 # minimum number of observations per lipizone\n",
    "min_diff_lipids = 2 # minimum number of differential lipids (out of 108 used for clustering) to accept a split\n",
    "min_fc = 0.2 # minimal fold change required to consider lipids differential\n",
    "pthr = 0.05 # adjusted pvalue threshold for the wilcoxon test\n",
    "thr_signal = 0.0000000001 # threshold on signal to keep a component for clustering\n",
    "penalty1 = 1.5 # extent to penalize the embeddings of the previous split compared to the current one (division factor)\n",
    "penalty2 = 2 # extent to penalize the global embeddings compared to the current one (division factor)\n",
    "ACCTHR = 0.6 # lower bound generalization ability for the classifier to relabel and deem a split valid to go deeper\n",
    "#top_loaders_general = []\n",
    "\n",
    "def dosplit(current_adata, embds, path = [], splitlevel=0, plotting_active=False, loadings_active=False):\n",
    "    print(\"Splitting level: \"+str(splitlevel))\n",
    "    \n",
    "    # do not split further if the cluster is smaller than min_voxels\n",
    "    if current_adata.X.shape[0] < min_voxels:\n",
    "        print(\"one branch exhausted\")\n",
    "        return\n",
    "\n",
    "    # do NMF on the current subset of the data\n",
    "    nmfdf, loadings, N_factors, nmf = compute_seeded_NMF(pd.DataFrame(current_adata.X))\n",
    "    nmf_result = nmfdf.values\n",
    "    \n",
    "    # omit NMFs with very low signal overall\n",
    "    filter1 = np.abs(nmf_result).mean(axis=0) > thr_signal\n",
    "    loadings = loadings[filter1, :]\n",
    "    nmf_result = nmf_result[:, filter1]\n",
    "    original_nmf_indices = np.arange(N_factors)[filter1]\n",
    "\n",
    "    # rank the NMFs based on the variances score\n",
    "    tempadata = sc.AnnData(X=nmf_result)\n",
    "    tempadata.obsm['spatial'] = current_adata.obsm['spatial']\n",
    "    goodpcs = rank_features_by_combined_score(tempadata)\n",
    "    goodpcs_indices = original_nmf_indices[goodpcs.astype(int)]\n",
    "    top_pcs_data = nmf_result[:, goodpcs.astype(int)]\n",
    "    loadings = loadings[goodpcs.astype(int), :]\n",
    "\n",
    "    # attempt the kMeans split with large k, followed by backSPIN reaggregation \n",
    "    # start with the top ranking according to the variances criterion and go down until we find something that respects AP continuity criteria!\n",
    "    flag = False\n",
    "    aaa = 0\n",
    "    multiplets = generate_combinations(len(goodpcs))\n",
    "    \n",
    "    while (not flag) and (aaa < len(multiplets)):\n",
    "        bestpcs = multiplets[aaa]\n",
    "        embeddings = top_pcs_data[:,bestpcs]\n",
    "        loadings_sel = loadings[bestpcs,:]\n",
    "        selected_nmf_indices = goodpcs_indices[list(bestpcs)]\n",
    "        \n",
    "        # whiten the embeddings to prevent one direction to always dictate all splits - empirically results in better detection of blob-like structures\n",
    "        scaler = StandardScaler()\n",
    "        standardized_embeddings = scaler.fit_transform(embeddings)\n",
    "        standardized_embeddings = scaler.transform(embeddings)\n",
    "        \n",
    "        # do the kMeans with a big K\n",
    "        kmeans = KMeans(n_clusters=K, random_state=230598)\n",
    "        globembds = standardized_embeddings_GLOBAL.loc[current_adata.obs_names,:].values\n",
    "        embspace = np.concatenate((standardized_embeddings, embds/penalty1, globembds/penalty2),axis=1)\n",
    "        kmeans_labels = kmeans.fit_predict(embspace) \n",
    "        #kmeans_labels = leidenalg_clustering(embspace)\n",
    "       \n",
    "        # reaggregate hierarchically the centroids of the K clusters to get a bipartition using backSPIN\n",
    "        data_for_clustering = pd.DataFrame(current_adata.X,\n",
    "                                   index=current_adata.obs_names, \n",
    "                                   columns=current_adata.var_names)\n",
    "        data_for_clustering['label'] = kmeans_labels\n",
    "        centroids = data_for_clustering.groupby('label').mean()\n",
    "        centroids = pd.DataFrame(StandardScaler().fit_transform(centroids), columns=centroids.columns, index=centroids.index).T\n",
    "        row_ix, columns_ix = backSPIN.SPIN(centroids, widlist=4)\n",
    "        centroids = centroids.iloc[row_ix, columns_ix]\n",
    "        _, _, _, gr1, gr2, _, _ , _, _= backSPIN._divide_to_2and_resort(sorted_data=centroids.values, wid=5) # wid controls size of neighborhood\n",
    "        gr1 = np.array(centroids.columns)[gr1]\n",
    "        gr2 = np.array(centroids.columns)[gr2]\n",
    "        data_for_clustering['lab'] = 1\n",
    "        data_for_clustering['lab'][data_for_clustering['label'].isin(gr2)] = 2\n",
    "\n",
    "        # check the continuity of the resulting clusters along the AP axis\n",
    "        enough_sections0, enough_sections1, number_of_peaks0, number_of_peaks1, peak_ratio0, peak_ratio1 = continuity_check(current_adata.obsm['spatial'], np.array(data_for_clustering['lab']))\n",
    "\n",
    "        # check the differential lipids\n",
    "        alteredlips, promoted = differential_lipids(current_adata.obsm['lipids'].values, kmeans_labels, min_fc, pthr)\n",
    "\n",
    "        # also check that both branches of the split comprise at least N voxels (\"min cells\") - we don't try to trust super small clusters\n",
    "        flag = ((np.sum(kmeans_labels == 1) > min_voxels) | (np.sum(kmeans_labels == 0) > min_voxels)) & (alteredlips > min_diff_lipids) & enough_sections0 and  enough_sections1 and ((number_of_peaks0 <3) or (peak_ratio0 > 1.4)) and ((number_of_peaks1 <3) or (peak_ratio1 > 1.4))\n",
    "        aaa = aaa+1\n",
    "        kmeans_labels = data_for_clustering['lab'].astype(int)\n",
    "        \n",
    "    # don't split anymore here if no PCs choice can respect AP continuity and differential lipids criteria\n",
    "    if not flag:\n",
    "        print(\"one branch exhausted because embeddings do not respect bland criteria on rostrocaudal axis and diff. lipids\")\n",
    "        return\n",
    "    \n",
    "    \n",
    "    ############# train a classifier on these embeddings to predict the two labels with good generalization capability\n",
    "    \n",
    "    # extract the embeddings and the label for train, validation, and test set\n",
    "    embeddings = pd.DataFrame(embspace, index=current_adata.obs_names) # embeddings\n",
    "    X_train = embeddings.loc[embeddings.index.isin(trainpoints), :]\n",
    "    X_val = embeddings.loc[embeddings.index.isin(valpoints), :]\n",
    "    X_test = embeddings.loc[embeddings.index.isin(testpoints), :]\n",
    "    \n",
    "    kmeans_labels = kmeans_labels -1\n",
    "    y_train = kmeans_labels.loc[X_train.index] # split labels to be able to be recovered by a nonlinear XGB classifier\n",
    "    y_val = kmeans_labels.loc[X_val.index]\n",
    "    y_test = kmeans_labels.loc[X_test.index]\n",
    "    \n",
    "    # balance the classes, maybe it's not really needed\n",
    "    X_train_sub_US, y_train_sub_US = undersample(X_train, y_train)\n",
    "    \n",
    "    # train an XGB classifierwith good generalization ability on the validation set\n",
    "    xgb_model = XGBClassifier(\n",
    "        n_estimators=1000, \n",
    "        max_depth=8, \n",
    "        learning_rate=0.02, \n",
    "        subsample=0.8, \n",
    "        colsample_bytree=0.8, \n",
    "        gamma=0.5,  \n",
    "        random_state=42,\n",
    "        n_jobs=6 \n",
    "    )\n",
    "    xgb_model.fit(\n",
    "        X_train_sub_US, \n",
    "        y_train_sub_US,\n",
    "        eval_set=[(X_val, y_val)],\n",
    "        early_stopping_rounds=7,\n",
    "        verbose=False\n",
    "    )\n",
    "    print(\"XGBoosted!\")\n",
    "    print(datetime.now().strftime(\"%H:%M\"))\n",
    "    \n",
    "    feature_importances = xgb_model.feature_importances_\n",
    "\n",
    "    test_pred = xgb_model.predict(X_test)\n",
    "    test_accuracy = accuracy_score(y_test, test_pred)\n",
    "    print(f\"Test accuracy in this subset: {test_accuracy}\")\n",
    "    val_pred = xgb_model.predict(X_val)\n",
    "    val_accuracy = accuracy_score(y_val, val_pred)\n",
    "    print(f\"Validation accuracy in this subset: {val_accuracy}\")\n",
    "    train_pred = xgb_model.predict(X_train)\n",
    "    train_accuracy = accuracy_score(y_train, train_pred)\n",
    "    print(f\"Training accuracy in this subset: {train_accuracy}\")\n",
    "    \n",
    "    if test_accuracy < ACCTHR:\n",
    "        print(\"one branch exhausted due to poor test set accuracy\")\n",
    "        return\n",
    "    \n",
    "    test_pred = pd.DataFrame(test_pred, index=X_test.index, columns=[\"kmean\"])\n",
    "    val_pred = pd.DataFrame(val_pred, index=X_val.index, columns=[\"kmean\"])\n",
    "    train_pred = pd.DataFrame(train_pred, index=X_train.index, columns=[\"kmean\"])\n",
    "\n",
    "    # overwrite the cluster labels with those picked by the classifier\n",
    "    ind = kmeans_labels.index\n",
    "    kmeans_labels = pd.concat([test_pred, val_pred, train_pred]).loc[ind]\n",
    "    kmeans_labels = kmeans_labels+1\n",
    "    kmeans_labels = kmeans_labels['kmean']\n",
    "\n",
    "    cl1members = kmeans_labels[kmeans_labels == 1].index.values\n",
    "    cl2members = kmeans_labels[kmeans_labels == 2].index.values\n",
    "\n",
    "    # create a Node and store its transformations and feature importances\n",
    "    node = Node(splitlevel, path=path)\n",
    "    node.scaler = scaler\n",
    "    node.nmf = nmf\n",
    "    node.xgb_model = xgb_model\n",
    "    node.feature_importances = feature_importances\n",
    "    node.factors_to_use = selected_nmf_indices\n",
    "\n",
    "    standardized_embeddings = pd.DataFrame(standardized_embeddings, index=current_adata.obs_names)\n",
    "    cl1members = kmeans_labels[kmeans_labels == 1].index.values\n",
    "    cl2members = kmeans_labels[kmeans_labels == 2].index.values\n",
    "\n",
    "    current_adata1 = current_adata[current_adata.obs_names.isin(cl1members)]#.copy()\n",
    "    current_adata2 = current_adata[current_adata.obs_names.isin(cl2members)]#.copy()\n",
    "    embd1 = standardized_embeddings.loc[standardized_embeddings.index.isin(cl1members),:].values\n",
    "    embd2 = standardized_embeddings.loc[standardized_embeddings.index.isin(cl2members),:].values\n",
    "\n",
    "    splitlev = splitlevel+1\n",
    "\n",
    "    clusteringLOG.loc[cl1members, \"level_\"+str(splitlev)] = 1\n",
    "    clusteringLOG.loc[cl2members, \"level_\"+str(splitlev)] = 2\n",
    "    \n",
    "    \n",
    "    if plotting_active:\n",
    "        # show the result in space, and keep track of the lipids which have contributed to splits, counting them as a \"diversity metric\"\n",
    "        figs = plot_spatial_localPCA(current_adata.obsm['spatial'], embeddings)\n",
    "        for fig in figs:\n",
    "            pdf_pages.savefig(fig)\n",
    "            plt.close(fig)\n",
    "        \n",
    "        figs = plot_embeddingsPCA(standardized_embeddings, kmeans_labels)\n",
    "        for fig in figs:\n",
    "            pdf_pages.savefig(fig)\n",
    "            plt.close(fig)\n",
    "        \n",
    "        fig = plot_tSNE(np.array(data_for_clustering.index), kmeans_labels)\n",
    "        pdf_pages.savefig(fig)\n",
    "        plt.close(fig)\n",
    "        \n",
    "        fig = plot_spatial_localPCA_kMeans(current_adata.obsm['spatial'], kmeans_labels)\n",
    "        pdf_pages.savefig(fig)\n",
    "        plt.close(fig)\n",
    "\n",
    "    if loadings_active:\n",
    "        # check which lipids are dictating this split and the diversity of lipids that have been used until now\n",
    "        loadings_sel = pd.DataFrame(loadings_sel, columns = data.columns).T\n",
    "        loadings_sel = np.abs(loadings_sel)\n",
    "        top_loaders_ALL = []\n",
    "        for col in loadings_sel.columns:\n",
    "            elbow_point = find_elbow_point(loadings_sel[col].values)\n",
    "            top_loaders = loadings_sel.nlargest(elbow_point, col).index.tolist()\n",
    "            top_loaders_ALL = top_loaders_ALL+top_loaders\n",
    "            print(\"Top 5 drivers PC\" + str(col) + \": \" + str(top_loaders[:5]))\n",
    "        #top_loaders_general = top_loaders_general+top_loaders_ALL\n",
    "        diversity_metric = len(np.unique(top_loaders_ALL))\n",
    "        print(\"Diversity metric: \" + str(diversity_metric))\n",
    "        print(\"Differential lipids between the two branches: \")\n",
    "        print(np.array(adata.obsm['lipids'].columns)[promoted['lipid'].values].astype(str))\n",
    "\n",
    "    # do the recursion\n",
    "    if splitlev < (max_depth+1):\n",
    "        child_path0 = path + [0]\n",
    "        child_path1 = path + [1]\n",
    "        childnode0, childnode1 = dosplit(current_adata1, embd1, child_path0, splitlevel=splitlev), dosplit(current_adata2, embd2, child_path1, splitlevel=splitlev)\n",
    "        node.children[0] = childnode0\n",
    "        node.children[1] = childnode1\n",
    "        return node\n",
    "    else:\n",
    "        print(\"depth reached!\")\n",
    "        return\n",
    "\n",
    "DSFACT = 1 # downscale for testing if needed\n",
    "column_names = [f\"level_{i}\" for i in range(1,max_depth+1)]\n",
    "clusteringLOG = pd.DataFrame(0, index=data.index, columns=column_names)[::DSFACT]   \n",
    "root_node = dosplit(adata[::DSFACT], standardized_embeddings_GLOBAL[::DSFACT], splitlevel=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7903f816-291a-4f19-a2e5-3c64128f5b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the hierarchical bipartite tree to file\n",
    "tree = clusteringLOG\n",
    "tree['cluster'] = tree['level_1'].astype(str) + tree['level_2'].astype(str) + tree['level_3'].astype(str) + tree['level_4'].astype(str) + tree['level_5'].astype(str) + tree['level_6'].astype(str)  + tree['level_7'].astype(str)  + tree['level_8'].astype(str)  + tree['level_9'].astype(str)  + tree['level_10'].astype(str) + tree['level_11'].astype(str) + tree['level_12'].astype(str) + tree['level_13'].astype(str) + tree['level_14'].astype(str) + tree['level_15'].astype(str)\n",
    "tree['class'] = tree['level_1'].astype(str) + tree['level_2'].astype(str) + tree['level_3'].astype(str)\n",
    "tree.to_hdf(\"tree_clustering_down_clean.h5ad\", key=\"table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78f52ad-07e5-48bb-8aa6-70b7e4977f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the tree to file\n",
    "import pickle\n",
    "\n",
    "filename = \"rootnode_clustering_whole_clean.pkl\"\n",
    "with open(filename, \"wb\") as file:\n",
    "    pickle.dump(root_node, file) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032a06a9-8197-49a2-8bf8-27d14d0c70fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tree['cluster'].unique()) # oh lol wow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf4bf9f-ed7a-4bd0-9baf-bd3e3b90194e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36fbaa5-8155-474a-8276-8ad9a5458cd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f0fa71c1-2758-448e-ad93-d5a06b62e6c6",
   "metadata": {},
   "source": [
    "## Define a function to deploy the trained hierarchical classifier on new data and assign lipizones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d752516d-684b-4368-b670-20a86441e80c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def traverse_tree(node, current_adata, embds, paths, level=0):\n",
    "    print(level)\n",
    "    \n",
    "    if node is None or not node.children:\n",
    "        return\n",
    "\n",
    "    if current_adata.shape[0] == 0:\n",
    "        print(f\"Empty data at level {level}. Returning.\")\n",
    "        return\n",
    "    \n",
    "    # apply NMF to the current data subset\n",
    "    nmf = node.nmf\n",
    "    X_nmf = nmf.transform(current_adata.X)\n",
    "    \n",
    "    # select the factors used at this node\n",
    "    factors_to_use = node.factors_to_use\n",
    "    X_nmf = X_nmf[:, factors_to_use]\n",
    "\n",
    "    # scale (whiten) the NMF-transformed data\n",
    "    scaler = node.scaler\n",
    "    X_scaled = scaler.transform(X_nmf)\n",
    "\n",
    "    globembds = standardized_embeddings_GLOBAL.loc[current_adata.obs_names].values / penalty2\n",
    "    embspace = np.concatenate((X_scaled, embds / penalty1, globembds), axis=1)\n",
    "\n",
    "    # predict the child cluster using the stored XGBoost model\n",
    "    xgb_model = node.xgb_model\n",
    "    child_labels = xgb_model.predict(embspace)\n",
    "    \n",
    "    unique_labels, counts = np.unique(child_labels, return_counts=True)\n",
    "    \n",
    "    for i, index in enumerate(current_adata.obs_names):\n",
    "        if index not in paths:\n",
    "            paths[index] = []\n",
    "        paths[index].append(child_labels[i])\n",
    "\n",
    "    cl0members = current_adata.obs_names[child_labels == 0]\n",
    "    cl1members = current_adata.obs_names[child_labels == 1]\n",
    "\n",
    "    current_adata0 = current_adata[current_adata.obs_names.isin(cl0members)]\n",
    "    current_adata1 = current_adata[current_adata.obs_names.isin(cl1members)]\n",
    "\n",
    "    if current_adata0.shape[0] == 0 or current_adata1.shape[0] == 0:\n",
    "        print(f\"Warning: One child node has no data at level {level}. Skipping.\")\n",
    "        return\n",
    "\n",
    "    embd0 = X_scaled[child_labels == 0]\n",
    "    embd1 = X_scaled[child_labels == 1]\n",
    "\n",
    "    # recursively traverse the child nodes\n",
    "    traverse_tree(node.children[0], current_adata0, embd0, paths, level + 1)\n",
    "    traverse_tree(node.children[1], current_adata1, embd1, paths, level + 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0759ed5f-5f95-42f3-8c0d-2339daab586a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the lipizone prediction function on the data itself to make sure it is consistent with the annotations we derived from the clustering method\n",
    "\n",
    "paths = {}\n",
    "DSFACT = 1\n",
    "embds = standardized_embeddings_GLOBAL[::DSFACT].values\n",
    "traverse_tree(root_node, adata[::DSFACT], embds, paths)\n",
    "df_paths = pd.DataFrame.from_dict(paths, orient='index')\n",
    "df_paths = df_paths.fillna(-1)\n",
    "df_paths = df_paths.astype(int) + 1\n",
    "df_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846b1785-e590-4bb6-98e0-3e6d50dea19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_paths.to_hdf(\"df_paths_clean.h5ad\", key=\"table\")\n",
    "df_paths.columns = \"level_\" + (df_paths.columns+1).astype(str)\n",
    "for i in range(2,11):\n",
    "    df_paths['level_'+ str(i)] = df_paths['level_'+ str(i)].astype(str) + df_paths['level_'+ str(i-1)].astype(str)\n",
    "    tree['level_'+ str(i)] = tree['level_'+ str(i)].astype(str) + tree['level_'+ str(i-1)].astype(str)\n",
    "    print(np.sum(df_paths['level_'+ str(i)] != tree['level_'+ str(i)]) / df_paths.shape[0])\n",
    "df_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c429f9-385e-4f18-b9a3-38b42f1b7a0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4bf91591-39e8-4d67-8854-ce45ccf5702c",
   "metadata": {},
   "source": [
    "## Visualize the resulting lipizones with unique colors in space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5655c6-dabb-4a2f-a86d-76b33a2db706",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# represent all clusters in space with nice colors\n",
    "\n",
    "def spatial_plot_all_lipizones_atlas(data, coordinates, tree):\n",
    "    \n",
    "    conto = np.load(\"eroded_annot.npy\")\n",
    "\n",
    "    coordinates = coordinates.fillna(0)\n",
    "    coordinates = coordinates.replace([np.inf, -np.inf], 0)\n",
    "    xs,ys,zs = (coordinates['xccf']*40).astype(int), (coordinates['yccf']*40).astype(int), (coordinates['zccf']*40).astype(int)\n",
    "    xs.loc[xs>527]=527\n",
    "    ys.loc[ys>319]=319\n",
    "    zs.loc[zs>455]=455\n",
    "    coordinates['border'] = conto[xs,ys,zs]\n",
    "    \n",
    "    data_std = (data.values - np.mean(data.values, axis=1)[:, None]) / (np.std(data.values, axis=1)[:, None] + 1e-8)\n",
    "    data_std = pd.DataFrame(data_std, index=data.index, columns= np.array(data.columns)) \n",
    "    \n",
    "    levels = pd.concat([data_std, coordinates.loc[data_std.index,:], tree], axis=1) #_std\n",
    "    \n",
    "    dd2 = levels\n",
    "    \n",
    "    lipid_columns = np.array(data.columns)\n",
    "    \n",
    "    divisions = dd2['class'].unique()\n",
    "    colormaps = [\"RdYlBu\",  \"terrain\", \"PiYG\", \"cividis\", \"plasma\", \"PuRd\", \"inferno\", \"PuOr\"]\n",
    "    \n",
    "    \n",
    "    \n",
    "    dd2['R'] = np.nan\n",
    "    dd2['G'] = np.nan\n",
    "    dd2['B'] = np.nan\n",
    "    \n",
    "    dfs = []\n",
    "\n",
    "    for division, colormap_name in tqdm(zip(divisions, colormaps)):\n",
    "    \n",
    "        if len(dd2.loc[dd2['class'] == division, 'cluster'].unique()) > 1:\n",
    "            \n",
    "            print(division)\n",
    "            \n",
    "            datasubaqueo = dd2[dd2['class'] == division]\n",
    "        \n",
    "            clusters = datasubaqueo['cluster'].unique()\n",
    "        \n",
    "            lipid_df = pd.DataFrame(columns=lipid_columns)\n",
    "        \n",
    "            for i in range(len(clusters)):\n",
    "                datasub = datasubaqueo[datasubaqueo['cluster'] == clusters[i]] \n",
    "                lipid_data = datasub.loc[:,lipid_columns].mean(axis=0)\n",
    "                lipid_row = pd.DataFrame([lipid_data], columns=lipid_columns)\n",
    "                lipid_df = pd.concat([lipid_df, lipid_row], ignore_index=True)\n",
    "        \n",
    "            column_means = lipid_df.mean()\n",
    "            normalized_lipid_df = lipid_df.div(column_means, axis='columns')\n",
    "            \n",
    "            normalized_lipid_df.index = clusters\n",
    "            normalized_lipid_df = normalized_lipid_df.T\n",
    "        \n",
    "            pca_columns = datasubaqueo.loc[:, lipid_columns]\n",
    "            grouped_data = datasubaqueo[['cluster']].join(pca_columns)\n",
    "            centroids = grouped_data.groupby('cluster').mean()\n",
    "        \n",
    "            distance_matrix = squareform(pdist(centroids, metric='euclidean'))\n",
    "            distance_df = pd.DataFrame(distance_matrix, index=centroids.index, columns=centroids.index)\n",
    "        \n",
    "            np.fill_diagonal(distance_df.values, np.inf)\n",
    "            initial_min_index = np.unravel_index(np.argmin(distance_df.values), distance_df.shape)\n",
    "            ordered_elements = [distance_df.index[initial_min_index[0]], distance_df.columns[initial_min_index[1]]]\n",
    "            distances = [0, distance_df.iloc[initial_min_index]]\n",
    "        \n",
    "            while len(ordered_elements) < len(distance_df):\n",
    "                last_added = ordered_elements[-1]\n",
    "                remaining_distances = distance_df.loc[last_added, ~distance_df.columns.isin(ordered_elements)]\n",
    "                next_element = remaining_distances.idxmin()\n",
    "                ordered_elements.append(next_element)\n",
    "                distances.append(remaining_distances[next_element])\n",
    "    \n",
    "            ordered_elements\n",
    "        \n",
    "            leaf_sequence = ordered_elements\n",
    "        \n",
    "            sequential_distances = distances\n",
    "        \n",
    "            cumulative_sequential_distances = np.cumsum(sequential_distances)\n",
    "        \n",
    "            normalized_distances = cumulative_sequential_distances / cumulative_sequential_distances[-1]\n",
    "            colormap = plt.get_cmap(colormap_name)\n",
    "            colors = [colormap(value) for value in normalized_distances]\n",
    "        \n",
    "            hsv_colors = [mcolors.rgb_to_hsv(rgb[:3]) for rgb in colors] \n",
    "        \n",
    "            modified_hsv_colors = []\n",
    "            for i, (h, s, v) in enumerate(hsv_colors):\n",
    "                if (i + 1) % 2 != 0: \n",
    "                    s = min(1, s + 0.7 * s)\n",
    "                modified_hsv_colors.append((h, s, v))\n",
    "    \n",
    "            modified_rgb_from_hsv = [mcolors.hsv_to_rgb(hsv) for hsv in modified_hsv_colors]\n",
    "        \n",
    "            rgb_list = [list(rgb) for rgb in modified_rgb_from_hsv]\n",
    "        \n",
    "            lipocolor = pd.DataFrame(rgb_list, index=leaf_sequence, columns=['R', 'G', 'B'])\n",
    "        \n",
    "            lipocolor_reset = lipocolor.reset_index().rename(columns={'index': 'cluster'})\n",
    "            print(lipocolor_reset)\n",
    "            indices = datasubaqueo.index\n",
    "            \n",
    "            datasubaqueo = datasubaqueo.iloc[:,:-3]\n",
    "            datasubaqueo = pd.merge(datasubaqueo, lipocolor_reset, on='cluster', how='left')\n",
    "        \n",
    "            datasubaqueo.index = indices\n",
    "        \n",
    "            dd2.update(datasubaqueo[['R', 'G', 'B']])\n",
    "    \n",
    "        else:\n",
    "            datasubaqueo = dd2[dd2['class'] == division]\n",
    "            datasubaqueo['R'] = 0\n",
    "            datasubaqueo['G'] = 0\n",
    "            datasubaqueo['B'] = 0\n",
    "            dd2.update(datasubaqueo[['R', 'G', 'B']])\n",
    "\n",
    "    def rgb_to_hex(r, g, b):\n",
    "        try:\n",
    "            \"\"\"Convert RGB (0-1 range) to hexadecimal color.\"\"\"\n",
    "            r, g, b = [int(255 * x) for x in [r, g, b]]  # scale to 0-255\n",
    "            return f'#{r:02x}{g:02x}{b:02x}'\n",
    "        except:\n",
    "            return np.nan\n",
    "    \n",
    "    dd2['lipizone_color'] = dd2.apply(lambda row: rgb_to_hex(row['R'], row['G'], row['B']), axis=1)\n",
    "    \n",
    "    dd2['lipizone_color'].fillna('gray', inplace=True) \n",
    "\n",
    "    fig, axes = plt.subplots(4, 8, figsize=(40, 20))\n",
    "    axes = axes.flatten()\n",
    "    dot_size = 0.3\n",
    "    \n",
    "    sections_to_plot = range(1,33)\n",
    "    \n",
    "    global_min_z = dd2['zccf'].min()\n",
    "    global_max_z = dd2['zccf'].max()\n",
    "    global_min_y = -dd2['yccf'].max() \n",
    "    global_max_y = -dd2['yccf'].min()  \n",
    "    \n",
    "    for i, section_num in enumerate(sections_to_plot):\n",
    "        ax = axes[i]\n",
    "        xx = dd2[dd2[\"Section\"] == section_num]\n",
    "        sc1 = ax.scatter(xx['zccf'], -xx['yccf'],\n",
    "                         c=np.array(xx['lipizone_color']), s=dot_size, alpha=1, rasterized=True)\n",
    "        \n",
    "        cont = coordinates.loc[xx.index,:]\n",
    "        cont = cont.loc[cont['border'] == 1,:]\n",
    "\n",
    "        ## add an overlaid contour\n",
    "        ax.scatter(cont['zccf'], -cont['yccf'],\n",
    "                         c='black', s=dot_size, alpha=0.2, rasterized=True)\n",
    "        \n",
    "        ax.axis('off')\n",
    "        ax.set_aspect('equal')  \n",
    "        ax.set_xlim(global_min_z, global_max_z)\n",
    "        ax.set_ylim(global_min_y, global_max_y)\n",
    "\n",
    "    for j in range(i+1, len(axes)):\n",
    "        fig.delaxes(axes[j])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return dd2['lipizone_color']\n",
    "\n",
    "lipizone_colors = spatial_plot_all_lipizones_atlas(data, coordinates, tree)\n",
    "tree.to_hdf(\"colorzones.h5ad\", key=\"table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a37ffd-7233-4f68-a603-238581304535",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# check in space how the spliter behaved in subsequent iterations\n",
    "\n",
    "def spatial_plots_splitter(coordinates, tree):\n",
    "    \n",
    "    dd2 = pd.concat([coordinates.loc[tree.index,:], tree], axis=1)\n",
    "\n",
    "    for levelnow in range(1,5):\n",
    "        fig, axes = plt.subplots(4, 8, figsize=(40, 20))\n",
    "        axes = axes.flatten()\n",
    "        dot_size = 0.3\n",
    "        \n",
    "        if levelnow > 1:\n",
    "            dd2['level_'+str(levelnow)] = dd2['level_'+str(levelnow-1)].astype(str) + dd2['level_'+str(levelnow)].astype(str)\n",
    "        \n",
    "        sections_to_plot = range(1,33)\n",
    "        \n",
    "        global_min_z = dd2['zccf'].min()\n",
    "        global_max_z = dd2['zccf'].max()\n",
    "        global_min_y = -dd2['yccf'].max() \n",
    "        global_max_y = -dd2['yccf'].min()  \n",
    "        \n",
    "        for i, section_num in enumerate(sections_to_plot):\n",
    "            ax = axes[i]\n",
    "            xx = dd2[dd2[\"Section\"] == section_num]\n",
    "            sc1 = ax.scatter(xx['zccf'], -xx['yccf'],\n",
    "                             c=xx['level_'+str(levelnow)].astype(\"category\").cat.codes, cmap=\"tab20\", s=dot_size, alpha=1, rasterized=True)\n",
    "            ax.axis('off')\n",
    "            ax.set_aspect('equal')  \n",
    "            ax.set_xlim(global_min_z, global_max_z)\n",
    "            ax.set_ylim(global_min_y, global_max_y)\n",
    "        \n",
    "        for j in range(i+1, len(axes)):\n",
    "            fig.delaxes(axes[j])\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "spatial_plots_splitter(coordinates, tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b1c2f4-d1d5-45d6-b804-7662947d39c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e3dcb104-a37c-424d-8ee4-b347cb6242ce",
   "metadata": {},
   "source": [
    "## Analyze the splitter behavior in feature space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1782b3-f4f8-44f5-b151-75503fa6a08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the distribution of NMFs for each cluster, at various splitter levels\n",
    "# to make sure we are no more naively cutting a trivial gradient of a single component recurrently\n",
    "tree_and_pcs = pd.concat([tree, standardized_embeddings_GLOBAL],axis=1)\n",
    "\n",
    "pcs = np.arange(15)\n",
    "for iii in range(1,7):\n",
    "    fig, axs = plt.subplots(3, 5, figsize=(18, 12))\n",
    "\n",
    "    axs = axs.flatten()\n",
    "\n",
    "    for i, pc in enumerate(pcs):\n",
    "        tree_and_pcs.boxplot(column=pc, by='level_' + str(iii), ax=axs[i], showfliers=False)\n",
    "        axs[i].set_title(f'Boxplot of {pc} by level_' + str(iii))\n",
    "        axs[i].set_xlabel('') \n",
    "        axs[i].set_ylabel('Value')\n",
    "\n",
    "    for j in range(len(pcs), len(axs)):\n",
    "        fig.delaxes(axs[j])\n",
    "\n",
    "    fig.suptitle('Boxplots of PCs by level_' + str(iii), fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=0.9)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860393ba-e9cf-483a-b79e-5df33866cbd3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## check how we are splitting onto the tSNE\n",
    "tsne = tsne_df.loc[data.index,:].values\n",
    "\n",
    "plt.figure(figsize=(18, 6))\n",
    "\n",
    "for i in range(5): \n",
    "    plt.subplot(2, 6, i + 1)\n",
    "    plt.scatter(tsne[:, 0], tsne[:, 1], c=tree.iloc[:,i].astype('category').cat.codes, cmap=\"tab20\", s=0.05, alpha=0.5)\n",
    "    plt.title(f'Splitter Level {i + 1}', fontsize=18)\n",
    "    plt.tick_params(axis='both', which='both', bottom=False, top=False, left=False, right=False, labelbottom=False, labelleft=False)\n",
    "    for spine in plt.gca().spines.values():\n",
    "        spine.set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f0c117-a437-49f0-8a60-0165f497d00f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fbac0622-8859-4903-9a04-293915f02d73",
   "metadata": {},
   "source": [
    "## Assign a name to each lipizone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e901ce29-b64c-4e6f-bdc6-ae250afaf236",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def regional_names_lipizones(lba, tree, normalized_lips):\n",
    "    \n",
    "    # calculate the lipizone centroid lipidomic profile                             \n",
    "    tree['lipizone'] = tree['cluster']\n",
    "    normalized_lips['lipizone'] = tree['lipizone']\n",
    "    centroids = normalized_lips.groupby('lipizone').mean()\n",
    "    \n",
    "    # import the dataset where we annotated the Allen ID to annotate the most represented region\n",
    "    file_path = 'allen_name_to_annots.pkl'\n",
    "    \n",
    "    with open(file_path, 'rb') as file:\n",
    "        allen_name_to_annots = pickle.load(file)\n",
    "    \n",
    "    regions = [\n",
    "        \"Frontal pole, cerebral cortex\",\n",
    "        \"Somatomotor areas\",\n",
    "        \"Somatosensory areas\",\n",
    "        \"Gustatory areas\",\n",
    "        \"Visceral area\",\n",
    "        \"Auditory areas\",\n",
    "        \"Visual areas\",\n",
    "        \"Anterior cingulate area\",\n",
    "        \"Prelimbic area\",\n",
    "        \"Infralimbic area\",\n",
    "        \"Orbital area\",\n",
    "        \"Agranular insular area\",\n",
    "        \"Retrosplenial area\",\n",
    "        \"Posterior parietal association areas\",\n",
    "        \"Temporal association areas\",\n",
    "        \"Perirhinal area\",\n",
    "        \"Ectorhinal area\",\n",
    "        \"Olfactory areas\",\n",
    "        \"Ammon's horn\",\n",
    "        \"Dentate gyrus\",\n",
    "        \"Fasciola cinerea\",\n",
    "        \"Induseum griseum\",\n",
    "        \"Retrohippocampal region\",\n",
    "        \"Layer 6b, isocortex\",\n",
    "        \"Claustrum\",\n",
    "        \"Endopiriform nucleus\",\n",
    "        \"Lateral amygdalar nucleus\",\n",
    "        \"Basolateral amygdalar nucleus\",\n",
    "        \"Basomedial amygdalar nucleus\",\n",
    "        \"Posterior amygdalar nucleus\",\n",
    "        \"Striatum dorsal region\",\n",
    "        \"Striatum ventral region\",\n",
    "        \"Nucleus accumbens\",\n",
    "        \"Fundus of striatum\",\n",
    "        \"Olfactory tubercle\",\n",
    "        \"Lateral strip of striatum\",\n",
    "        \"Lateral septal complex\",\n",
    "        \"Striatum-like amygdalar nuclei\",\n",
    "        \"Pallidum\",\n",
    "        \"Thalamus, sensory-motor cortex related\",\n",
    "        \"Thalamus, polymodal association cortex related\",\n",
    "        \"Periventricular zone\",\n",
    "        \"Periventricular region\",\n",
    "        \"Hypothalamic medial zone\",\n",
    "        \"Hypothalamic lateral zone\",\n",
    "        \"Median eminence\",\n",
    "        \"Midbrain, sensory related\",\n",
    "        \"Midbrain, motor related\",\n",
    "        \"Midbrain, behavioral state related\",\n",
    "        \"Pons, sensory related\",\n",
    "        \"Pons, motor related\",\n",
    "        \"Pons, behavioral state related\",\n",
    "        \"Medulla, sensory related\",\n",
    "        \"Medulla, motor related\",\n",
    "        \"Medulla, behavioral state related\",\n",
    "        \"Cerebellar cortex\",\n",
    "        \"Cerebellar nuclei\",\n",
    "        \"cranial nerves\",\n",
    "        \"cerebellum related fiber tracts\",\n",
    "        \"supra-callosal cerebral white matter\",\n",
    "        \"lateral forebrain bundle system\",\n",
    "        \"extrapyramidal fiber systems\",\n",
    "        \"medial forebrain bundle system\",\n",
    "        \"ventricular systems\"\n",
    "    ]\n",
    "    \n",
    "    lba['region'] = \"General\"\n",
    "    for i in regions:\n",
    "        lba['region'][lba['id'].isin(allen_name_to_annots[i])] = i\n",
    "\n",
    "    # give a name to the lipizones based on their enriched anatomical region\n",
    "    # use enrichments instead of absolute counts to avoid favoring large regions\n",
    "    lba['cluster'] = tree['cluster']\n",
    "    lba = lba[lba['region'] != 'root']\n",
    "    grouped = lba.groupby('cluster')['region'].value_counts()\n",
    "    total_counts = lba['region'].value_counts()\n",
    "    average_occurrence = total_counts / len(lba['cluster'].unique())\n",
    "    \n",
    "    most_enriched_acronyms = {}\n",
    "    \n",
    "    for color, group in grouped.groupby(level=0):\n",
    "        enrichment = group / average_occurrence[group.index.get_level_values('region')]\n",
    "        most_enriched_acronyms[color] = enrichment.idxmax()[1]\n",
    "    \n",
    "    acronym_counts = {}    \n",
    "    \n",
    "    # diversify by progressive numbering\n",
    "    for key, value in most_enriched_acronyms.items():\n",
    "        if value in acronym_counts:\n",
    "            acronym_counts[value] += 1\n",
    "        else:\n",
    "            acronym_counts[value] = 1\n",
    "    \n",
    "        most_enriched_acronyms[key] = f\"{value} {acronym_counts[value]}\"\n",
    "    \n",
    "    lba = lba.loc[tree.index,:]\n",
    "    lba['lipizone_names'] = lba['cluster'].map(most_enriched_acronyms)\n",
    "\n",
    "    return lba['lipizone_names']\n",
    "\n",
    "lba = pd.read_hdf(\"20241104_FullyAnnotatedLBA.h5ad\", key=\"table\")\n",
    "lipizone_names = regional_names_lipizones(lba, tree, normalized_datemp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9154c8-5012-4453-a1af-a619bb80aa8a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plot the lipizones one by one to file\n",
    "\n",
    "levels = pd.concat([coordinates.loc[data.index,:], tree], axis=1)\n",
    "levels['lipizone_names'] = lipizone_names\n",
    "levels['Section'] = coordinates['Section']\n",
    "levels['zccf'] = coordinates['zccf']\n",
    "levels['yccf'] = coordinates['yccf']\n",
    "levels['xccf'] = coordinates['xccf']\n",
    "\n",
    "output_folder = \"lipizones539\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "dot_size = 0.3\n",
    "sections_to_plot = range(1, 33)\n",
    "dd2 = levels\n",
    "global_min_z = dd2['zccf'].min()\n",
    "global_max_z = dd2['zccf'].max()\n",
    "global_min_y = -dd2['yccf'].max()\n",
    "global_max_y = -dd2['yccf'].min()\n",
    "unique_lev4cols = np.sort(dd2['lipizone_names'].unique())\n",
    "\n",
    "for unique_val in tqdm(unique_lev4cols):\n",
    "    print(unique_val)\n",
    "    fig, axes = plt.subplots(4, 8, figsize=(40, 20))\n",
    "    axes = axes.flatten()\n",
    "    for i, section_num in enumerate(sections_to_plot):\n",
    "        ax = axes[i]\n",
    "        xx = dd2[dd2[\"Section\"] == section_num]\n",
    "        sc1 = ax.scatter(xx['zccf'], -xx['yccf'], c=xx['lipizone_names'].astype(\"category\").cat.codes,\n",
    "                         cmap='Grays', s=dot_size * 2, alpha=0.2, rasterized=True)\n",
    "        xx_highlight = xx[xx['lipizone_names'] == unique_val]\n",
    "        sc2 = ax.scatter(xx_highlight['zccf'], -xx_highlight['yccf'],\n",
    "                         c='red', s=dot_size, alpha=1, rasterized=True)\n",
    "        ax.axis('off')\n",
    "        ax.set_aspect('equal')\n",
    "        ax.set_xlim(global_min_z, global_max_z)\n",
    "        ax.set_ylim(global_min_y, global_max_y)\n",
    "    for j in range(i + 1, len(axes)):\n",
    "        fig.delaxes(axes[j])\n",
    "    plt.suptitle(unique_val)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    filename = f\"{unique_val}.pdf\"\n",
    "    filepath = os.path.join(output_folder, filename)\n",
    "    plt.savefig(filepath)\n",
    "    plt.show()\n",
    "    plt.close(fig)\n",
    "\n",
    "subfolder_name = \"lipizones589\" # note this should be updated after running the XGBC\n",
    "cwd = os.getcwd()\n",
    "subfolder_path = os.path.join(cwd, subfolder_name)\n",
    "merger = PdfMerger()\n",
    "\n",
    "for filename in tqdm(sorted(os.listdir(subfolder_path))):\n",
    "    if filename.endswith(\".pdf\"):\n",
    "        file_path = os.path.join(subfolder_path, filename)\n",
    "        merger.append(file_path)\n",
    "\n",
    "output_filename = \"20241125_lipizones_539.pdf\"\n",
    "output_file_path = os.path.join(cwd, output_filename)\n",
    "merger.write(output_file_path)\n",
    "merger.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb20838-7158-476b-9adc-9a26c727e0ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84db0ac-4d12-41e2-9db7-477935e1a2b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
