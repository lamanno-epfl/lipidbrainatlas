{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26584e6c-69d5-47bb-919d-5c123b745b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "connectome = pd.read_parquet(\"./connectomic_datasets/connectome2992features.parquet\")\n",
    "connectome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87cb808-e4d9-43c6-a9ae-ee2e98acc412",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scanpy as sc\n",
    "import anndata\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.cm import ScalarMappable\n",
    "from matplotlib.colors import Normalize\n",
    "from scipy.cluster.hierarchy import linkage, fcluster\n",
    "from scipy.spatial.distance import squareform\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import NMF\n",
    "from openTSNE import TSNEEmbedding, affinity, initialization\n",
    "from tqdm import tqdm\n",
    "from collections import deque\n",
    "import harmonypy as hm\n",
    "import networkx as nx\n",
    "from threadpoolctl import threadpool_limits, threadpool_info\n",
    "\n",
    "# configure thread limits\n",
    "threadpool_limits(limits=8)\n",
    "os.environ['OMP_NUM_THREADS'] = '6'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c8b49d-6913-4440-8efd-0508b9b5ccc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_parquet(\"atlas.parquet\")\n",
    "coordinates = data[['Section', 'xccf', 'yccf', 'zccf']]\n",
    "coordinates['Section'] = coordinates['Section'].astype(int)\n",
    "\n",
    "data = connectome\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d36173-b767-448d-97a5-e9adcae7f1c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# use NMF to decompose the data into factors\n",
    "\n",
    "def compute_seeded_NMF(data):  # data is a dataframe pixels x lipids\n",
    "    # 1. calculate the correlation matrix of this dataset\n",
    "    corr = np.corrcoef(data.values.T)\n",
    "    corr_matrix = np.abs(corr)  # anticorrelated lipids convey the same info\n",
    "    np.fill_diagonal(corr_matrix, 0)\n",
    "    \n",
    "    adata = anndata.AnnData(X=np.zeros_like(corr_matrix))\n",
    "    adata.obsp['connectivities'] = csr_matrix(corr_matrix)\n",
    "    adata.uns['neighbors'] = {\n",
    "        'connectivities_key': 'connectivities',\n",
    "        'distances_key': 'distances',\n",
    "        'params': {'n_neighbors': 10, 'method': 'custom'}\n",
    "    }\n",
    "    \n",
    "    G = nx.from_numpy_array(corr_matrix)\n",
    "    \n",
    "    # span reasonable Leiden resolution parameters\n",
    "    gamma_values = np.linspace(0.8, 1.5, num=6) ##########################\n",
    "    num_communities = []\n",
    "    modularity_scores = []\n",
    "    objective_values = []\n",
    "    \n",
    "    for gamma in gamma_values:\n",
    "        sc.tl.leiden(adata, resolution=gamma, key_added=f'leiden_{gamma}')\n",
    "        clusters = adata.obs[f'leiden_{gamma}'].astype(int).values\n",
    "        num_comms = len(np.unique(clusters))\n",
    "        num_communities.append(num_comms)\n",
    "        partition = {i: clusters[i] for i in range(len(clusters))}\n",
    "        modularity = nx.community.modularity(G, [np.where(clusters == i)[0] for i in range(num_comms)])\n",
    "        modularity_scores.append(modularity)\n",
    "    \n",
    "    # 3. pick a number of blocks that is relatively high while preserving good modularity\n",
    "    epsilon = 1e-10\n",
    "    alpha = 0.7  # controls the weight of modularity vs pushing higher the number of communities\n",
    "    for Q, N_c in zip(modularity_scores, num_communities):\n",
    "        f_gamma = Q**alpha * np.log(N_c + 1 + epsilon)\n",
    "        objective_values.append(f_gamma)\n",
    "        \n",
    "    plt.plot(np.arange(len(objective_values)), objective_values)\n",
    "    plt.title(\"obj\")\n",
    "    plt.show()\n",
    "    \n",
    "    plt.plot(np.arange(len(modularity_scores)), modularity_scores)\n",
    "    plt.title(\"mod\")\n",
    "    plt.show()\n",
    "        \n",
    "    plt.plot(np.arange(len(num_communities)), num_communities)\n",
    "    plt.title(\"ncomms\")\n",
    "    plt.show()\n",
    "    \n",
    "    max_index = np.argmax(objective_values)\n",
    "    best_gamma = gamma_values[max_index]\n",
    "    best_modularity = modularity_scores[max_index]\n",
    "    best_num_comms = num_communities[max_index]\n",
    "    print(f'Number of communities at best gamma: {best_num_comms}')\n",
    "    \n",
    "    sc.tl.leiden(adata, resolution=best_gamma, key_added='leiden_best') # run Leiden one final time with best parameters\n",
    "    clusters = adata.obs['leiden_best'].astype(int).values\n",
    "    print(clusters)\n",
    "    \n",
    "    N_factors = best_num_comms\n",
    "    \n",
    "    # 4. pick a representative lipid from each block, use to initialize W\n",
    "    dist = 1 - corr_matrix\n",
    "    np.fill_diagonal(dist, 0)\n",
    "    dist = np.maximum(dist, dist.T)  # as numerical instability makes it unreasonably asymmetric\n",
    "    dist_condensed = squareform(dist, checks=True)\n",
    "    representatives = []\n",
    "    \n",
    "    for i in range(0, N_factors):\n",
    "        cluster_members = np.where(clusters == i)[0]\n",
    "        print(cluster_members)\n",
    "        if len(cluster_members) > 0:  # find most central feature in cluster\n",
    "            mean_dist = dist[cluster_members][:, cluster_members].mean(axis=1)\n",
    "            central_idx = cluster_members[np.argmin(mean_dist)]\n",
    "            representatives.append(central_idx)\n",
    "    \n",
    "    W_init = data.values[:, representatives]\n",
    "    \n",
    "    # 5. initialize H as a subset of the correlation matrix\n",
    "    H_init = corr[representatives,:]\n",
    "    H_init[H_init < 0.] = 0.  # only positive correlated can contribute by def in NMF\n",
    "    \n",
    "    # 6. compute the NMF with this initialization and rank N\n",
    "    N_factors = W_init.shape[1]\n",
    "    nmf = NMF(\n",
    "        n_components=N_factors,\n",
    "        init='custom',\n",
    "        random_state=42\n",
    "    )\n",
    "    data_offset = data - np.min(data) + 1e-7\n",
    "    \n",
    "    data_offset = np.ascontiguousarray(data_offset)\n",
    "    W_init = np.ascontiguousarray(W_init)\n",
    "    H_init = np.ascontiguousarray(H_init)\n",
    "    \n",
    "    nmf_result = nmf.fit_transform(\n",
    "        data_offset,\n",
    "        W=W_init,\n",
    "        H=H_init\n",
    "    )\n",
    "    nmfdf = pd.DataFrame(nmf_result, index=data.index)\n",
    "    factor_to_lipid = nmf.components_\n",
    "    \n",
    "    return nmfdf, factor_to_lipid, N_factors, nmf\n",
    "\n",
    "nmfdf, factor_to_lipid, N_factors, nmf = compute_seeded_NMF(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033af27c-3264-42df-b512-125795397924",
   "metadata": {},
   "outputs": [],
   "source": [
    "nmfdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38c4d0d-34a5-49bc-a0b3-d3ea4f9937c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = np.corrcoef(data.values.T)\n",
    "corr_matrix = np.abs(corr)\n",
    "np.fill_diagonal(corr_matrix, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cefff81e-11bb-4547-afb9-d6dcd5156d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf_result = nmfdf.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40944bca-3a19-41c7-82f7-8ff0fb117136",
   "metadata": {},
   "outputs": [],
   "source": [
    "for PC_I in range(0, N_factors):\n",
    "\n",
    "    results = []\n",
    "    filtered_data = pd.concat([coordinates.loc[data.index,:], pd.DataFrame(nmf_result[:,PC_I], index=coordinates.loc[data.index,:].index,columns=[\"test\"])], axis=1)\n",
    "\n",
    "    currentPC = \"test\"\n",
    "    \n",
    "    for section in filtered_data['Section'].unique():\n",
    "        subset = filtered_data[filtered_data['Section'] == section]\n",
    "\n",
    "        perc_2 = subset[currentPC].quantile(0.02) \n",
    "        perc_98 = subset[currentPC].quantile(0.98)\n",
    "\n",
    "        results.append([section, perc_2, perc_98])\n",
    "    percentile_df = pd.DataFrame(results, columns=['Section', '2-perc', '98-perc'])\n",
    "    med2p = percentile_df['2-perc'].median()\n",
    "    med98p = percentile_df['98-perc'].median()\n",
    "\n",
    "    cmap = plt.cm.PuOr\n",
    "\n",
    "    fig, axes = plt.subplots(4, 8, figsize=(20, 10))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for section in range(1, 33):\n",
    "        ax = axes[section - 1]\n",
    "        ddf = filtered_data[(filtered_data['Section'] == section)]\n",
    "\n",
    "        ax.scatter(ddf['zccf'], -ddf['yccf'], c=ddf[currentPC], cmap=\"PuOr\", s=0.5,rasterized=True, vmin=med2p, vmax=med98p) \n",
    "        ax.axis('off')\n",
    "        ax.set_aspect('equal')\n",
    "\n",
    "    cbar_ax = fig.add_axes([0.92, 0.15, 0.02, 0.7])\n",
    "    norm = Normalize(vmin=med2p, vmax=med98p)\n",
    "    sm = ScalarMappable(norm=norm, cmap=cmap)\n",
    "    fig.colorbar(sm, cax=cbar_ax)\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 0.9, 1])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff18b5f6-35ae-42c4-a3ad-521124570c9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4d6b47-88c9-4d50-adaf-f9cbee260bd0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408ef8b9-e0bc-45bf-b6cb-b7acd9c92704",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b68bdca-ee04-4f25-ad18-8e7906b4033b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009119ba-5c80-4e11-8d2b-d2d72e9fbb72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee665b3-3b45-4846-9972-f8e8cc1f2b94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecbeef88-18ba-4cf5-b7e8-ff9cac7111c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
