{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257f7f8e-1eaa-4885-bb79-98b694ac2b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scanpy as sc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.cluster.hierarchy import linkage, fcluster\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from collections import deque\n",
    "from scipy.cluster.hierarchy import linkage, fcluster\n",
    "import scanpy as sc\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import squidpy as sq\n",
    "from matplotlib.cm import ScalarMappable\n",
    "from matplotlib.colors import Normalize\n",
    "from threadpoolctl import threadpool_limits, threadpool_info\n",
    "threadpool_limits(limits=8)\n",
    "import itertools\n",
    "from collections import deque\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.utils import resample\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from threadpoolctl import threadpool_limits, threadpool_info\n",
    "threadpool_limits(limits=8)\n",
    "import backSPIN\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c410963-0ee8-4506-a234-0ebd61a8d23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do the feature selection, if needed\n",
    "# the features we selected are based on:\n",
    "# a) Moran's I (vs salt-and-pepper lipids, white noise-like)\n",
    "# b) variance vs mean of section-wise variances\n",
    "\n",
    "data = pd.read_hdf(\"/data/luca/lipidatlas/ManuscriptAnalysisRound2/20240724_featsel_lba.h5\", key=\"table\")\n",
    "data = data.loc[(data['Section'] != 6) & (data['Section'] != 10) & (data['Section'] != 13) & (data['Section'] != 20) & (data['Section'] != 25),:]\n",
    "\n",
    "coordinates = data[['Section', 'zccf', 'yccf', 'xccf']]\n",
    "coordinates['Section'] = coordinates['Section'].astype(int)\n",
    "\n",
    "data = data.iloc[:,20:210]\n",
    "spatvar = np.load(\"/data/luca/lipidatlas/ManuscriptAnalysis/feature_selected_peaks-Copy1.npy\", allow_pickle=True)\n",
    "\n",
    "data = data.loc[:, np.intersect1d(data.columns, spatvar.astype(float))]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd97ce8f-0371-4721-9b11-a39706602644",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use NMF to decompose the data into factors; it works well with exp but not log data with current settings\n",
    "\n",
    "from sklearn.decomposition import NMF\n",
    "from threadpoolctl import threadpool_limits, threadpool_info\n",
    "threadpool_limits(limits=8)\n",
    "import os\n",
    "import pickle\n",
    "import uuid\n",
    "\n",
    "os.environ['OMP_NUM_THREADS'] = '6'\n",
    "\n",
    "nmfdfs = []\n",
    "\n",
    "# specify the expected number of factors\n",
    "N_factors = 15 #40 \n",
    "num_iterations = 10 \n",
    "\n",
    "for _ in tqdm(range(num_iterations)):\n",
    "    drop_fraction = np.random.uniform(0.05, 0.30)\n",
    "    columns_to_keep = np.random.choice(data.columns, \n",
    "                                       size=int(len(data.columns) * (1 - drop_fraction)), \n",
    "                                       replace=False)\n",
    "    datads = data[columns_to_keep]\n",
    "    \n",
    "    iteration_id = str(uuid.uuid4())\n",
    "    with open(f'kept_columns_{iteration_id}.pkl', 'wb') as f:\n",
    "        pickle.dump(columns_to_keep, f)\n",
    "    \n",
    "    nmf = NMF(n_components=N_factors, init='random', random_state=230598)\n",
    "    nmf_result = nmf.fit_transform(datads-np.min(datads)+1e-7)\n",
    "    nmfdf = pd.DataFrame(nmf_result, index=datads.index)\n",
    "    \n",
    "    nmfdf.to_parquet(f'nmf_result_{iteration_id}.parquet')\n",
    "    \n",
    "    nmfdfs.append(nmfdf)\n",
    "\n",
    "nmfdfs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9878096c-d3d1-4f5f-8687-e28ae951c73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the baseline\n",
    "\n",
    "from sklearn.decomposition import NMF\n",
    "from threadpoolctl import threadpool_limits, threadpool_info\n",
    "threadpool_limits(limits=8)\n",
    "import os\n",
    "os.environ['OMP_NUM_THREADS'] = '6'\n",
    "\n",
    "# specify the expected number of factors\n",
    "N_factors = 15 #40 \n",
    "\n",
    "nmf = NMF(n_components=N_factors, init='random', random_state=230598)\n",
    "\n",
    "nmf_result = nmf.fit_transform(data-np.min(data)+1e-7)\n",
    "\n",
    "nmfdf = pd.DataFrame(nmf_result, index=data.index)\n",
    "\n",
    "nmfdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db164152-a9bb-451e-90fa-aef8ba93cbf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnn_corrected_nmf = np.load(\"nmf_result_corrected.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac303d3a-afc3-45d9-b00a-d83e72ba4221",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "n_rows = 15 \n",
    "n_cols = 1 \n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(2, 15))\n",
    "\n",
    "axes = axes.flatten()\n",
    "\n",
    "for xxx, ax in enumerate(axes):\n",
    "    ax.scatter(nmfdf.iloc[:, xxx][::10], mnn_corrected_nmf[:, xxx][::10], \n",
    "               s=0.000005, alpha=0.5, c=mnn_corrected_nmf[:, xxx][::10], cmap=\"PuOr\", rasterized=True)\n",
    "\n",
    "    ax.grid(False)\n",
    "    ax.set_xticks([])  # Remove x-axis ticks\n",
    "    ax.set_yticks([])  # Remove y-axis ticks\n",
    "    ax.set_xticklabels([])  # Remove x-axis tick labels\n",
    "    ax.set_yticklabels([])  # Remove y-axis tick labels\n",
    "    ax.spines['top'].set_visible(False)  # Remove top spine\n",
    "    ax.spines['right'].set_visible(False)  # Remove right spine\n",
    "    ax.spines['left'].set_visible(False)  # Remove left spine\n",
    "    ax.spines['bottom'].set_visible(False)  # Remove bottom spine\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0845e56-4b18-4219-b26e-9fc1f30eb4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "downsamp_nmf = nmfdf[::20]\n",
    "downsamp_mnn_nmf = mnn_corrected_nmf[::20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b29841-2428-4ebb-9a81-d8ed54fd4e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "means = np.mean(downsamp_nmf, axis=0)\n",
    "stds = np.std(downsamp_nmf, axis=0)\n",
    "downsamp_nmf = (downsamp_nmf - means) / stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21922eaa-f3d7-42f4-a3db-a650d633078b",
   "metadata": {},
   "outputs": [],
   "source": [
    "means = np.mean(downsamp_mnn_nmf, axis=0)\n",
    "stds = np.std(downsamp_mnn_nmf, axis=0)\n",
    "downsamp_mnn_nmf = (downsamp_mnn_nmf - means) / stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5829e1f-d1f6-4be4-b1b2-72b60d6419b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## do a tSNE to see how the clustering method behaves globally as we go\n",
    "\n",
    "from openTSNE import TSNEEmbedding\n",
    "from openTSNE import affinity\n",
    "from openTSNE import initialization\n",
    "import numpy as np\n",
    "\n",
    "x_train = downsamp_nmf.values\n",
    "\n",
    "affinities_train = affinity.PerplexityBasedNN(\n",
    "    x_train,\n",
    "    perplexity=30,\n",
    "    metric=\"euclidean\",\n",
    "    n_jobs=8,\n",
    "    random_state=42,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "\n",
    "init_train = x_train[:,[0,5]] # initialize with two uncorrelated NMFs, note this affects results a bit\n",
    "\n",
    "embedding_train = TSNEEmbedding(\n",
    "    init_train,\n",
    "    affinities_train,\n",
    "    negative_gradient_method=\"fft\",\n",
    "    n_jobs=8,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "embedding_train_1 = embedding_train.optimize(n_iter=500, exaggeration=1.2) ########## parameters..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe9bafc-b11b-40f4-8be0-80ff990c7c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(embedding_train_1[:,0], embedding_train_1[:,1],c=coordinates['Section'][::20].astype(\"category\").cat.codes, cmap=\"nipy_spectral\", s=0.1, alpha=0.5, rasterized=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1e64a4-5cdf-45ce-a7d6-dc0ebedfe8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Assuming you have already imported necessary libraries and have the data ready\n",
    "\n",
    "# Get unique sections\n",
    "unique_sections = coordinates['Section'].unique()\n",
    "\n",
    "# Create a figure with subplots for each unique section\n",
    "fig, axs = plt.subplots(nrows=(len(unique_sections)+1)//2, ncols=2, figsize=(20, 5*len(unique_sections)//2))\n",
    "axs = axs.flatten()  # Flatten the 2D array of axes for easy indexing\n",
    "\n",
    "for i, section in enumerate(unique_sections):\n",
    "    # Create a mask for the current section\n",
    "    mask = coordinates['Section'][::20] == section\n",
    "    \n",
    "    # Plot all points in gray\n",
    "    axs[i].scatter(embedding_train_1[:,0], embedding_train_1[:,1], \n",
    "                   c='gray', s=0.1, alpha=0.5, rasterized=True)\n",
    "    \n",
    "    # Plot the current section in red\n",
    "    axs[i].scatter(embedding_train_1[mask,0], embedding_train_1[mask,1], \n",
    "                   c='red', s=0.1, alpha=0.5, rasterized=True)\n",
    "    \n",
    "    axs[i].set_title(f'Section: {section}')\n",
    "    axs[i].set_xlabel('Dimension 1')\n",
    "    axs[i].set_ylabel('Dimension 2')\n",
    "\n",
    "# Remove any unused subplots\n",
    "for j in range(i+1, len(axs)):\n",
    "    fig.delaxes(axs[j])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ecc19c-76e9-4a70-abc5-5702f683b0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## do a tSNE to see how the clustering method behaves globally as we go\n",
    "\n",
    "from openTSNE import TSNEEmbedding\n",
    "from openTSNE import affinity\n",
    "from openTSNE import initialization\n",
    "import numpy as np\n",
    "\n",
    "x_train = downsamp_mnn_nmf\n",
    "\n",
    "affinities_train = affinity.PerplexityBasedNN(\n",
    "    x_train,\n",
    "    perplexity=30,\n",
    "    metric=\"euclidean\",\n",
    "    n_jobs=8,\n",
    "    random_state=42,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "\n",
    "init_train = x_train[:,[0,5]] # initialize with two uncorrelated NMFs, note this affects results a bit\n",
    "\n",
    "embedding_train = TSNEEmbedding(\n",
    "    init_train,\n",
    "    affinities_train,\n",
    "    negative_gradient_method=\"fft\",\n",
    "    n_jobs=8,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "embedding_train_2 = embedding_train.optimize(n_iter=500, exaggeration=1.2) ########## parameters..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ff8551-8903-48d5-83dc-5e8e5c92d6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(embedding_train_2[:,0], embedding_train_2[:,1],c=coordinates['Section'][::20].astype(\"category\").cat.codes, cmap=\"nipy_spectral\", s=0.1, alpha=0.5, rasterized=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d5cd73-37f1-48f9-9d27-f3a3c5886661",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Assuming you have already imported necessary libraries and have the data ready\n",
    "\n",
    "# Get unique sections\n",
    "unique_sections = coordinates['Section'].unique()\n",
    "\n",
    "# Create a figure with subplots for each unique section\n",
    "fig, axs = plt.subplots(nrows=(len(unique_sections)+1)//2, ncols=2, figsize=(20, 5*len(unique_sections)//2))\n",
    "axs = axs.flatten()  # Flatten the 2D array of axes for easy indexing\n",
    "\n",
    "for i, section in enumerate(unique_sections):\n",
    "    # Create a mask for the current section\n",
    "    mask = coordinates['Section'][::20] == section\n",
    "    \n",
    "    # Plot all points in gray\n",
    "    axs[i].scatter(embedding_train_2[:,0], embedding_train_2[:,1], \n",
    "                   c='gray', s=0.1, alpha=0.5, rasterized=True)\n",
    "    \n",
    "    # Plot the current section in red\n",
    "    axs[i].scatter(embedding_train_2[mask,0], embedding_train_2[mask,1], \n",
    "                   c='red', s=0.1, alpha=0.5, rasterized=True)\n",
    "    \n",
    "    axs[i].set_title(f'Section: {section}')\n",
    "    axs[i].set_xlabel('Dimension 1')\n",
    "    axs[i].set_ylabel('Dimension 2')\n",
    "\n",
    "# Remove any unused subplots\n",
    "for j in range(i+1, len(axs)):\n",
    "    fig.delaxes(axs[j])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b2809f-809b-4d03-b83e-ee99a2e53c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_nmf = nmfdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36243d38-5a14-4d6a-9eea-0e2e8ac74f57",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pairwise_correlations_with_ref = []\n",
    "maxs = []\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "for df in tqdm(nmfdfs):\n",
    "    correlation_matrix = pd.DataFrame(np.corrcoef(df.T, ref_nmf.T)[:len(df.columns), len(df.columns):])\n",
    "    pairwise_correlations_with_ref.append(correlation_matrix)\n",
    "    maxs.append(correlation_matrix.max())\n",
    "    \n",
    "maxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5987fa45-1565-48a4-8170-0c1dc4abd97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "maxs = pd.DataFrame(maxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b836af7b-829e-4975-a2cd-ed3164b548ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "maxs = maxs.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39d6bf5-6d4e-4145-a236-a9013de0ace3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(maxs,cmap=\"cividis\")\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc06fb2-9058-4530-9af6-006996d66548",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "# List to store the contents of each .pkl file\n",
    "kept_columns_list = []\n",
    "\n",
    "# Directory where your files are stored (replace '.' with the path to your folder if needed)\n",
    "directory = '.'\n",
    "\n",
    "# Get all files starting with \"kept_columns\"\n",
    "iteration_files = [f for f in os.listdir(directory) if f.startswith('kept_columns') and f.endswith('.pkl')]\n",
    "\n",
    "# Iterate over each file and load the pickle contents\n",
    "for file_name in iteration_files:\n",
    "    file_path = os.path.join(directory, file_name)\n",
    "    with open(file_path, 'rb') as f:\n",
    "        columns_to_keep = pickle.load(f)\n",
    "        kept_columns_list.append(len(columns_to_keep))\n",
    "\n",
    "kept_columns_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42282e48-0270-468e-9bba-a7f0a85e1ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "len_kept_columns_list = kept_columns_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0485d355-2426-4be5-b683-75ea29578135",
   "metadata": {},
   "outputs": [],
   "source": [
    "len_kept_columns_list = [str(len_value) for len_value in len_kept_columns_list]  # Convert entries to strings if needed\n",
    "\n",
    "plt.imshow(maxs, cmap=\"cividis\")\n",
    "plt.colorbar()\n",
    "\n",
    "plt.yticks(ticks=range(15), labels=[f\"NMF{i+1}\" for i in range(15)])\n",
    "\n",
    "plt.xticks(ticks=range(len(len_kept_columns_list)), labels=[f\"{entry}/108\" for entry in len_kept_columns_list], rotation=90)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414defba-b943-45c6-9745-85be5a09305f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#clustering_robn1.ipynb\n",
    "#checks_on_clustering.ipynb\n",
    "#control1sec.ipynb\n",
    "#!!! the two leiden notebooks here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567e66fd-a4b7-4632-9e88-9c0af648ab3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f7eedf-0cda-4a8e-ae6d-e1ac5df3460f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0254af63-97ca-4527-a665-bdeb7c5f2909",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de519bb-6a1d-4989-b6ad-b28e68902124",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
