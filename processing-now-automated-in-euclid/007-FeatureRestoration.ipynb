{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4387118b-af49-4f01-9a61-5caa0599affd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import pearsonr\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from xgboost import XGBRegressor\n",
    "from threadpoolctl import threadpool_limits, threadpool_info\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "from matplotlib.cm import ScalarMappable\n",
    "from matplotlib.colors import Normalize\n",
    "import seaborn as sns\n",
    "import scanpy as sc\n",
    "import umap.umap_ as umap\n",
    "\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = \"10\"\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"10\"\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"6\"\n",
    "threadpool_limits(limits=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0451b88a-f92e-4442-bf35-b7111517aa89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1b2fbd68-a84a-4d81-a851-5ad56dc9b938",
   "metadata": {},
   "source": [
    "## Import and prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0786e330-4207-475c-a0e9-459e9fb44971",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the harmonized-NMF embeddings: they will be used as predictors\n",
    "\n",
    "embeddings = pd.read_hdf(\"corrected_nmfall_nochunking.h5ad\", key=\"table\")\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4e5b6f-4574-4934-83c7-8bc13e968961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the Moran's I scores - they will be used to pick sections that are spatially good enough to train on\n",
    "\n",
    "morans = pd.read_csv(\"morans_by_sec.csv\", index_col=0)\n",
    "morans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f3854c-e24e-4dc7-9da0-e80ca0f30e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "isitrestorable = (morans > 0.4).sum(axis=1).sort_values()\n",
    "torestore = isitrestorable[isitrestorable > 3].index # there must be at least three good sections to train on and one to evaluate on\n",
    "torestore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028ad96a-2988-4560-ba0c-a2c40442e55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import and preprocess the whole dataset\n",
    "alldata = pd.read_hdf(\"20241103_pixels_allips_allbrains_allen_pixelcleaned.h5ad\")\n",
    "\n",
    "cols = np.array(alldata.columns)\n",
    "cols[:1400]=cols[:1400].astype(float).astype(str)\n",
    "alldata.columns = cols\n",
    "\n",
    "lipids_to_restore = alldata.loc[:,torestore.astype(float).astype(str)]\n",
    "lipids_to_restore = lipids_to_restore.iloc[:-5,:]\n",
    "lipids_to_restore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6949d4-60b4-41a0-821d-342bc08428fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c53a02d0-f986-4da9-b3c0-8187a002755f",
   "metadata": {},
   "source": [
    "## Select the sections to be used to train XGB models for imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54fabf5b-5467-4556-8eae-2d8981c77e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "usage_dataframe = morans.iloc[:,:70].copy() # use the atlases as the basis to impute on\n",
    "\n",
    "# remove the broken sections\n",
    "brokenones = alldata[['SectionID', 'BadSection']].drop_duplicates().dropna()\n",
    "goodones = brokenones.loc[brokenones['BadSection'] == 0,'SectionID'].values\n",
    "usage_dataframe = usage_dataframe.loc[:, usage_dataframe.columns.astype(float).isin(goodones)]\n",
    "\n",
    "# choose the best sections to train and validate XGBoost models on\n",
    "def top_3_above_threshold(row, threshold=0.4):\n",
    "    \n",
    "    above_threshold = row >= threshold\n",
    "    \n",
    "    if above_threshold.sum() >= 3:\n",
    "        \n",
    "        top_3 = row.nlargest(3).index\n",
    "        result = pd.Series(False, index=row.index)\n",
    "        result[top_3] = True\n",
    "    else:\n",
    "        result = above_threshold\n",
    "    \n",
    "    return result\n",
    "\n",
    "usage_dataframe = usage_dataframe.apply(top_3_above_threshold, axis=1)\n",
    "\n",
    "usage_dataframe=usage_dataframe.loc[np.array(usage_dataframe.sum(axis=1).index[usage_dataframe.sum(axis=1) > 2]),:]\n",
    "usage_dataframe = usage_dataframe.loc[usage_dataframe.index.astype(float).astype(str) != '953.120019',:]\n",
    "usage_dataframe # could be further be optimized by ensuring the 3 training sections are not-so-close-to-each-other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b34d4a7-e66a-40dc-a578-722dee087914",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "usage_dataframe.sum() # (strange distribution...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bba7007-5b7f-4944-9df8-8343481b4751",
   "metadata": {},
   "outputs": [],
   "source": [
    "usage_dataframe.T.sum().min() # ok all good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678852d1-4bcf-4e55-acda-7bebbe559f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some data prep\n",
    "lipids_to_restore = lipids_to_restore.loc[:,usage_dataframe.index.astype(float).astype(str)]\n",
    "lipids_to_restore['SectionID'] = alldata['SectionID']\n",
    "coordinates = alldata.loc[embeddings.index, ['SectionID', 'x', 'y']]\n",
    "coordinates['SectionID'] = coordinates['SectionID'].astype(float).astype(int).astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0915586-b1ab-441b-a44c-0913c8b3a1c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c75e0060-adad-47fc-952e-b9ac33b589c6",
   "metadata": {},
   "source": [
    "## Train XGB models for imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddcb4afc-d6ed-489e-b6b7-2ce0e1e1fa39",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "metrics_df = pd.DataFrame(\n",
    "    columns=['train_pearson_r', 'train_rmse', 'val_pearson_r', 'val_rmse']\n",
    ")\n",
    "\n",
    "for index, row in tqdm(usage_dataframe.iterrows(), total=usage_dataframe.shape[0]):\n",
    "    #try:\n",
    "    train_sections = row[row].index.tolist()  \n",
    "    val_sections = train_sections[1]\n",
    "    train_sections = [train_sections[0], train_sections[2]]\n",
    "\n",
    "    train_data = embeddings.loc[coordinates['SectionID'].isin(train_sections),:]\n",
    "    y_train = lipids_to_restore.loc[train_data.index, str(index)]\n",
    "\n",
    "    # take one out and use it for validation: can we trust this XGB model? \n",
    "    val_data = embeddings.loc[coordinates['SectionID'] == val_sections,:]\n",
    "    y_val = lipids_to_restore.loc[val_data.index, str(index)]\n",
    "\n",
    "    model = XGBRegressor()\n",
    "    model.fit(train_data, y_train)\n",
    "\n",
    "    train_pred = model.predict(train_data)\n",
    "    val_pred = model.predict(val_data)\n",
    "\n",
    "    train_pearson = pearsonr(y_train, train_pred)[0]\n",
    "    val_pearson = pearsonr(y_val, val_pred)[0]\n",
    "    print(val_pearson)\n",
    "    train_rmse = np.sqrt(mean_squared_error(y_train, train_pred))\n",
    "    val_rmse = np.sqrt(mean_squared_error(y_val, val_pred))\n",
    "    metrics_df.loc[index] = {\n",
    "        'train_pearson_r': train_pearson,\n",
    "        'train_rmse': train_rmse,\n",
    "        'val_pearson_r': val_pearson,\n",
    "        'val_rmse': val_rmse\n",
    "    }\n",
    "\n",
    "    # save the model\n",
    "    model_path = os.path.join('xgbmodels_onmnnnmf', str(index)+'_xgb_model.joblib')\n",
    "    joblib.dump(model, model_path)\n",
    "\n",
    "    #except Exception as e:\n",
    "     #   print(\"exception at index: \"+str(index))\n",
    "      #  continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec89ea03-ff85-493a-ba04-387b57caa854",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the performance on the hold-out test set across to-be-imputed lipids\n",
    "plt.hist(metrics_df['val_pearson_r'], bins=30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2578982-b3d4-4da4-88a7-4e29bf63176c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "de1cf05a-11af-4056-860d-62687ff0b86f",
   "metadata": {},
   "source": [
    "## Deploy the trained XGB models across all acquisitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ef555f-4c52-4698-881c-58ac69be32bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# loop to import and deploy the models, creating one column at a time. deploy on all sections, also on the training ones, to be in-distribution\n",
    "coordinates = coordinates[['SectionID',\t'x',\t'y']]\n",
    "for file in tqdm(os.listdir('xgbmodels_onmnnnmf')[1:]):\n",
    "    model_path = os.path.join('xgbmodels_onmnnnmf', file)\n",
    "    model = joblib.load(model_path)\n",
    "    coordinates[file] = model.predict(embeddings)\n",
    "coordinates.columns = [\n",
    "    col.replace('_xgb_model.joblib', '') if i >= 3 else col \n",
    "    for i, col in enumerate(coordinates.columns)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1458f8d-7ada-461d-8c2c-0fd30ad16cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter with the metrics df to keep only \"reliably imputed\" lipids\n",
    "metrics_df.to_csv(\"metrics_imputation_df.csv\")\n",
    "\n",
    "# keep only the lipids whose generalization Pearson's R is good enough (0.4 threshold)\n",
    "cols = np.array(coordinates.columns)\n",
    "cols[3:] = cols[3:].astype(float).astype(str)\n",
    "coordinates.columns = cols\n",
    "coordinates = coordinates.loc[:, metrics_df.loc[metrics_df['val_pearson_r'] > 0.4,:].index.astype(float).astype(str)]\n",
    "coordinates.to_hdf(\"20241113_xgboost_recovered_lipids.h5ad\", key=\"table\")\n",
    "coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e290721-4c47-4a03-bfb0-b35b232e6066",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5c1be8e0-4e07-424d-b743-d87e19cb06f3",
   "metadata": {},
   "source": [
    "## Check on examples the effect of imputation with spatial plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847b73f7-d3a7-40c9-9f44-99190ce02eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "currentPC = '1002.581042'\n",
    "filtered_data = coordinates\n",
    "for PC_I in range(1):\n",
    "\n",
    "    results = []\n",
    "    \n",
    "    for section in filtered_data['SectionID'].unique():\n",
    "        subset = filtered_data[filtered_data['SectionID'] == section]\n",
    "\n",
    "        perc_2 = subset[currentPC].quantile(0.02)\n",
    "        perc_98 = subset[currentPC].quantile(0.98)\n",
    "\n",
    "        results.append([section, perc_2, perc_98])\n",
    "    percentile_df = pd.DataFrame(results, columns=['SectionID', '2-perc', '98-perc'])\n",
    "    med2p = percentile_df['2-perc'].median()\n",
    "    med98p = percentile_df['98-perc'].median()\n",
    "\n",
    "    cmap = plt.cm.plasma\n",
    "\n",
    "    fig, axes = plt.subplots(14, 10, figsize=(20, 38))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for section in np.unique(filtered_data['SectionID']):\n",
    "        ax = axes[int(section) - 1]\n",
    "        try:\n",
    "            ddf = filtered_data[(filtered_data['SectionID'] == section)]\n",
    "\n",
    "            ax.scatter(ddf['y'], -ddf['x'], c=ddf[currentPC], cmap=\"plasma\", s=0.5,rasterized=True, vmin=med2p, vmax=med98p) \n",
    "            ax.axis('off')\n",
    "            ax.set_aspect('equal')\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    cbar_ax = fig.add_axes([0.92, 0.15, 0.02, 0.7])\n",
    "    norm = Normalize(vmin=med2p, vmax=med98p)\n",
    "    sm = ScalarMappable(norm=norm, cmap=cmap)\n",
    "    fig.colorbar(sm, cax=cbar_ax)\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 0.9, 1])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81fcd2c-d4a1-41a5-b62f-0ffa0a0fa958",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1716bd26-99b1-4e39-ac60-7b59d78970a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
